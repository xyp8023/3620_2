{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing of data\n",
    "# Function to load data\n",
    "\n",
    "def get_power_data():\n",
    "    \"\"\"\n",
    "    Read the Individual household electric power consumption dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assume that the dataset is located on folder \"data\"\n",
    "    data = pd.read_csv('./household_power_consumption.txt',\n",
    "                       sep=';', low_memory=False)\n",
    "#     data = pd.read_csv('../../dataset/household_power_consumption.txt',\n",
    "#                        sep=';', low_memory=False)\n",
    "\n",
    "    # Drop some non-predictive variables\n",
    "    data = data.drop(columns=['Date', 'Time'], axis=1)\n",
    "\n",
    "    #print(data.head())\n",
    "\n",
    "    # Replace missing values\n",
    "    data = data.replace('?', np.nan)\n",
    "\n",
    "    # Drop NA\n",
    "    data = data.dropna(axis=0)\n",
    "\n",
    "    # Normalize\n",
    "    standard_scaler = preprocessing.StandardScaler()\n",
    "    np_scaled = standard_scaler.fit_transform(data)\n",
    "    data = pd.DataFrame(np_scaled)\n",
    "\n",
    "    # Goal variable assumed to be the first\n",
    "    X = data.values[:, 1:].astype('float32')\n",
    "    y = data.values[:, 0].astype('float32')\n",
    "\n",
    "    # Create categorical y for binary classification with balanced classes\n",
    "    y = np.sign(y+0.46)\n",
    "\n",
    "    # Split train and test data here: (X_train, Y_train, X_test, Y_test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    no_class = 2                 #binary classification\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, no_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X,y types: <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "X size (1536960, 6)\n",
      "Y size (1536960,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, no_class = get_power_data()\n",
    "print(\"X,y types: {} {}\".format(type(X_train), type(y_train)))\n",
    "print(\"X size {}\".format(X_train.shape))\n",
    "print(\"Y size {}\".format(y_train.shape))\n",
    "\n",
    "# Create a binary variable from one of the columns.\n",
    "# You can use this OR not\n",
    "\n",
    "idx = y_train >= 0\n",
    "notidx = y_train < 0\n",
    "y_train[idx] = 1\n",
    "y_train[notidx] = -1\n",
    "\n",
    "\n",
    "# X_test = X_test/np.linalg.norm(X_test)\n",
    "# X_train = X_train/np.linalg.norm(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_train.shape)\n",
    "# #print(X_train.min())\n",
    "# print(X_test.max(), X_test.min())\n",
    "\n",
    "\n",
    "# print(X_test.max(), X_test.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(x, derivative=False):\n",
    "    sigm = 1. / (1. + np.exp(-x)) \n",
    "    if derivative:\n",
    "        return sigm * (1. - sigm)\n",
    "    return sigm\n",
    "\n",
    "# Define weights initialization\n",
    "def initialize_w(N, d):\n",
    "    return 2*np.random.random((N,d)) - 1\n",
    "\n",
    "# Fill in feed forward propagation\n",
    "def feed_forward_propagation(X, y, w_1, w_2, w_3, lmbda):\n",
    "    # Fill in\n",
    "    # X (N,d)\n",
    "    # w_1 (d,h)\n",
    "    # w_2 (h,g)\n",
    "    # w_3 (g,1)\n",
    "    N,d = X.shape\n",
    "    layer_0 = X # (N,d)\n",
    "    layer_1 = sigmoid(np.dot(layer_0, w_1)) # (N, h)\n",
    "    layer_2 = sigmoid(np.dot(layer_1, w_2)) # (N, g)\n",
    "    layer_3 = np.dot(layer_2, w_3) # (N, 1)\n",
    "    \n",
    "    return layer_0, layer_1, layer_2, layer_3\n",
    "def back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda):\n",
    "    N = y.shape[0]\n",
    "    y = y.reshape((-1,1))\n",
    "    layer_3_delta = np.zeros_like(w_3) # (g,1)\n",
    "    layer_2_delta = np.zeros_like(w_2) # (h, g)\n",
    "    layer_1_delta = np.zeros_like(w_1) # (d, h)\n",
    "    layer_3_delta = 2 * np.dot(layer_2.T, (layer_3 - y)) # (g,1)\n",
    "    # print(\"np.dot(w_3,(layer_3-y)) shape \", np.dot(w_3,(layer_3-y).T).shape)\n",
    "    # print(\" sigmoid(np.dot(layer_1,w_2), derivative=True).T shape\", sigmoid(np.dot(layer_1,w_2), derivative=True).T.shape)\n",
    "    dJ_dl2 = 2 * np.dot(w_3,(layer_3-y).T) # # (g,N)\n",
    "    dl2_ds2 = sigmoid(np.dot(layer_1,w_2), derivative=True).T # (g,N)\n",
    "\n",
    "    layer_2_delta  = np.dot(dJ_dl2 * dl2_ds2, layer_1).T    \n",
    "    # layer_2_delta = 2 * np.dot(np.dot(w_3,(layer_3-y).T)*sigmoid(np.dot(layer_1,w_2), derivative=True).T, layer_1).T\n",
    "    ds2_dl1 = w_2 # (h,g)\n",
    "    dl1_ds1 =  sigmoid(np.dot(layer_0,w_1), derivative=True).T # (h,N)\n",
    "    ds1_dw1 = layer_0 # (N,d)\n",
    "\n",
    "    layer_1_delta = np.dot(np.dot(ds2_dl1, dJ_dl2 * dl2_ds2) * dl1_ds1, ds1_dw1).T # (d,h)\n",
    "    return layer_1_delta/N, layer_2_delta/N, layer_3_delta/N\n",
    "\n",
    "def back_propagation_blocklayer(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda, activelayer = 3):\n",
    "    N = y.shape[0]\n",
    "    y = y.reshape((-1,1))\n",
    "    layer_3_delta = np.zeros_like(w_3) # (g,1)\n",
    "    layer_2_delta = np.zeros_like(w_2) # (h, g)\n",
    "    layer_1_delta = np.zeros_like(w_1) # (d, h)\n",
    "    layer_3_delta = 2 * np.dot(layer_2.T, (layer_3 - y)) # (g,1)\n",
    "    \n",
    "    if activelayer == 3:\n",
    "        return layer_1_delta/N, layer_2_delta/N, layer_3_delta/N\n",
    "    \n",
    "    # print(\"np.dot(w_3,(layer_3-y)) shape \", np.dot(w_3,(layer_3-y).T).shape)\n",
    "    # print(\" sigmoid(np.dot(layer_1,w_2), derivative=True).T shape\", sigmoid(np.dot(layer_1,w_2), derivative=True).T.shape)\n",
    "    dJ_dl2 = 2 * np.dot(w_3,(layer_3-y).T) # # (g,N)\n",
    "    dl2_ds2 = sigmoid(np.dot(layer_1,w_2), derivative=True).T # (g,N)\n",
    "\n",
    "    layer_2_delta  = np.dot(dJ_dl2 * dl2_ds2, layer_1).T    \n",
    "    if activelayer == 2:\n",
    "        return layer_1_delta/N, layer_2_delta/N, layer_3_delta/N\n",
    "    \n",
    "    # layer_2_delta = 2 * np.dot(np.dot(w_3,(layer_3-y).T)*sigmoid(np.dot(layer_1,w_2), derivative=True).T, layer_1).T\n",
    "    ds2_dl1 = w_2 # (h,g)\n",
    "    dl1_ds1 =  sigmoid(np.dot(layer_0,w_1), derivative=True).T # (h,N)\n",
    "    ds1_dw1 = layer_0 # (N,d)\n",
    "\n",
    "    layer_1_delta = np.dot(np.dot(ds2_dl1, dJ_dl2 * dl2_ds2) * dl1_ds1, ds1_dw1).T # (d,h)\n",
    "    if activelayer == 1:\n",
    "        return layer_1_delta/N, layer_2_delta/N, layer_3_delta/N\n",
    "\n",
    "# Cost function\n",
    "def cost(X, y, w_1, w_2, w_3, lmbda):\n",
    "    N, d = X.shape\n",
    "    a1,a2,a3,a4 = feed_forward_propagation(X,y,w_1,w_2,w_3,lmbda)\n",
    "\n",
    "#     return np.linalg.norm(a4[:,0] - y,2) ** 2 / N + lmbda * (np.linalg.norm(w_1)**2 + np.linalg.norm(w_2)**2 + np.linalg.norm(w_3)**2)\n",
    "    return np.linalg.norm(a4[:,0] - y,2) ** 2 / N\n",
    "\n",
    "# Define SGD\n",
    "def SGD(X, y, w_1, w_2, w_3, lmbda, learning_rate, batch_size, iterations):\n",
    "    # Complete here:\n",
    "    y = y.reshape((-1,1))\n",
    "    for i in range(iterations):\n",
    "        randomInd = np.arange(X.shape[0])\n",
    "        np.random.shuffle(randomInd)\n",
    "        randomInd = randomInd[:batch_size]\n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X[randomInd,:],y[randomInd,:],w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y[randomInd,:], w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda)\n",
    "        \n",
    "        g = (np.linalg.norm(layer_1_delta) + np.linalg.norm(layer_2_delta) + np.linalg.norm(layer_3_delta))/3\n",
    "        if i%20==0:\n",
    "            print(\"g is \", g)   \n",
    "        if (g <= epsilon):\n",
    "            print(\"converge, break! current i: \", i)\n",
    "            break\n",
    "        \n",
    "        w_1 = w_1 - learning_rate*layer_1_delta\n",
    "        w_2 = w_2 - learning_rate*layer_2_delta\n",
    "        w_3 = w_3 - learning_rate*layer_3_delta\n",
    "    \n",
    "        \n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        if i%20==0:\n",
    "            print(i,loss)\n",
    "        \n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define SVRG here:\n",
    "def SVRG(X, y, w_1, w_2, w_3, lmbda, learning_rate, T, batch_size, iterations):\n",
    "    # Complete here:\n",
    "    y = y.reshape((-1,1))\n",
    "    N = X.shape[0]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # compute all gradient and store\n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X,y,w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda)\n",
    "            \n",
    "        \n",
    "        g = (np.linalg.norm(layer_1_delta) + np.linalg.norm(layer_2_delta) + np.linalg.norm(layer_3_delta))/3\n",
    "        if i%20==0:\n",
    "            print(\"g is \", g)   \n",
    "        if (g <= epsilon):\n",
    "            print(\"converge, break! current i: \", i)\n",
    "            break\n",
    "        \n",
    "        # initialize the w_previous\n",
    "        # w_previous = w.copy()\n",
    "        w_1_previous, w_2_previous, w_3_previous = w_1.copy(), w_2.copy(), w_3.copy()\n",
    "        for t in range(T//batch_size):\n",
    "            # random sample\n",
    "            # randomInd = int(np.random.rand() * N)\n",
    "            randomInd = np.arange(N)\n",
    "            np.random.shuffle(randomInd)\n",
    "            randomInd = randomInd[:batch_size]\n",
    "            # randomInd = np.random.randint(0,N)\n",
    "            layer_0_p1, layer_1_p1, layer_2_p1, layer_3_p1 = feed_forward_propagation(X[randomInd,:],y[randomInd,:],w_1_previous,w_2_previous,w_3_previous,lmbda)\n",
    "            layer_1_delta_p1, layer_2_delta_p1, layer_3_delta_p1 = back_propagation(y[randomInd,:], w_1_previous, w_2_previous, w_3_previous, layer_0_p1, layer_1_p1, layer_2_p1, layer_3_p1, lmbda)\n",
    "\n",
    "            layer_0_p2, layer_1_p2, layer_2_p2, layer_3_p2 = feed_forward_propagation(X[randomInd,:],y[randomInd,:],w_1,w_2,w_3,lmbda)\n",
    "            layer_1_delta_p2, layer_2_delta_p2, layer_3_delta_p2 = back_propagation(y[randomInd,:], w_1, w_2, w_3, layer_0_p2, layer_1_p2, layer_2_p2, layer_3_p2, lmbda)\n",
    "            \n",
    "            # calculate the update term\n",
    "            # part1 = function_gradient_vectorization(x[:,randomInd], y[:,randomInd], w_previous, lambda_, gradclip = gradclip)\n",
    "            # part2 = function_gradient_vectorization(x[:,randomInd], y[:,randomInd], w, lambda_, gradclip = gradclip)\n",
    "            # part3 = g\n",
    "\n",
    "            w_1_previous = w_1_previous - learning_rate * (layer_1_delta_p1 - layer_1_delta_p2 + layer_1_delta)\n",
    "            w_2_previous = w_2_previous - learning_rate * (layer_2_delta_p1 - layer_2_delta_p2 + layer_2_delta)\n",
    "            w_3_previous = w_3_previous - learning_rate * (layer_3_delta_p1 - layer_3_delta_p2 + layer_3_delta)\n",
    "\n",
    "            # w_previous = w_previous - alpha * (part1 - part2 + part3)\n",
    "            \n",
    "        # w = w_previous\n",
    "        \n",
    "        w_1, w_2, w_3 = w_1_previous, w_2_previous, w_3_previous\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        if i%20==0:\n",
    "            print(i,loss)\n",
    "    \n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define GD here:\n",
    "def GD(X, y, w_1,w_2,w_3, lmbda, learning_rate, iterations):\n",
    "    N = X.shape[0]\n",
    "    for i in range(iterations):\n",
    "\n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda)\n",
    "        \n",
    "        g = (np.linalg.norm(layer_1_delta) + np.linalg.norm(layer_2_delta) + np.linalg.norm(layer_3_delta))/3\n",
    "        if i%20==0:\n",
    "            print(\"g is \", g)   \n",
    "        if (g <= epsilon):\n",
    "            print(\"converge, break! current i: \", i)\n",
    "            break\n",
    "#         w_1 = w_1 -  learning_rate * layer_1_delta + (lmbda / N * w_1)\n",
    "#         w_2 = w_2 - learning_rate * layer_2_delta + (lmbda / N * w_2)\n",
    "#         w_3 = w_3 - learning_rate * layer_3_delta + (lmbda / N * w_3)\n",
    "        w_1 = w_1 -  learning_rate * layer_1_delta\n",
    "        w_2 = w_2 - learning_rate * layer_2_delta\n",
    "        w_3 = w_3 - learning_rate * layer_3_delta\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        if i%20==0:\n",
    "            print(i,loss)\n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define projected GD here:\n",
    "def PGD(X, y, w_1,w_2,w_3, lmbda, learning_rate, iterations, noise = None):\n",
    "    # Complete here:\n",
    "    N = X.shape[0]\n",
    "    for i in range(iterations):\n",
    "\n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda)\n",
    "        \n",
    "        g = (np.linalg.norm(layer_1_delta) + np.linalg.norm(layer_2_delta) + np.linalg.norm(layer_3_delta))/3\n",
    "        if i%20==0:\n",
    "            print(\"g is \", g)   \n",
    "        if (g <= epsilon):\n",
    "            print(\"converge, break! current i: \", i)\n",
    "            break\n",
    "            \n",
    "        if noise == None:\n",
    "            # the noise level is determined by the delta\n",
    "            noise = []\n",
    "            noise.append((np.random.random((w_1.shape)) * 2 - 1) *np.mean(layer_1_delta))\n",
    "            noise.append((np.random.random((w_2.shape)) * 2 - 1) *np.mean(layer_2_delta))\n",
    "            noise.append((np.random.random((w_3.shape)) * 2 - 1) *np.mean(layer_3_delta))\n",
    "        \n",
    "        w_1 = w_1 -  learning_rate * layer_1_delta + noise[0]*0.1\n",
    "        w_2 = w_2 - learning_rate * layer_2_delta + noise[1]*0.1\n",
    "        w_3 = w_3 - learning_rate * layer_3_delta + noise[2]*0.1\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        \n",
    "        if i%20==0:\n",
    "            print(i,loss)\n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define BCD here:\n",
    "def BCD(X, y, w_1,w_2,w_3, lmbda, learning_rate, iterations, strategy = 0, blockrate = 0.5):\n",
    "    # Complete here:\n",
    "    # 2 strategies:\n",
    "    # a. randomly select weight by a random mask (random coordinate selection)\n",
    "    # b. we train the layers asynchronously to archieve the goal of blocking coordinate (cyclic update rule) \n",
    "    for i in range(iterations):\n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda)\n",
    "        \n",
    "        g = (np.linalg.norm(layer_1_delta) + np.linalg.norm(layer_2_delta) + np.linalg.norm(layer_3_delta))/3\n",
    "        if i%20==0:\n",
    "            print(\"g is \", g)   \n",
    "        if (g <= epsilon):\n",
    "            print(\"converge, break! current i: \", i)\n",
    "            break\n",
    "            \n",
    "        if strategy == 0:\n",
    "            layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "            # generate random mask for each layer\n",
    "            l1_mask = np.random.choice([1, 0], size=layer_1_delta.shape, p=[blockrate, 1 - blockrate])\n",
    "            l2_mask = np.random.choice([1, 0], size=layer_2_delta.shape, p=[blockrate, 1 - blockrate])\n",
    "            l3_mask = np.random.choice([1, 0], size=layer_3_delta.shape, p=[blockrate, 1 - blockrate])\n",
    "            w_1 = w_1 - learning_rate * layer_1_delta * l1_mask\n",
    "            w_2 = w_2 - learning_rate * layer_2_delta * l2_mask\n",
    "            w_3 = w_3 - learning_rate * layer_3_delta * l3_mask\n",
    "        else:\n",
    "            # cyclic\n",
    "            # layer 1\n",
    "            layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "            layer_1_delta, _, _ = back_propagation_blocklayer(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda, 1)\n",
    "            w_1 = w_1 - learning_rate * layer_1_delta\n",
    "            # layer 2\n",
    "            layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "            _, layer_2_delta, _ = back_propagation_blocklayer(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda, 2)\n",
    "            w_2 = w_2 - learning_rate * layer_2_delta\n",
    "            # layer 3\n",
    "            layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "            _, _, layer_3_delta = back_propagation_blocklayer(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda, 3)\n",
    "            w_3 = w_3 - learning_rate * layer_3_delta\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        if i%20==0:\n",
    "            print(i,loss)\n",
    "    return w_1, w_2, w_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nw_size = 10\\n\\n# Initialize weights for debug\\nw_1 = initialize_w(X_train.shape[1], w_size)\\n\\nw_2 = initialize_w(w_size,w_size+1)\\n\\nw_3 = initialize_w(w_size+1, 1)\\n\\nlmbda = 0\\n\\nlayer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X_test, y_test, w_1, w_2, w_3, lmbda)\\n\\nprint(\"layer_0 shape \", layer_0.shape)\\nprint(\"layer_1 shape \",layer_1.shape)\\nprint(\"layer_2 shape \",layer_2.shape)\\nprint(\"layer_3 shape \",layer_3.shape)\\n\\nlayer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y_test, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda)\\n\\n\\nprint(\"w_3 shape \",w_3.shape)\\nprint(\"w_2 shape \",w_2.shape)\\nprint(\"w_1 shape \",w_1.shape)\\n\\nprint(\"layer_3_delta shape\", layer_3_delta.shape)\\nprint(\"layer_2_delta shape\", layer_2_delta.shape)\\nprint(\"layer_1_delta shape\", layer_1_delta.shape)\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "w_size = 10\n",
    "\n",
    "# Initialize weights for debug\n",
    "w_1 = initialize_w(X_train.shape[1], w_size)\n",
    "\n",
    "w_2 = initialize_w(w_size,w_size+1)\n",
    "\n",
    "w_3 = initialize_w(w_size+1, 1)\n",
    "\n",
    "lmbda = 0\n",
    "\n",
    "layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X_test, y_test, w_1, w_2, w_3, lmbda)\n",
    "\n",
    "print(\"layer_0 shape \", layer_0.shape)\n",
    "print(\"layer_1 shape \",layer_1.shape)\n",
    "print(\"layer_2 shape \",layer_2.shape)\n",
    "print(\"layer_3 shape \",layer_3.shape)\n",
    "\n",
    "layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y_test, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda)\n",
    "\n",
    "\n",
    "print(\"w_3 shape \",w_3.shape)\n",
    "print(\"w_2 shape \",w_2.shape)\n",
    "print(\"w_1 shape \",w_1.shape)\n",
    "\n",
    "print(\"layer_3_delta shape\", layer_3_delta.shape)\n",
    "print(\"layer_2_delta shape\", layer_2_delta.shape)\n",
    "print(\"layer_1_delta shape\", layer_1_delta.shape)\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD\tinitial loss is : 14.976088889930564\n",
      "g is  16.902111105984616\n",
      "0 4.860076561368949\n",
      "g is  0.6354420685437092\n",
      "20 0.5795200091244677\n",
      "g is  0.40640805403000946\n",
      "40 0.4185373448177388\n",
      "g is  0.27374900157793663\n",
      "60 0.34963467612785076\n",
      "g is  0.19705345101568686\n",
      "80 0.3164430650591008\n",
      "g is  0.1520662581073288\n",
      "100 0.29806978903284465\n",
      "g is  0.1246691587340974\n",
      "120 0.28645701727737355\n",
      "g is  0.10708327730281553\n",
      "140 0.27828270048959824\n",
      "g is  0.0951616548780783\n",
      "160 0.2720471877991603\n",
      "g is  0.08667637394550569\n",
      "180 0.267003523575385\n",
      "g is  0.0803790056156557\n",
      "200 0.2627450928661829\n",
      "g is  0.07553128218440723\n",
      "220 0.2590341146381136\n",
      "g is  0.07167612153918686\n",
      "240 0.25572348034430886\n",
      "g is  0.06852134327844384\n",
      "260 0.25271782372165735\n",
      "g is  0.06587604779921123\n",
      "280 0.24995267244593278\n",
      "g is  0.0636132243450173\n",
      "300 0.24738267928671973\n",
      "g is  0.06164670563794344\n",
      "320 0.24497472885884963\n",
      "g is  0.059916630194473464\n",
      "340 0.24270378778294804\n",
      "g is  0.05838019895847274\n",
      "360 0.24055034711585827\n",
      "g is  0.057005793927959895\n",
      "380 0.23849880887178432\n",
      "g is  0.0557692403559236\n",
      "400 0.23653644176515284\n",
      "g is  0.05465143130493253\n",
      "420 0.23465268559614136\n",
      "g is  0.05363681330219428\n",
      "440 0.23283867287221252\n",
      "g is  0.052712413341053595\n",
      "460 0.23108688854759973\n",
      "g is  0.05186720482402571\n",
      "480 0.2293909197184222\n",
      "Training time for GD:  4313.2904233932495\n",
      "PGD\tinitial loss is : 14.976088889930564\n",
      "g is  16.902111105984616\n",
      "0 14.903968451727438\n",
      "g is  74.52755120814173\n",
      "20 265.7555764441367\n",
      "g is  97.51910997086121\n",
      "40 192.58034348962323\n",
      "g is  70.54466461881839\n",
      "60 116.03804577021428\n",
      "g is  5770.046831012161\n",
      "80 1934.5588310306277\n",
      "g is  77.46699360761666\n",
      "100 307.4358992079467\n",
      "g is  92.17509590799915\n",
      "120 228.31374777606575\n",
      "g is  91.43761569402501\n",
      "140 259.711987570565\n",
      "g is  105.82745730752087\n",
      "160 315.06058370475137\n",
      "g is  104.35832427042531\n",
      "180 316.9776915363902\n",
      "g is  125.73133089877199\n",
      "200 357.9753776202876\n",
      "g is  122.3231811318012\n",
      "220 312.1523172075835\n",
      "g is  90.83050122816171\n",
      "240 368.7177260786823\n",
      "g is  158.45802400393146\n",
      "260 828.2574550421406\n",
      "g is  162.87772804662225\n",
      "280 603.2620228072905\n",
      "g is  137.61401782585452\n",
      "300 582.1104602258191\n",
      "g is  161.29427643781938\n",
      "320 640.7548761698217\n",
      "g is  495.56084153571175\n",
      "340 3247.7358471575308\n",
      "g is  134.1838592742636\n",
      "360 642.647497756022\n",
      "g is  104.8328365875302\n",
      "380 468.2605457199003\n",
      "g is  226.57919901259024\n",
      "400 1148.1043559913376\n",
      "g is  71.65324567638217\n",
      "420 327.05877935585914\n",
      "g is  84.25520215927313\n",
      "440 317.83484705255273\n",
      "g is  71.60791673234336\n",
      "460 329.3451614501783\n",
      "g is  77.37309657401781\n",
      "480 297.5771268790255\n",
      "Training time for PGD:  4316.896549463272\n",
      "SGD\tinitial loss is : 14.976088889930564\n",
      "g is  16.53875614929473\n",
      "0 4.998964355108093\n",
      "g is  0.6441768715248907\n",
      "20 0.5811650348236562\n",
      "g is  0.4229379938412832\n",
      "40 0.4233546305351019\n",
      "g is  0.2708635296155382\n",
      "60 0.3513370741875641\n",
      "g is  0.6849136579856628\n",
      "80 0.32209736390127996\n",
      "g is  0.42656100985562767\n",
      "100 0.29545682102067483\n",
      "g is  0.33133003818953627\n",
      "120 0.2834307103282023\n",
      "g is  0.14448753542624648\n",
      "140 0.27615080325611463\n",
      "g is  0.21438596200386306\n",
      "160 0.2703655948221144\n",
      "g is  0.5149273202037712\n",
      "180 0.2693479440196854\n",
      "g is  0.25765399051947313\n",
      "200 0.26189013164290104\n",
      "g is  0.35416617700096675\n",
      "220 0.2582806706772174\n",
      "g is  0.6533347797807018\n",
      "240 0.25537594089833654\n",
      "g is  0.15986829536343075\n",
      "260 0.2515776503144743\n",
      "g is  0.5656540533516904\n",
      "280 0.2490830116762746\n",
      "g is  0.10209072968453498\n",
      "300 0.24720525706881621\n",
      "g is  0.3495868256130388\n",
      "320 0.24478077459268005\n",
      "g is  0.5328849587330791\n",
      "340 0.24444223417934322\n",
      "g is  0.35395053313740893\n",
      "360 0.24065384653794836\n",
      "g is  0.2151627647463031\n",
      "380 0.24040519166139376\n",
      "g is  0.11643718883069047\n",
      "400 0.23727638802532633\n",
      "g is  0.12907991731302546\n",
      "420 0.23464324587016464\n",
      "g is  0.1847080941882886\n",
      "440 0.2329242674882947\n",
      "g is  0.13811851004592768\n",
      "460 0.23175624349947863\n",
      "g is  0.21776151439557745\n",
      "480 0.23006797426385428\n",
      "Training time for SGD:  1240.1058201789856\n",
      "SVRG\tinitial loss is : 14.976088889930564\n",
      "g is  16.902111105984616\n",
      "0 0.5961366644606663\n",
      "g is  0.055921422051080215\n",
      "20 0.23495358206306116\n",
      "converge, break! current i:  27\n",
      "Training time for SVRG:  455.84049797058105\n",
      "BCD\tinitial loss is : 14.976088889930564\n",
      "g is  16.902111105984616\n",
      "0 10.457152098414797\n",
      "g is  0.8544943094630345\n",
      "20 0.7722265294968187\n",
      "g is  0.6548510595214948\n",
      "40 0.5974694775342958\n",
      "g is  0.5178138371779711\n",
      "60 0.49183538671334043\n",
      "g is  0.41264525508929956\n",
      "80 0.4241055874388187\n",
      "g is  0.3352422770408399\n",
      "100 0.3813510917128957\n",
      "g is  0.2766990546612779\n",
      "120 0.3525350545486371\n",
      "g is  0.23131264374100227\n",
      "140 0.33194372871994177\n",
      "g is  0.19825478182159162\n",
      "160 0.3178082047538699\n",
      "g is  0.17241562450479442\n",
      "180 0.30696641890601506\n",
      "g is  0.15254955135113507\n",
      "200 0.2987212869319768\n",
      "g is  0.13744470005578546\n",
      "220 0.29224898278453737\n",
      "g is  0.12559037050709052\n",
      "240 0.2869829915017349\n",
      "g is  0.11605507092505873\n",
      "260 0.28257167833849856\n",
      "g is  0.10806191009512789\n",
      "280 0.27871050477347054\n",
      "g is  0.10162171047685797\n",
      "300 0.2753726482898192\n",
      "g is  0.09632088741884699\n",
      "320 0.27239872595287273\n",
      "g is  0.09150941329855351\n",
      "340 0.2696094263029788\n",
      "g is  0.08744180259652701\n",
      "360 0.26707759475521964\n",
      "g is  0.0841581239080257\n",
      "380 0.2648205723733351\n",
      "g is  0.08136698012427317\n",
      "400 0.2627582401276713\n",
      "g is  0.0787296356292897\n",
      "420 0.2607156680459836\n",
      "g is  0.07659876185511573\n",
      "440 0.2588439147458162\n",
      "g is  0.07443674379880022\n",
      "460 0.2571064392779165\n",
      "g is  0.07255624636647934\n",
      "480 0.25540479506562197\n",
      "Training time for BCD:  8170.833303451538\n"
     ]
    }
   ],
   "source": [
    "# Should be a hyperparameter that you tune, not an argument - Fill in the values\n",
    "lmbda =0.01\n",
    "w_size = 50\n",
    "lr = 0.01\n",
    "iterations = 500 # 100\n",
    "T = 2000\n",
    "batch_size = 100\n",
    "epsilon = 0.05\n",
    "\n",
    "# Initialize weights\n",
    "w_1 = initialize_w(X_train.shape[1], w_size)\n",
    "\n",
    "w_2 = initialize_w(w_size,w_size)\n",
    "\n",
    "w_3 = initialize_w(w_size, 1)\n",
    "\n",
    "print(\"GD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_gd = time.time()\n",
    "w_1_star,w_2_star,w_3_star = GD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations)\n",
    "end_gd = time.time()\n",
    "print(\"Training time for GD: \", end_gd-start_gd)\n",
    "\n",
    "print(\"PGD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_pgd = time.time()\n",
    "w_1_star,w_2_star,w_3_star = PGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations)\n",
    "end_pgd = time.time()\n",
    "print(\"Training time for PGD: \", end_pgd-start_pgd)\n",
    "\n",
    "\n",
    "print(\"SGD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_sgd = time.time()\n",
    "w_1_star,w_2_star,w_3_star = SGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, batch_size, iterations)\n",
    "end_sgd = time.time()\n",
    "print(\"Training time for SGD: \", end_sgd-start_sgd)\n",
    "\n",
    "\n",
    "\n",
    "print(\"SVRG\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_svrg = time.time()\n",
    "w_1_star1,w_2_star1,w_3_star1 = SVRG(X_train, y_train, w_1, w_2, w_3, lmbda, lr, T, batch_size, iterations)\n",
    "end_svrg = time.time()\n",
    "print(\"Training time for SVRG: \", end_svrg-start_svrg)\n",
    "\n",
    "print(\"BCD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "strategy = 0 # 0 for random block mask, 1 for layer block\n",
    "blockrate = 0.5\n",
    "start_bcd = time.time()\n",
    "w_1_star,w_2_star,w_3_star = BCD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations, strategy, blockrate)\n",
    "end_bcd = time.time()\n",
    "print(\"Training time for BCD: \", end_bcd-start_bcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be a hyperparameter that you tune, not an argument - Fill in the values\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lambda', type=float, default=0., dest='lmbda') \n",
    "parser.add_argument('--w_size', type=int, default=10, dest='w_size')\n",
    "parser.add_argument('--lr', type=float, default=0.01)\n",
    "parser.add_argument('--iterations', type=int, default=10)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# Initialize weights\n",
    "w_1 = initialize_w(X_train.shape[1], args.w_size)\n",
    "\n",
    "w_2 = initialize_w(args.w_size,args.w_size)\n",
    "\n",
    "w_3 = initialize_w(args.w_size, 1)\n",
    "\n",
    "# Get iterations\n",
    "iterations = args.iterations\n",
    "# Define plotting variables\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "# Define the optimizers for the loop\n",
    "optimizers = [\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": SGD(X_train, y_train, w_1, w_2, w_3, args.lmbda, args.lr, batch_size),\n",
    "            \"name\": \"SGD\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": SVRG(X_train, y_train, w_1, w_2, w_3, args.lmbda, args.lr),\n",
    "            \"name\": \"SVRG\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": GD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations),\n",
    "            \"name\": \"GD\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": PGD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations, noise=),\n",
    "            \"name\": \"PGD\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": BCD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations),\n",
    "            \"name\": \"BCD\",\n",
    "            \"inner\": # Fill in\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the iterates over the algorithms above\n",
    "\n",
    "for opt in optimizers:\n",
    "    #\n",
    "    # Fill in\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "ax[0].legend(loc=\"upper right\")\n",
    "ax[0].set_xlabel(r\"Iteration\", fontsize=16)\n",
    "ax[0].set_ylabel(\"Loss\", fontsize=16)\n",
    "ax[0].set_title(\"CA3 - Training a deep neural network for the power consumption Dataset\")\n",
    "ax[0].set_ylim(ymin=0)\n",
    "\n",
    "ax[1].legend(loc=\"upper right\")\n",
    "ax[1].set_xlabel(r\"Time [s]\", fontsize=16)\n",
    "ax[1].set_ylabel(\"Loss\", fontsize=16)\n",
    "ax[1].set_ylim(ymin=0)\n",
    "\n",
    "plt.savefig(\"power.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlon",
   "language": "python",
   "name": "mlon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
