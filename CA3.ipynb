{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing of data\n",
    "# Function to load data\n",
    "\n",
    "def get_power_data():\n",
    "    \"\"\"\n",
    "    Read the Individual household electric power consumption dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assume that the dataset is located on folder \"data\"\n",
    "    data = pd.read_csv('data/household_power_consumption.txt',\n",
    "                       sep=';', low_memory=False)\n",
    "\n",
    "    # Drop some non-predictive variables\n",
    "    data = data.drop(columns=['Date', 'Time'], axis=1)\n",
    "\n",
    "    #print(data.head())\n",
    "\n",
    "    # Replace missing values\n",
    "    data = data.replace('?', np.nan)\n",
    "\n",
    "    # Drop NA\n",
    "    data = data.dropna(axis=0)\n",
    "\n",
    "    # Normalize\n",
    "    standard_scaler = preprocessing.StandardScaler()\n",
    "    np_scaled = standard_scaler.fit_transform(data)\n",
    "    data = pd.DataFrame(np_scaled)\n",
    "\n",
    "    # Goal variable assumed to be the first\n",
    "    X = data.values[:, 1:].astype('float32')\n",
    "    y = data.values[:, 0].astype('float32')\n",
    "\n",
    "    # Create categorical y for binary classification with balanced classes\n",
    "    y = np.sign(y+0.46)\n",
    "\n",
    "    # Split train and test data here: (X_train, Y_train, X_test, Y_test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    no_class = 2                 #binary classification\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, no_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "X,y types: <class 'numpy.ndarray'> <class 'numpy.ndarray'>\nX size (1536960, 6)\nY size (1536960,)\n"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, no_class = get_power_data()\n",
    "print(\"X,y types: {} {}\".format(type(X_train), type(y_train)))\n",
    "print(\"X size {}\".format(X_train.shape))\n",
    "print(\"Y size {}\".format(y_train.shape))\n",
    "\n",
    "# Create a binary variable from one of the columns.\n",
    "# You can use this OR not\n",
    "\n",
    "idx = y_train >= 0\n",
    "notidx = y_train < 0\n",
    "y_train[idx] = 1\n",
    "y_train[notidx] = -1\n",
    "\n",
    "\n",
    "# X_test = X_test/np.linalg.norm(X_test)\n",
    "# X_train = X_train/np.linalg.norm(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_train.shape)\n",
    "# #print(X_train.min())\n",
    "# print(X_test.max(), X_test.min())\n",
    "\n",
    "\n",
    "# print(X_test.max(), X_test.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x, derivative=False):\n",
    "    sigm = 1. / (1. + np.exp(-x)) \n",
    "    if derivative:\n",
    "        return sigm * (1. - sigm)\n",
    "    return sigm\n",
    "\n",
    "# Define weights initialization\n",
    "def initialize_w(N, d):\n",
    "    return 2*np.random.random((N,d)) - 1\n",
    "\n",
    "# Fill in feed forward propagation\n",
    "def feed_forward_propagation(X, y, w_1, w_2, w_3, lmbda):\n",
    "    # Fill in\n",
    "    # X (N,d)\n",
    "    # w_1 (d,h)\n",
    "    # w_2 (h,h)\n",
    "    # w_3 (h,1)\n",
    "    # y = y.reshape((-1,1))\n",
    "\n",
    "    layer_0 = X\n",
    "    layer_1 = sigmoid(np.dot(X, w_1)) # (N,h)\n",
    "    layer_2 = sigmoid(np.dot(layer_1, w_2)) # (N,h)\n",
    "    layer_3 = np.dot(layer_2, w_3) # (N,1)\n",
    "\n",
    "    # np.linalg.norm(np.dot(np.dot(sigmoid(np.dot(X, w_1)),w_2),w_3) - y.reshape((-1,1)))**2/X.shape[0]\n",
    "    \n",
    "    return layer_0, layer_1, layer_2, layer_3\n",
    "    \n",
    "# Fill in backpropagation    \n",
    "def back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3):\n",
    "    y = y.reshape((-1,1))\n",
    "    N = layer_0.shape[0]\n",
    "    # Calculate the gradient here\n",
    "    # print(\"w_1 shape: \",w_1.shape)\n",
    "    # print(\"layer_0 shape: \",layer_0.shape)\n",
    "    dl_dl3 = 2 * (layer_3-y) # (N,1)\n",
    "    dl3_dw3 = layer_2 # (N, h)\n",
    "    dl3_dl2 = w_3 # (h,1)\n",
    "    dl2_dw2 = -sigmoid(np.dot(layer_1, w_2), derivative=True) * layer_1 # (N,h)\n",
    "    # print(\"dl2_dw2.shape: \", dl2_dw2.shape)\n",
    "\n",
    "    dl2_dl1 = -np.dot(sigmoid(np.dot(layer_1, w_2), derivative=True), w_2) # (N,h)\n",
    "    # print(\"dl2_dl1.shape: \", dl2_dl1.shape)\n",
    "    \n",
    "    # (N,h) (N,d)\n",
    "    dl1_dw1 = -np.dot(sigmoid(np.dot(layer_0, w_1), derivative=True).T, layer_0) # (h,d)\n",
    "    # print(\"dl1_dw1.shape: \", dl1_dw1.shape)\n",
    "\n",
    "    # TODO\n",
    "    # gradient w.r.t w_3 # (h,1)\n",
    "    layer_3_delta = np.dot(dl3_dw3.T, dl_dl3)/N\n",
    "\n",
    "    # gradient w.r.t w_2 # (h,h)\n",
    "    layer_2_delta = np.dot(np.dot(dl3_dl2,dl_dl3.T), dl2_dw2)/N\n",
    "\n",
    "    # gradient w.r.t w_1\n",
    "\n",
    "    # (d,h) = (N,1) (h,1) (N,h) (h,d)\n",
    "    # layer_1_delta = dl_dl3 * dl3_dl2 * dl2_dl1 *dl1_dw1\n",
    "    layer_1_delta = np.dot(dl1_dw1.T, np.dot(dl2_dl1.T, np.dot(dl_dl3, dl3_dl2.T)))/N\n",
    "    return layer_1_delta, layer_2_delta, layer_3_delta\n",
    "\n",
    "# Cost function\n",
    "def cost(X, y, w_1, w_2, w_3, lmbda):\n",
    "    N, d = X.shape\n",
    "    a1,a2,a3,a4 = feed_forward_propagation(X,y,w_1,w_2,w_3,lmbda)\n",
    "\n",
    "    return np.linalg.norm(a4[:,0] - y,2) ** 2 / N\n",
    "\n",
    "# Define SGD\n",
    "def SGD(X, y, w_1, w_2, w_3, lmbda, learning_rate, batch_size):\n",
    "    # Complete here:\n",
    "    # y = y.reshape((-1,1))\n",
    "    randomInd = np.arange(X.shape[0])[:batch_size]\n",
    "    layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X[randomInd,:],y[randomInd,:],w_1,w_2,w_3,lmbda)\n",
    "    layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y[randomInd,:], w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3)\n",
    "    w_1 = w_1 - learning_rate*layer_1_delta\n",
    "    w_2 = w_2 - learning_rate*layer_2_delta\n",
    "    w_3 = w_3 - learning_rate*layer_3_delta\n",
    "\n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define SVRG here:\n",
    "def SVRG(X, y, w_1, w_2, w_3, lmbda, learning_rate, T, batch_size):\n",
    "    # Complete here:\n",
    "    y = y.reshape((-1,1))\n",
    "    N = X.shape[0]\n",
    "\n",
    "    # compute all gradient and store\n",
    "    layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X,y,w_1,w_2,w_3,lmbda)\n",
    "    layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3)\n",
    "        \n",
    "    # g = function_gradient_vectorization(x, y, w, lambda_, gradclip)\n",
    "    \n",
    "    # initialize the w_previous\n",
    "    # w_previous = w.copy()\n",
    "    w_1_previous, w_2_previous, w_3_previous = w_1.copy(), w_2.copy(), w_3.copy()\n",
    "    for t in range(T//batch_size):\n",
    "        # random sample\n",
    "        # randomInd = int(np.random.rand() * N)\n",
    "        randomInd = np.arange(N)\n",
    "        np.random.shuffle(randomInd)\n",
    "        randomInd = randomInd[:batch_size]\n",
    "        # randomInd = np.random.randint(0,N)\n",
    "        layer_0_p1, layer_1_p1, layer_2_p1, layer_3_p1 = feed_forward_propagation(X[randomInd,:],y[randomInd,:],w_1_previous,w_2_previous,w_3_previous,lmbda)\n",
    "        layer_1_delta_p1, layer_2_delta_p1, layer_3_delta_p1 = back_propagation(y[randomInd,:], w_1_previous, w_2_previous, w_3_previous, layer_0_p1, layer_1_p1, layer_2_p1, layer_3_p1)\n",
    "\n",
    "        layer_0_p2, layer_1_p2, layer_2_p2, layer_3_p2 = feed_forward_propagation(X[randomInd,:],y[randomInd,:],w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta_p2, layer_2_delta_p2, layer_3_delta_p2 = back_propagation(y[randomInd,:], w_1, w_2, w_3, layer_0_p2, layer_1_p2, layer_2_p2, layer_3_p2)\n",
    "        \n",
    "        # calculate the update term\n",
    "        # part1 = function_gradient_vectorization(x[:,randomInd], y[:,randomInd], w_previous, lambda_, gradclip = gradclip)\n",
    "        # part2 = function_gradient_vectorization(x[:,randomInd], y[:,randomInd], w, lambda_, gradclip = gradclip)\n",
    "        # part3 = g\n",
    "\n",
    "        w_1_previous = w_1_previous - learning_rate * (layer_1_delta_p1 - layer_1_delta_p2 + layer_1_delta)\n",
    "        w_2_previous = w_2_previous - learning_rate * (layer_2_delta_p1 - layer_2_delta_p2 + layer_2_delta)\n",
    "        w_3_previous = w_3_previous - learning_rate * (layer_3_delta_p1 - layer_3_delta_p2 + layer_3_delta)\n",
    "\n",
    "        # w_previous = w_previous - alpha * (part1 - part2 + part3)\n",
    "        \n",
    "    # w = w_previous\n",
    "    w_1, w_2, w_3 = w_1_previous, w_2_previous, w_3_previous\n",
    "\n",
    "    \n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define GD here:\n",
    "def GD(X, y, w_1,w_2,w_3, learning_rate, lmbda, iterations):\n",
    "    # Complete here:\n",
    "    layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X,y,w_1,w_2,w_3,lmbda)\n",
    "    layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3)\n",
    "    w_1 = w_1 - learning_rate*layer_1_delta\n",
    "    w_2 = w_2 - learning_rate*layer_2_delta\n",
    "    w_3 = w_3 - learning_rate*layer_3_delta\n",
    "    \n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define projected GD here:\n",
    "def PGD(X, y, w_1,w_2,w_3, learning_rate, lmbda, iterations, noise):\n",
    "    # Complete here:\n",
    "    \n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define BCD here:\n",
    "def BCD(X, y, w_1,w_2,w_3, learning_rate, lmbda, iterations):\n",
    "    # Complete here:\n",
    "    \n",
    "    return w_1, w_2, w_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_size = 10\n",
    "\n",
    "# # Initialize weights\n",
    "# w_1 = initialize_w(X_train.shape[1], w_size)\n",
    "\n",
    "# w_2 = initialize_w(w_size,w_size)\n",
    "\n",
    "# w_3 = initialize_w(w_size, 1)\n",
    "\n",
    "# lmbda = 0\n",
    "\n",
    "# layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X_test, y_test, w_1, w_2, w_3, lmbda)\n",
    "\n",
    "# print(layer_0.shape)\n",
    "# print(layer_1.shape)\n",
    "# print(layer_2.shape)\n",
    "# print(layer_3.shape)\n",
    "\n",
    "# layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y_test, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3)\n",
    "# # print(layer_1_delta.shape)\n",
    "# print(\"layer_3_delta shape\", layer_3_delta.shape)\n",
    "# print(\"layer_2_delta shape\", layer_2_delta.shape)\n",
    "# print(\"layer_1_delta shape\", layer_1_delta.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "SVRG\tinitial loss is : 2.110085126794523\n<ipython-input-5-f7976efc64de>:3: RuntimeWarning: overflow encountered in exp\n  sigm = 1. / (1. + np.exp(-x))\n0 0.40845452413559913\n1 0.4051852536128354\n2 0.4046117444497941\n3 0.4044774968702398\n4 0.4044507897522062\n"
    }
   ],
   "source": [
    "# Should be a hyperparameter that you tune, not an argument - Fill in the values\n",
    "lmbda =0.\n",
    "w_size = 40\n",
    "lr = 0.02\n",
    "iterations = 5\n",
    "T = 2000\n",
    "batch_size = 100\n",
    "\n",
    "# Initialize weights\n",
    "w_1 = initialize_w(X_train.shape[1], w_size)\n",
    "\n",
    "w_2 = initialize_w(w_size,w_size)\n",
    "\n",
    "w_3 = initialize_w(w_size, 1)\n",
    "\n",
    "print(\"SVRG\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "# print(\"SVRG\")\n",
    "for i in range(iterations):\n",
    "    # w_1,w_2,w_3 = SGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, batch_size)\n",
    "    w_1,w_2,w_3 = SVRG(X_train, y_train, w_1, w_2, w_3, lmbda, lr, T, batch_size)\n",
    "\n",
    "    loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "    # if i%10==0:\n",
    "    print(i,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be a hyperparameter that you tune, not an argument - Fill in the values\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lambda', type=float, default=0., dest='lmbda') \n",
    "parser.add_argument('--w_size', type=int, default=10, dest='w_size')\n",
    "parser.add_argument('--lr', type=float, default=0.01)\n",
    "parser.add_argument('--iterations', type=int, default=10)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# Initialize weights\n",
    "w_1 = initialize_w(X_train.shape[1], args.w_size)\n",
    "\n",
    "w_2 = initialize_w(args.w_size,args.w_size)\n",
    "\n",
    "w_3 = initialize_w(args.w_size, 1)\n",
    "\n",
    "# Get iterations\n",
    "iterations = args.iterations\n",
    "# Define plotting variables\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "# Define the optimizers for the loop\n",
    "optimizers = [\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": SGD(X_train, y_train, w_1, w_2, w_3, args.lmbda, args.lr, batch_size),\n",
    "            \"name\": \"SGD\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": SVRG(X_train, y_train, w_1, w_2, w_3, args.lmbda, args.lr),\n",
    "            \"name\": \"SVRG\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": GD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations),\n",
    "            \"name\": \"GD\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": PGD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations, noise=),\n",
    "            \"name\": \"PGD\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": BCD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations),\n",
    "            \"name\": \"BCD\",\n",
    "            \"inner\": # Fill in\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the iterates over the algorithms above\n",
    "\n",
    "for opt in optimizers:\n",
    "    #\n",
    "    # Fill in\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "ax[0].legend(loc=\"upper right\")\n",
    "ax[0].set_xlabel(r\"Iteration\", fontsize=16)\n",
    "ax[0].set_ylabel(\"Loss\", fontsize=16)\n",
    "ax[0].set_title(\"CA3 - Training a deep neural network for the power consumption Dataset\")\n",
    "ax[0].set_ylim(ymin=0)\n",
    "\n",
    "ax[1].legend(loc=\"upper right\")\n",
    "ax[1].set_xlabel(r\"Time [s]\", fontsize=16)\n",
    "ax[1].set_ylabel(\"Loss\", fontsize=16)\n",
    "ax[1].set_ylim(ymin=0)\n",
    "\n",
    "plt.savefig(\"power.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('sklearn': conda)",
   "language": "python",
   "name": "python38164bitsklearnconda11bdb1c82f4e41ea847f64346c43e799"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}