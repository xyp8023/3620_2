{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing of data\n",
    "# Function to load data\n",
    "\n",
    "def get_power_data():\n",
    "    \"\"\"\n",
    "    Read the Individual household electric power consumption dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assume that the dataset is located on folder \"data\"\n",
    "    data = pd.read_csv('data/household_power_consumption.txt',\n",
    "                       sep=';', low_memory=False)\n",
    "\n",
    "    # Drop some non-predictive variables\n",
    "    data = data.drop(columns=['Date', 'Time'], axis=1)\n",
    "\n",
    "    #print(data.head())\n",
    "\n",
    "    # Replace missing values\n",
    "    data = data.replace('?', np.nan)\n",
    "\n",
    "    # Drop NA\n",
    "    data = data.dropna(axis=0)\n",
    "\n",
    "    # Normalize\n",
    "    standard_scaler = preprocessing.StandardScaler()\n",
    "    np_scaled = standard_scaler.fit_transform(data)\n",
    "    data = pd.DataFrame(np_scaled)\n",
    "\n",
    "    # Goal variable assumed to be the first\n",
    "    X = data.values[:, 1:].astype('float32')\n",
    "    y = data.values[:, 0].astype('float32')\n",
    "\n",
    "    # Create categorical y for binary classification with balanced classes\n",
    "    y = np.sign(y+0.46)\n",
    "\n",
    "    # Split train and test data here: (X_train, Y_train, X_test, Y_test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    no_class = 2                 #binary classification\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, no_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "X,y types: <class 'numpy.ndarray'> <class 'numpy.ndarray'>\nX size (1536960, 6)\nY size (1536960,)\n"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, no_class = get_power_data()\n",
    "print(\"X,y types: {} {}\".format(type(X_train), type(y_train)))\n",
    "print(\"X size {}\".format(X_train.shape))\n",
    "print(\"Y size {}\".format(y_train.shape))\n",
    "\n",
    "# Create a binary variable from one of the columns.\n",
    "# You can use this OR not\n",
    "\n",
    "idx = y_train >= 0\n",
    "notidx = y_train < 0\n",
    "y_train[idx] = 1\n",
    "y_train[notidx] = -1\n",
    "\n",
    "\n",
    "# X_test = X_test/np.linalg.norm(X_test)\n",
    "# X_train = X_train/np.linalg.norm(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_train.shape)\n",
    "# #print(X_train.min())\n",
    "# print(X_test.max(), X_test.min())\n",
    "\n",
    "\n",
    "# print(X_test.max(), X_test.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x, derivative=False):\n",
    "    sigm = 1. / (1. + np.exp(-x)) \n",
    "    if derivative:\n",
    "        return sigm * (1. - sigm)\n",
    "    return sigm\n",
    "\n",
    "# Define weights initialization\n",
    "def initialize_w(N, d):\n",
    "    return 2*np.random.random((N,d)) - 1\n",
    "\n",
    "# Fill in feed forward propagation\n",
    "def feed_forward_propagation(X, y, w_1, w_2, w_3, lmbda):\n",
    "    # Fill in\n",
    "    # X (N,d)\n",
    "    # w_1 (d,h)\n",
    "    # w_2 (h,g)\n",
    "    # w_3 (g,1)\n",
    "    N,d = X.shape\n",
    "    layer_0 = X # (N,d)\n",
    "    layer_1 = sigmoid(np.dot(layer_0, w_1)) # (N, h)\n",
    "    layer_2 = sigmoid(np.dot(layer_1, w_2)) # (N, g)\n",
    "    layer_3 = np.dot(layer_2, w_3) # (N, 1)\n",
    "    \n",
    "    return layer_0, layer_1, layer_2, layer_3\n",
    "def back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda):\n",
    "    N = y.shape[0]\n",
    "    y = y.reshape((-1,1))\n",
    "    layer_3_delta = np.zeros_like(w_3) # (g,1)\n",
    "    layer_2_delta = np.zeros_like(w_2) # (h, g)\n",
    "    layer_1_delta = np.zeros_like(w_1) # (d, h)\n",
    "    layer_3_delta = 2 * np.dot(layer_2.T, (layer_3 - y)) # (g,1)\n",
    "    # print(\"np.dot(w_3,(layer_3-y)) shape \", np.dot(w_3,(layer_3-y).T).shape)\n",
    "    # print(\" sigmoid(np.dot(layer_1,w_2), derivative=True).T shape\", sigmoid(np.dot(layer_1,w_2), derivative=True).T.shape)\n",
    "    dJ_dl2 = 2 * np.dot(w_3,(layer_3-y).T) # # (g,N)\n",
    "    dl2_ds2 = sigmoid(np.dot(layer_1,w_2), derivative=True).T # (g,N)\n",
    "\n",
    "    layer_2_delta  = np.dot(dJ_dl2 * dl2_ds2, layer_1).T    \n",
    "    # layer_2_delta = 2 * np.dot(np.dot(w_3,(layer_3-y).T)*sigmoid(np.dot(layer_1,w_2), derivative=True).T, layer_1).T\n",
    "    ds2_dl1 = w_2 # (h,g)\n",
    "    dl1_ds1 =  sigmoid(np.dot(layer_0,w_1), derivative=True).T # (h,N)\n",
    "    ds1_dw1 = layer_0 # (N,d)\n",
    "\n",
    "    layer_1_delta = np.dot(np.dot(ds2_dl1, dJ_dl2 * dl2_ds2) * dl1_ds1, ds1_dw1).T # (d,h)\n",
    "    return layer_1_delta/N, layer_2_delta/N, layer_3_delta/N\n",
    "\n",
    "# Cost function\n",
    "def cost(X, y, w_1, w_2, w_3, lmbda):\n",
    "    N, d = X.shape\n",
    "    a1,a2,a3,a4 = feed_forward_propagation(X,y,w_1,w_2,w_3,lmbda)\n",
    "\n",
    "    return np.linalg.norm(a4[:,0] - y,2) ** 2 / N + lmbda * (np.linalg.norm(w_1)**2 + np.linalg.norm(w_2)**2 + np.linalg.norm(w_3)**2)\n",
    "\n",
    "# Define SGD\n",
    "def SGD(X, y, w_1, w_2, w_3, lmbda, learning_rate, batch_size):\n",
    "    # Complete here:\n",
    "    # y = y.reshape((-1,1))\n",
    "    randomInd = np.arange(X.shape[0])[:batch_size]\n",
    "    layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X[randomInd,:],y[randomInd,:],w_1,w_2,w_3,lmbda)\n",
    "    layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y[randomInd,:], w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda)\n",
    "    w_1 = w_1 - learning_rate*layer_1_delta\n",
    "    w_2 = w_2 - learning_rate*layer_2_delta\n",
    "    w_3 = w_3 - learning_rate*layer_3_delta\n",
    "\n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define SVRG here:\n",
    "def SVRG(X, y, w_1, w_2, w_3, lmbda, learning_rate, T, batch_size, iterations):\n",
    "    # Complete here:\n",
    "    y = y.reshape((-1,1))\n",
    "    N = X.shape[0]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # compute all gradient and store\n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X,y,w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda)\n",
    "            \n",
    "        # g = function_gradient_vectorization(x, y, w, lambda_, gradclip)\n",
    "        \n",
    "        # initialize the w_previous\n",
    "        # w_previous = w.copy()\n",
    "        w_1_previous, w_2_previous, w_3_previous = w_1.copy(), w_2.copy(), w_3.copy()\n",
    "        for t in range(T//batch_size):\n",
    "            # random sample\n",
    "            # randomInd = int(np.random.rand() * N)\n",
    "            randomInd = np.arange(N)\n",
    "            np.random.shuffle(randomInd)\n",
    "            randomInd = randomInd[:batch_size]\n",
    "            # randomInd = np.random.randint(0,N)\n",
    "            layer_0_p1, layer_1_p1, layer_2_p1, layer_3_p1 = feed_forward_propagation(X[randomInd,:],y[randomInd,:],w_1_previous,w_2_previous,w_3_previous,lmbda)\n",
    "            layer_1_delta_p1, layer_2_delta_p1, layer_3_delta_p1 = back_propagation(y[randomInd,:], w_1_previous, w_2_previous, w_3_previous, layer_0_p1, layer_1_p1, layer_2_p1, layer_3_p1, lmbda)\n",
    "\n",
    "            layer_0_p2, layer_1_p2, layer_2_p2, layer_3_p2 = feed_forward_propagation(X[randomInd,:],y[randomInd,:],w_1,w_2,w_3,lmbda)\n",
    "            layer_1_delta_p2, layer_2_delta_p2, layer_3_delta_p2 = back_propagation(y[randomInd,:], w_1, w_2, w_3, layer_0_p2, layer_1_p2, layer_2_p2, layer_3_p2, lmbda)\n",
    "            \n",
    "            # calculate the update term\n",
    "            # part1 = function_gradient_vectorization(x[:,randomInd], y[:,randomInd], w_previous, lambda_, gradclip = gradclip)\n",
    "            # part2 = function_gradient_vectorization(x[:,randomInd], y[:,randomInd], w, lambda_, gradclip = gradclip)\n",
    "            # part3 = g\n",
    "\n",
    "            w_1_previous = w_1_previous - learning_rate * (layer_1_delta_p1 - layer_1_delta_p2 + layer_1_delta)\n",
    "            w_2_previous = w_2_previous - learning_rate * (layer_2_delta_p1 - layer_2_delta_p2 + layer_2_delta)\n",
    "            w_3_previous = w_3_previous - learning_rate * (layer_3_delta_p1 - layer_3_delta_p2 + layer_3_delta)\n",
    "\n",
    "            # w_previous = w_previous - alpha * (part1 - part2 + part3)\n",
    "            \n",
    "        # w = w_previous\n",
    "        w_1, w_2, w_3 = w_1_previous, w_2_previous, w_3_previous\n",
    "        print(i,loss)\n",
    "    \n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define GD here:\n",
    "def GD(X, y, w_1,w_2,w_3, learning_rate, lmbda, iterations):\n",
    "    N = X.shape[0]\n",
    "    for i in range(iterations):\n",
    "\n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda)\n",
    "\n",
    "        w_1 = w_1 -  learning_rate * layer_1_delta + (lmbda / N * w_1)\n",
    "        w_2 = w_2 - learning_rate * layer_2_delta + (lmbda / N * w_2)\n",
    "        w_3 = w_3 - learning_rate * layer_3_delta + (lmbda / N * w_3)\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        print(i,loss)\n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define projected GD here:\n",
    "def PGD(X, y, w_1,w_2,w_3, learning_rate, lmbda, iterations, noise):\n",
    "    # Complete here:\n",
    "    \n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define BCD here:\n",
    "def BCD(X, y, w_1,w_2,w_3, learning_rate, lmbda, iterations):\n",
    "    # Complete here:\n",
    "    \n",
    "    return w_1, w_2, w_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "layer_0 shape  (512320, 6)\nlayer_1 shape  (512320, 10)\nlayer_2 shape  (512320, 11)\nlayer_3 shape  (512320, 1)\nw_3 shape  (11, 1)\nw_2 shape  (10, 11)\nw_1 shape  (6, 10)\nlayer_3_delta shape (11, 1)\nlayer_2_delta shape (10, 11)\nlayer_1_delta shape (6, 10)\n"
    }
   ],
   "source": [
    "w_size = 10\n",
    "\n",
    "# Initialize weights for debug\n",
    "w_1 = initialize_w(X_train.shape[1], w_size)\n",
    "\n",
    "w_2 = initialize_w(w_size,w_size+1)\n",
    "\n",
    "w_3 = initialize_w(w_size+1, 1)\n",
    "\n",
    "lmbda = 0\n",
    "\n",
    "layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X_test, y_test, w_1, w_2, w_3, lmbda)\n",
    "\n",
    "print(\"layer_0 shape \", layer_0.shape)\n",
    "print(\"layer_1 shape \",layer_1.shape)\n",
    "print(\"layer_2 shape \",layer_2.shape)\n",
    "print(\"layer_3 shape \",layer_3.shape)\n",
    "\n",
    "layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y_test, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda)\n",
    "\n",
    "\n",
    "print(\"w_3 shape \",w_3.shape)\n",
    "print(\"w_2 shape \",w_2.shape)\n",
    "print(\"w_1 shape \",w_1.shape)\n",
    "\n",
    "print(\"layer_3_delta shape\", layer_3_delta.shape)\n",
    "print(\"layer_2_delta shape\", layer_2_delta.shape)\n",
    "print(\"layer_1_delta shape\", layer_1_delta.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "GD\tinitial loss is : 2.176887181318325\n0 1.558352053446679\n1 1.5362190903458717\n2 1.521202612974463\n3 1.5073730339443716\n4 1.4945735354266714\n5 1.4827116508612987\n6 1.4717039930196636\n7 1.4614749291501168\n8 1.4519558476769736\n9 1.4430845001507904\n10 1.4348044060208522\n11 1.427064313716575\n12 1.4198177123841176\n13 1.4130223892939604\n14 1.4066400285012008\n15 1.4006358468240268\n16 1.3949782636222436\n17 1.3896386012188429\n18 1.3845908131229379\n19 1.3798112374895346\n20 1.3752783734966798\n21 1.3709726785385166\n22 1.3668763843275131\n23 1.362973330174002\n24 1.359248811868643\n25 1.355689444735821\n26 1.3522830395549483\n27 1.349018490163795\n28 1.3458856716645078\n29 1.3428753482499727\n30 1.339979089756624\n31 1.3371891961304403\n32 1.3344986290664504\n33 1.331900950149212\n34 1.3293902648830014\n35 1.3269611720563361\n36 1.3246087179364587\n37 1.3223283548358864\n38 1.3201159036355128\n39 1.3179675198873597\n40 1.3158796631552248\n41 1.3138490692834814\n42 1.3118727253133773\n43 1.3099478467926686\n44 1.308071857248455\n45 1.3062423696149417\n46 1.304457169427677\n47 1.302714199613814\n48 1.3010115467242567\n49 1.2993474284683575\n50 1.2977201824252225\n51 1.2961282558178386\n52 1.2945701962472143\n53 1.2930446432936842\n54 1.2915503209015315\n55 1.290086030471199\n56 1.2886506445907528\n57 1.2872431013448895\n58 1.28586239914579\n59 1.28450759203557\n60 1.2831777854149498\n61 1.281872132157229\n62 1.2805898290706197\n63 1.2793301136756035\n64 1.2780922612672567\n65 1.2768755822353843\n66 1.2756794196179908\n67 1.2745031468659824\n68 1.273346165799159\n69 1.2722079047354975\n70 1.2710878167774817\n71 1.2699853782408104\n72 1.2689000872122485\n73 1.267831462224661\n74 1.2667790410384385\n75 1.2657423795195577\n76 1.2647210506054747\n77 1.2637146433508781\n78 1.2627227620461245\n79 1.2617450254018339\n80 1.2607810657937764\n81 1.2598305285627256\n82 1.2588930713644622\n83 1.257968363565574\n84 1.2570560856811082\n85 1.2561559288504986\n86 1.2552675943485343\n87 1.2543907931284277\n88 1.2535252453943306\n89 1.2526706802008714\n90 1.2518268350775303\n91 1.250993455675851\n92 1.2501702954376923\n93 1.2493571152828575\n94 1.248553683314619\n95 1.2477597745417666\n96 1.2469751706159369\n97 1.246199659583099\n98 1.2454330356481533\n99 1.2446750989517075\nSVRG\tinitial loss is : 2.176887181318325\n"
    }
   ],
   "source": [
    "# Should be a hyperparameter that you tune, not an argument - Fill in the values\n",
    "lmbda =0.001\n",
    "w_size = 50\n",
    "lr = 0.02\n",
    "iterations = 100\n",
    "T = 2000\n",
    "batch_size = 100\n",
    "\n",
    "# Initialize weights\n",
    "w_1 = initialize_w(X_train.shape[1], w_size)\n",
    "\n",
    "w_2 = initialize_w(w_size,w_size)\n",
    "\n",
    "w_3 = initialize_w(w_size, 1)\n",
    "\n",
    "print(\"GD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "# for i in range(iterations):\n",
    "w_1_star,w_2_star,w_3_star = GD(X_train, y_train, w_1, w_2, w_3, lr, lmbda, iterations)\n",
    "    # w_1,w_2,w_3 = SVRG(X_train, y_train, w_1, w_2, w_3, lmbda, lr, T, batch_size)\n",
    "\n",
    "\n",
    "print(\"SVRG\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "w_1_star1,w_2_star1,w_3_star1 = SVRG(X_train, y_train, w_1, w_2, w_3, lmbda, lr, T, batch_size, iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be a hyperparameter that you tune, not an argument - Fill in the values\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lambda', type=float, default=0., dest='lmbda') \n",
    "parser.add_argument('--w_size', type=int, default=10, dest='w_size')\n",
    "parser.add_argument('--lr', type=float, default=0.01)\n",
    "parser.add_argument('--iterations', type=int, default=10)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# Initialize weights\n",
    "w_1 = initialize_w(X_train.shape[1], args.w_size)\n",
    "\n",
    "w_2 = initialize_w(args.w_size,args.w_size)\n",
    "\n",
    "w_3 = initialize_w(args.w_size, 1)\n",
    "\n",
    "# Get iterations\n",
    "iterations = args.iterations\n",
    "# Define plotting variables\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "# Define the optimizers for the loop\n",
    "optimizers = [\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": SGD(X_train, y_train, w_1, w_2, w_3, args.lmbda, args.lr, batch_size),\n",
    "            \"name\": \"SGD\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": SVRG(X_train, y_train, w_1, w_2, w_3, args.lmbda, args.lr),\n",
    "            \"name\": \"SVRG\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": GD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations),\n",
    "            \"name\": \"GD\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": PGD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations, noise=),\n",
    "            \"name\": \"PGD\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": BCD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations),\n",
    "            \"name\": \"BCD\",\n",
    "            \"inner\": # Fill in\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the iterates over the algorithms above\n",
    "\n",
    "for opt in optimizers:\n",
    "    #\n",
    "    # Fill in\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "ax[0].legend(loc=\"upper right\")\n",
    "ax[0].set_xlabel(r\"Iteration\", fontsize=16)\n",
    "ax[0].set_ylabel(\"Loss\", fontsize=16)\n",
    "ax[0].set_title(\"CA3 - Training a deep neural network for the power consumption Dataset\")\n",
    "ax[0].set_ylim(ymin=0)\n",
    "\n",
    "ax[1].legend(loc=\"upper right\")\n",
    "ax[1].set_xlabel(r\"Time [s]\", fontsize=16)\n",
    "ax[1].set_ylabel(\"Loss\", fontsize=16)\n",
    "ax[1].set_ylim(ymin=0)\n",
    "\n",
    "plt.savefig(\"power.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('sklearn': conda)",
   "language": "python",
   "name": "python38164bitsklearnconda11bdb1c82f4e41ea847f64346c43e799"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}