{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Data Analytics Acceleration Library (Intel(R) DAAL) solvers for sklearn enabled: https://intelpython.github.io/daal4py/sklearn.html\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing of data\n",
    "# Function to load data\n",
    "\n",
    "def get_power_data():\n",
    "    \"\"\"\n",
    "    Read the Individual household electric power consumption dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assume that the dataset is located on folder \"data\"\n",
    "    data = pd.read_csv('../../dataset/household_power_consumption.txt',\n",
    "                       sep=';', low_memory=False)\n",
    "\n",
    "    # Drop some non-predictive variables\n",
    "    data = data.drop(columns=['Date', 'Time'], axis=1)\n",
    "\n",
    "    #print(data.head())\n",
    "\n",
    "    # Replace missing values\n",
    "    data = data.replace('?', np.nan)\n",
    "\n",
    "    # Drop NA\n",
    "    data = data.dropna(axis=0)\n",
    "\n",
    "    # Normalize\n",
    "    standard_scaler = preprocessing.StandardScaler()\n",
    "    np_scaled = standard_scaler.fit_transform(data)\n",
    "    data = pd.DataFrame(np_scaled)\n",
    "\n",
    "    # Goal variable assumed to be the first\n",
    "    X = data.values[:, 1:].astype('float32')\n",
    "    y = data.values[:, 0].astype('float32')\n",
    "\n",
    "    # Create categorical y for binary classification with balanced classes\n",
    "    y = np.sign(y+0.46)\n",
    "\n",
    "    # Split train and test data here: (X_train, Y_train, X_test, Y_test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    no_class = 2                 #binary classification\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, no_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X,y types: <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "X size (1536960, 6)\n",
      "Y size (1536960,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, no_class = get_power_data()\n",
    "print(\"X,y types: {} {}\".format(type(X_train), type(y_train)))\n",
    "print(\"X size {}\".format(X_train.shape))\n",
    "print(\"Y size {}\".format(y_train.shape))\n",
    "\n",
    "# Create a binary variable from one of the columns.\n",
    "# You can use this OR not\n",
    "\n",
    "idx = y_train >= 0\n",
    "notidx = y_train < 0\n",
    "y_train[idx] = 1\n",
    "y_train[notidx] = -1\n",
    "\n",
    "\n",
    "# X_test = X_test/np.linalg.norm(X_test)\n",
    "# X_train = X_train/np.linalg.norm(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_train.shape)\n",
    "# #print(X_train.min())\n",
    "# print(X_test.max(), X_test.min())\n",
    "\n",
    "\n",
    "# print(X_test.max(), X_test.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x, derivative=False):\n",
    "    sigm = 1. / (1. + np.exp(-x)) \n",
    "    if derivative:\n",
    "        return sigm * (1. - sigm)\n",
    "    return sigm\n",
    "\n",
    "# Define weights initialization\n",
    "def initialize_w(N, d):\n",
    "    return 2*np.random.random((N,d)) - 1\n",
    "\n",
    "# Fill in feed forward propagation\n",
    "def feed_forward_propagation(X, y, w_1, w_2, w_3, lmbda):\n",
    "    # Fill in\n",
    "    # X (N,d)\n",
    "    # w_1 (d,h)\n",
    "    # w_2 (h,g)\n",
    "    # w_3 (g,1)\n",
    "    N,d = X.shape\n",
    "    layer_0 = X # (N,d)\n",
    "    layer_1 = sigmoid(np.dot(layer_0, w_1)) # (N, h)\n",
    "    layer_2 = sigmoid(np.dot(layer_1, w_2)) # (N, g)\n",
    "    layer_3 = np.dot(layer_2, w_3) # (N, 1)\n",
    "    \n",
    "    return layer_0, layer_1, layer_2, layer_3\n",
    "def back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda):\n",
    "    N = y.shape[0]\n",
    "    y = y.reshape((-1,1))\n",
    "    layer_3_delta = np.zeros_like(w_3) # (g,1)\n",
    "    layer_2_delta = np.zeros_like(w_2) # (h, g)\n",
    "    layer_1_delta = np.zeros_like(w_1) # (d, h)\n",
    "    layer_3_delta = 2 * np.dot(layer_2.T, (layer_3 - y)) # (g,1)\n",
    "    # print(\"np.dot(w_3,(layer_3-y)) shape \", np.dot(w_3,(layer_3-y).T).shape)\n",
    "    # print(\" sigmoid(np.dot(layer_1,w_2), derivative=True).T shape\", sigmoid(np.dot(layer_1,w_2), derivative=True).T.shape)\n",
    "    dJ_dl2 = 2 * np.dot(w_3,(layer_3-y).T) # # (g,N)\n",
    "    dl2_ds2 = sigmoid(np.dot(layer_1,w_2), derivative=True).T # (g,N)\n",
    "\n",
    "    layer_2_delta  = np.dot(dJ_dl2 * dl2_ds2, layer_1).T    \n",
    "    # layer_2_delta = 2 * np.dot(np.dot(w_3,(layer_3-y).T)*sigmoid(np.dot(layer_1,w_2), derivative=True).T, layer_1).T\n",
    "    ds2_dl1 = w_2 # (h,g)\n",
    "    dl1_ds1 =  sigmoid(np.dot(layer_0,w_1), derivative=True).T # (h,N)\n",
    "    ds1_dw1 = layer_0 # (N,d)\n",
    "\n",
    "    layer_1_delta = np.dot(np.dot(ds2_dl1, dJ_dl2 * dl2_ds2) * dl1_ds1, ds1_dw1).T # (d,h)\n",
    "    return layer_1_delta/N, layer_2_delta/N, layer_3_delta/N\n",
    "\n",
    "def back_propagation_blocklayer(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda, activelayer = 3):\n",
    "    N = y.shape[0]\n",
    "    y = y.reshape((-1,1))\n",
    "    layer_3_delta = np.zeros_like(w_3) # (g,1)\n",
    "    layer_2_delta = np.zeros_like(w_2) # (h, g)\n",
    "    layer_1_delta = np.zeros_like(w_1) # (d, h)\n",
    "    layer_3_delta = 2 * np.dot(layer_2.T, (layer_3 - y)) # (g,1)\n",
    "    \n",
    "    if activelayer == 3:\n",
    "        return layer_1_delta/N, layer_2_delta/N, layer_3_delta/N\n",
    "    \n",
    "    # print(\"np.dot(w_3,(layer_3-y)) shape \", np.dot(w_3,(layer_3-y).T).shape)\n",
    "    # print(\" sigmoid(np.dot(layer_1,w_2), derivative=True).T shape\", sigmoid(np.dot(layer_1,w_2), derivative=True).T.shape)\n",
    "    dJ_dl2 = 2 * np.dot(w_3,(layer_3-y).T) # # (g,N)\n",
    "    dl2_ds2 = sigmoid(np.dot(layer_1,w_2), derivative=True).T # (g,N)\n",
    "\n",
    "    layer_2_delta  = np.dot(dJ_dl2 * dl2_ds2, layer_1).T    \n",
    "    if activelayer == 2:\n",
    "        return layer_1_delta/N, layer_2_delta/N, layer_3_delta/N\n",
    "    \n",
    "    # layer_2_delta = 2 * np.dot(np.dot(w_3,(layer_3-y).T)*sigmoid(np.dot(layer_1,w_2), derivative=True).T, layer_1).T\n",
    "    ds2_dl1 = w_2 # (h,g)\n",
    "    dl1_ds1 =  sigmoid(np.dot(layer_0,w_1), derivative=True).T # (h,N)\n",
    "    ds1_dw1 = layer_0 # (N,d)\n",
    "\n",
    "    layer_1_delta = np.dot(np.dot(ds2_dl1, dJ_dl2 * dl2_ds2) * dl1_ds1, ds1_dw1).T # (d,h)\n",
    "    if activelayer == 1:\n",
    "        return layer_1_delta/N, layer_2_delta/N, layer_3_delta/N\n",
    "\n",
    "# Cost function\n",
    "def cost(X, y, w_1, w_2, w_3, lmbda):\n",
    "    N, d = X.shape\n",
    "    a1,a2,a3,a4 = feed_forward_propagation(X,y,w_1,w_2,w_3,lmbda)\n",
    "\n",
    "#     return np.linalg.norm(a4[:,0] - y,2) ** 2 / N + lmbda * (np.linalg.norm(w_1)**2 + np.linalg.norm(w_2)**2 + np.linalg.norm(w_3)**2)\n",
    "    return np.linalg.norm(a4[:,0] - y,2) ** 2 / N\n",
    "\n",
    "# Define SGD\n",
    "def SGD(X, y, w_1, w_2, w_3, lmbda, learning_rate, batch_size, iterations):\n",
    "    # Complete here:\n",
    "    y = y.reshape((-1,1))\n",
    "    for i in range(iterations):\n",
    "        randomInd = np.arange(X.shape[0])\n",
    "        np.random.shuffle(randomInd)\n",
    "        randomInd = randomInd[:batch_size]\n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X[randomInd,:],y[randomInd,:],w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y[randomInd,:], w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda)\n",
    "\n",
    "        w_1 = w_1 - learning_rate*layer_1_delta\n",
    "        w_2 = w_2 - learning_rate*layer_2_delta\n",
    "        w_3 = w_3 - learning_rate*layer_3_delta\n",
    "\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        print(i,loss)\n",
    "        \n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define SVRG here:\n",
    "def SVRG(X, y, w_1, w_2, w_3, lmbda, learning_rate, T, batch_size, iterations):\n",
    "    # Complete here:\n",
    "    y = y.reshape((-1,1))\n",
    "    N = X.shape[0]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # compute all gradient and store\n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X,y,w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda)\n",
    "            \n",
    "        # g = function_gradient_vectorization(x, y, w, lambda_, gradclip)\n",
    "        \n",
    "        # initialize the w_previous\n",
    "        # w_previous = w.copy()\n",
    "        w_1_previous, w_2_previous, w_3_previous = w_1.copy(), w_2.copy(), w_3.copy()\n",
    "        for t in range(T//batch_size):\n",
    "            # random sample\n",
    "            # randomInd = int(np.random.rand() * N)\n",
    "            randomInd = np.arange(N)\n",
    "            np.random.shuffle(randomInd)\n",
    "            randomInd = randomInd[:batch_size]\n",
    "            # randomInd = np.random.randint(0,N)\n",
    "            layer_0_p1, layer_1_p1, layer_2_p1, layer_3_p1 = feed_forward_propagation(X[randomInd,:],y[randomInd,:],w_1_previous,w_2_previous,w_3_previous,lmbda)\n",
    "            layer_1_delta_p1, layer_2_delta_p1, layer_3_delta_p1 = back_propagation(y[randomInd,:], w_1_previous, w_2_previous, w_3_previous, layer_0_p1, layer_1_p1, layer_2_p1, layer_3_p1, lmbda)\n",
    "\n",
    "            layer_0_p2, layer_1_p2, layer_2_p2, layer_3_p2 = feed_forward_propagation(X[randomInd,:],y[randomInd,:],w_1,w_2,w_3,lmbda)\n",
    "            layer_1_delta_p2, layer_2_delta_p2, layer_3_delta_p2 = back_propagation(y[randomInd,:], w_1, w_2, w_3, layer_0_p2, layer_1_p2, layer_2_p2, layer_3_p2, lmbda)\n",
    "            \n",
    "            # calculate the update term\n",
    "            # part1 = function_gradient_vectorization(x[:,randomInd], y[:,randomInd], w_previous, lambda_, gradclip = gradclip)\n",
    "            # part2 = function_gradient_vectorization(x[:,randomInd], y[:,randomInd], w, lambda_, gradclip = gradclip)\n",
    "            # part3 = g\n",
    "\n",
    "            w_1_previous = w_1_previous - learning_rate * (layer_1_delta_p1 - layer_1_delta_p2 + layer_1_delta)\n",
    "            w_2_previous = w_2_previous - learning_rate * (layer_2_delta_p1 - layer_2_delta_p2 + layer_2_delta)\n",
    "            w_3_previous = w_3_previous - learning_rate * (layer_3_delta_p1 - layer_3_delta_p2 + layer_3_delta)\n",
    "\n",
    "            # w_previous = w_previous - alpha * (part1 - part2 + part3)\n",
    "            \n",
    "        # w = w_previous\n",
    "        w_1, w_2, w_3 = w_1_previous, w_2_previous, w_3_previous\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        print(i,loss)\n",
    "    \n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define GD here:\n",
    "def GD(X, y, w_1,w_2,w_3, lmbda, learning_rate, iterations):\n",
    "    N = X.shape[0]\n",
    "    for i in range(iterations):\n",
    "\n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda)\n",
    "\n",
    "#         w_1 = w_1 -  learning_rate * layer_1_delta + (lmbda / N * w_1)\n",
    "#         w_2 = w_2 - learning_rate * layer_2_delta + (lmbda / N * w_2)\n",
    "#         w_3 = w_3 - learning_rate * layer_3_delta + (lmbda / N * w_3)\n",
    "        w_1 = w_1 -  learning_rate * layer_1_delta\n",
    "        w_2 = w_2 - learning_rate * layer_2_delta\n",
    "        w_3 = w_3 - learning_rate * layer_3_delta\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        print(i,loss)\n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define projected GD here:\n",
    "def PGD(X, y, w_1,w_2,w_3, lmbda, learning_rate, iterations, noise = None):\n",
    "    # Complete here:\n",
    "    N = X.shape[0]\n",
    "    for i in range(iterations):\n",
    "\n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda)\n",
    "\n",
    "        if noise == None:\n",
    "            # the noise level is determined by the delta\n",
    "            noise = []\n",
    "            noise.append((np.random.random((w_1.shape)) * 2 - 1) *np.mean(layer_1_delta))\n",
    "            noise.append((np.random.random((w_2.shape)) * 2 - 1) *np.mean(layer_2_delta))\n",
    "            noise.append((np.random.random((w_3.shape)) * 2 - 1) *np.mean(layer_3_delta))\n",
    "        \n",
    "        w_1 = w_1 -  learning_rate * layer_1_delta + noise[0]\n",
    "        w_2 = w_2 - learning_rate * layer_2_delta + noise[1]\n",
    "        w_3 = w_3 - learning_rate * layer_3_delta + noise[2]\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        print(i,loss)\n",
    "    return w_1, w_2, w_3\n",
    "\n",
    "# Define BCD here:\n",
    "def BCD(X, y, w_1,w_2,w_3, lmbda, learning_rate, iterations, strategy = 0, blockrate = 0.5):\n",
    "    # Complete here:\n",
    "    # 2 strategies:\n",
    "    # a. randomly select weight by a random mask (random coordinate selection)\n",
    "    # b. we train the layers asynchronously to archieve the goal of blocking coordinate (cyclic update rule) \n",
    "    for i in range(iterations):\n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda)\n",
    "\n",
    "        if strategy == 0:\n",
    "            layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "            # generate random mask for each layer\n",
    "            l1_mask = np.random.choice([1, 0], size=layer_1_delta.shape, p=[blockrate, 1 - blockrate])\n",
    "            l2_mask = np.random.choice([1, 0], size=layer_2_delta.shape, p=[blockrate, 1 - blockrate])\n",
    "            l3_mask = np.random.choice([1, 0], size=layer_3_delta.shape, p=[blockrate, 1 - blockrate])\n",
    "            w_1 = w_1 - learning_rate * layer_1_delta * l1_mask\n",
    "            w_2 = w_2 - learning_rate * layer_2_delta * l2_mask\n",
    "            w_3 = w_3 - learning_rate * layer_3_delta * l3_mask\n",
    "        else:\n",
    "            # cyclic\n",
    "            # layer 1\n",
    "            layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "            layer_1_delta, _, _ = back_propagation_blocklayer(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda, 1)\n",
    "            w_1 = w_1 - learning_rate * layer_1_delta\n",
    "            # layer 2\n",
    "            layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "            _, layer_2_delta, _ = back_propagation_blocklayer(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda, 2)\n",
    "            w_2 = w_2 - learning_rate * layer_2_delta\n",
    "            # layer 3\n",
    "            layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "            _, _, layer_3_delta = back_propagation_blocklayer(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda, 3)\n",
    "            w_3 = w_3 - learning_rate * layer_3_delta\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        print(i,loss)\n",
    "    return w_1, w_2, w_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_0 shape  (512320, 6)\n",
      "layer_1 shape  (512320, 10)\n",
      "layer_2 shape  (512320, 11)\n",
      "layer_3 shape  (512320, 1)\n",
      "w_3 shape  (11, 1)\n",
      "w_2 shape  (10, 11)\n",
      "w_1 shape  (6, 10)\n",
      "layer_3_delta shape (11, 1)\n",
      "layer_2_delta shape (10, 11)\n",
      "layer_1_delta shape (6, 10)\n"
     ]
    }
   ],
   "source": [
    "w_size = 10\n",
    "\n",
    "# Initialize weights for debug\n",
    "w_1 = initialize_w(X_train.shape[1], w_size)\n",
    "\n",
    "w_2 = initialize_w(w_size,w_size+1)\n",
    "\n",
    "w_3 = initialize_w(w_size+1, 1)\n",
    "\n",
    "lmbda = 0\n",
    "\n",
    "layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X_test, y_test, w_1, w_2, w_3, lmbda)\n",
    "\n",
    "print(\"layer_0 shape \", layer_0.shape)\n",
    "print(\"layer_1 shape \",layer_1.shape)\n",
    "print(\"layer_2 shape \",layer_2.shape)\n",
    "print(\"layer_3 shape \",layer_3.shape)\n",
    "\n",
    "layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y_test, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda)\n",
    "\n",
    "\n",
    "print(\"w_3 shape \",w_3.shape)\n",
    "print(\"w_2 shape \",w_2.shape)\n",
    "print(\"w_1 shape \",w_1.shape)\n",
    "\n",
    "print(\"layer_3_delta shape\", layer_3_delta.shape)\n",
    "print(\"layer_2_delta shape\", layer_2_delta.shape)\n",
    "print(\"layer_1_delta shape\", layer_1_delta.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD\tinitial loss is : 4.58730709649059\n",
      "0 1.928051513414025\n",
      "1 1.5411550369681806\n",
      "2 1.3954089057310668\n",
      "PGD\tinitial loss is : 4.58730709649059\n",
      "0 33.554498795165706\n",
      "1 12.40042290913207\n",
      "2 5.189230699760335\n",
      "SGD\tinitial loss is : 4.58730709649059\n",
      "0 2.0054489327762415\n",
      "1 1.621153865404641\n",
      "2 1.4099039777656865\n",
      "SVRG\tinitial loss is : 4.58730709649059\n",
      "0 0.5797562674912018\n",
      "1 0.38633098615260547\n",
      "2 0.33187460134343\n",
      "BCD\tinitial loss is : 4.58730709649059\n",
      "0 2.8726146021302483\n",
      "1 2.1424132474927977\n"
     ]
    }
   ],
   "source": [
    "# Should be a hyperparameter that you tune, not an argument - Fill in the values\n",
    "lmbda =0.001\n",
    "w_size = 50\n",
    "lr = 0.02\n",
    "iterations = 3 # 100\n",
    "T = 2000\n",
    "batch_size = 100\n",
    "\n",
    "# Initialize weights\n",
    "w_1 = initialize_w(X_train.shape[1], w_size)\n",
    "\n",
    "w_2 = initialize_w(w_size,w_size)\n",
    "\n",
    "w_3 = initialize_w(w_size, 1)\n",
    "\n",
    "print(\"GD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "w_1_star,w_2_star,w_3_star = GD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations)\n",
    "\n",
    "print(\"PGD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "w_1_star,w_2_star,w_3_star = PGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations)\n",
    "\n",
    "print(\"SGD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "w_1_star,w_2_star,w_3_star = SGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, batch_size, iterations)\n",
    "\n",
    "print(\"SVRG\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "w_1_star1,w_2_star1,w_3_star1 = SVRG(X_train, y_train, w_1, w_2, w_3, lmbda, lr, T, batch_size, iterations)\n",
    "\n",
    "print(\"BCD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "strategy = 0 # 0 for random block mask, 1 for layer block\n",
    "blockrate = 0.5\n",
    "w_1_star,w_2_star,w_3_star = BCD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations, strategy, blockrate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be a hyperparameter that you tune, not an argument - Fill in the values\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lambda', type=float, default=0., dest='lmbda') \n",
    "parser.add_argument('--w_size', type=int, default=10, dest='w_size')\n",
    "parser.add_argument('--lr', type=float, default=0.01)\n",
    "parser.add_argument('--iterations', type=int, default=10)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# Initialize weights\n",
    "w_1 = initialize_w(X_train.shape[1], args.w_size)\n",
    "\n",
    "w_2 = initialize_w(args.w_size,args.w_size)\n",
    "\n",
    "w_3 = initialize_w(args.w_size, 1)\n",
    "\n",
    "# Get iterations\n",
    "iterations = args.iterations\n",
    "# Define plotting variables\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "# Define the optimizers for the loop\n",
    "optimizers = [\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": SGD(X_train, y_train, w_1, w_2, w_3, args.lmbda, args.lr, batch_size),\n",
    "            \"name\": \"SGD\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": SVRG(X_train, y_train, w_1, w_2, w_3, args.lmbda, args.lr),\n",
    "            \"name\": \"SVRG\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": GD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations),\n",
    "            \"name\": \"GD\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": PGD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations, noise=),\n",
    "            \"name\": \"PGD\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": BCD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations),\n",
    "            \"name\": \"BCD\",\n",
    "            \"inner\": # Fill in\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the iterates over the algorithms above\n",
    "\n",
    "for opt in optimizers:\n",
    "    #\n",
    "    # Fill in\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "ax[0].legend(loc=\"upper right\")\n",
    "ax[0].set_xlabel(r\"Iteration\", fontsize=16)\n",
    "ax[0].set_ylabel(\"Loss\", fontsize=16)\n",
    "ax[0].set_title(\"CA3 - Training a deep neural network for the power consumption Dataset\")\n",
    "ax[0].set_ylim(ymin=0)\n",
    "\n",
    "ax[1].legend(loc=\"upper right\")\n",
    "ax[1].set_xlabel(r\"Time [s]\", fontsize=16)\n",
    "ax[1].set_ylabel(\"Loss\", fontsize=16)\n",
    "ax[1].set_ylim(ymin=0)\n",
    "\n",
    "plt.savefig(\"power.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlon",
   "language": "python",
   "name": "mlon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
