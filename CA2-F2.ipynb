{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports from libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import resource\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing of data\n",
    "# Load data here:\n",
    "# get data function\n",
    "def get_data(data_folder = '/home/yipingx/Data/ghg_data/'):\n",
    "    data = []\n",
    "    filelist = os.listdir(data_folder)\n",
    "    for file in filelist:\n",
    "#         print(file)\n",
    "        data_single = np.genfromtxt(data_folder+file,dtype=np.float)\n",
    "        data.append(data_single)\n",
    "#     data=np.genfromtxt('./ghg_data/ghg.gid.site2067.dat',dtype=np.float)\n",
    "    return data\n",
    "\n",
    "data = np.array(get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(2921, 16, 327)\n"
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(5231, 2336)\n(1, 2336)\n(5231, 585)\n(1, 585)\n"
    }
   ],
   "source": [
    "# Split train and test data here: (X_train, Y_train, X_test, Y_test)\n",
    "\n",
    "\n",
    "def splitDataset_v1(totaldata, train = 0.8, seed = 123, normalize = True):\n",
    "    # seed\n",
    "    np.random.seed(seed)\n",
    "    # number\n",
    "    numdata = totaldata.shape[0]\n",
    "    numtrain = int(numdata*train)\n",
    "    numtest = numdata - numtrain\n",
    "    # index\n",
    "    index = np.arange(numdata)\n",
    "    np.random.shuffle(index)\n",
    "    # shuffle\n",
    "    totaldata = totaldata[index,:,:]\n",
    "    # split\n",
    "    traindata = totaldata[:numtrain, :, :]\n",
    "    testdata = totaldata[numtrain:, :, :]\n",
    "    # split X, Y\n",
    "    # train\n",
    "    X_train = traindata[:,:15, :]\n",
    "    X_train = X_train.reshape(numtrain, -1).T\n",
    "    Y_train = traindata[:,15, :].T\n",
    "#     Y_train = Y_train.reshape(numtrain, 1, -1)\n",
    "    \n",
    "    # test\n",
    "    X_test = testdata[:,:15, :]\n",
    "    X_test = X_test.reshape(numtest, -1).T\n",
    "    Y_test = testdata[:,15, :].T\n",
    "    \n",
    "    # normalization\n",
    "    if normalize is True:\n",
    "        X_train = X_train/np.linalg.norm(X_train, axis=0)\n",
    "#         Y_train = Y_train/np.linalg.norm(Y_train, axis=0)\n",
    "        X_test = X_test/np.linalg.norm(X_test, axis=0)\n",
    "#         Y_test = Y_test/np.linalg.norm(Y_test, axis=0)\n",
    "    \n",
    "#     Y_test = Y_test.reshape(numtest, 1, -1)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def splitDataset_v2(totaldata, train = 0.8, seed = 123, normalize = True):\n",
    "    # seed\n",
    "    np.random.seed(seed)\n",
    "    # number\n",
    "    numdata = totaldata.shape[0]\n",
    "    numtrain = int(numdata*train)\n",
    "    numtest = numdata - numtrain\n",
    "    # index\n",
    "    index = np.arange(numdata)\n",
    "    np.random.shuffle(index)\n",
    "    # shuffle\n",
    "    totaldata = totaldata[index,:,:]\n",
    "    # split\n",
    "    traindata = totaldata[:numtrain, :, :].reshape(numtrain,-1)\n",
    "    testdata = totaldata[numtrain:, :, :].reshape(numtest,-1)\n",
    "    # split X, Y\n",
    "    # train\n",
    "    X_train = traindata[:,:-1].T\n",
    "    Y_train = traindata[:,-1].reshape(-1,1).T\n",
    "    \n",
    "    # test\n",
    "    X_test = testdata[:,:-1].T\n",
    "    Y_test = testdata[:,-1].reshape(-1,1).T\n",
    "    \n",
    "    # normalization\n",
    "    if normalize is True:\n",
    "        X_train = X_train/np.linalg.norm(X_train, axis=0)\n",
    "#         Y_train = Y_train/np.linalg.norm(Y_train, axis=0)\n",
    "        X_test = X_test/np.linalg.norm(X_test, axis=0)\n",
    "#         Y_test = Y_test/np.linalg.norm(Y_test, axis=0)\n",
    "    \n",
    "#     Y_test = Y_test.reshape(numtest, 1, -1)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "# Split train and test data here: (X_train, Y_train, X_test, Y_test)\n",
    "X_train, Y_train, X_test, Y_test = splitDataset_v2(data)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, N = X_train.shape\n",
    "d,_ = Y_train.shape\n",
    "i=1\n",
    "w = np.random.normal(0,0.1, D*d).reshape(D,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(5231, 1)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4fe1407087f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1 x 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "Z = Y_train[:,i] * np.dot(w.T , (X_train[:,i].reshape(D,1)).reshape(D,1)) # 1 x 1\n",
    "dw = np.zeros((D, d))\n",
    "dw = (X_train[:,i].reshape(D,1) * Y_train[:,i]/(1 + np.exp(Z))).reshape(-1)\n",
    "print(Z)\n",
    "print(dw[:].shape)\n",
    "print(X_train[:,i].reshape(D,1).shape)\n",
    "print((Y_train[:,i]/(1 + np.exp(Z))).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic ridge regression with different optimizers\n",
    "# cost function and gradient calculation\n",
    "def cost_vectorization(x,y,w,lambda_=0.01):\n",
    "    # x.shape (5231, 2336)\n",
    "    # y.shape (1, 2336)\n",
    "    # w.shape (5231, 1)\n",
    "    D, N = x.shape\n",
    "    value = 0\n",
    "    Z = -1 * y * np.dot(w.T , x) # (1, 2336)\n",
    "    value = np.sum(np.log(1+np.exp(Z)))\n",
    "    norm_w = np.linalg.norm(w)\n",
    "    c = lambda_ * norm_w ** 2\n",
    "    return value/N + c\n",
    "\n",
    "\n",
    "def cost(x,y,w,lambda_ = 0.01):\n",
    "    D, N = x.shape\n",
    "    value = 0\n",
    "    for i in range(N):\n",
    "        Z = -1 * y[:,i] * np.dot(w.T , (x[:,i].reshape(D,1)).reshape(D,1))\n",
    "        value += np.sum(np.log(1+np.exp(Z))) # y 1-d value\n",
    "#         value += np.sum(np.log(1+np.exp(Z))) / y.shape[0] # y 327-d vector\n",
    "    norm_w = np.linalg.norm(w)\n",
    "    c = lambda_ * norm_w ** 2\n",
    "#     print(\"=========================\")\n",
    "#     print(\"loss part 1: {}\".format(value/N))\n",
    "#     print(\"loss part 2, lambda term : {}\".format(c))\n",
    "    return value/N + c \n",
    "\n",
    "\n",
    "\n",
    "def function_gradient_vectorization(X, Y, w, lambda_, gradclip = None):\n",
    "    # x.shape (5231, 2336)\n",
    "    # y.shape (1, 2336)\n",
    "    # w.shape (5231, 1)\n",
    "    # dw.shape (5231,1)\n",
    "    D, N = X.shape\n",
    "    d, _ = Y.shape\n",
    "    \n",
    "#     dw = np.zeros((D, d))\n",
    "    dw = np.zeros((D, d))\n",
    "    Z = Y * np.dot(w.T , X) # (1, 2336)\n",
    "    dw = np.dot(X , (Y/(1 + np.exp(Z))).T)\n",
    "    # dw = X * Y/(1 + np.exp(Z))\n",
    "\n",
    "    c = lambda_ * w *2\n",
    "    \n",
    "    if gradclip != None:\n",
    "        c[c>=gradclip] = gradclip\n",
    "    \n",
    "    return c-dw/N\n",
    "\n",
    "\n",
    "\n",
    "def function_gradient(X, Y, w, lambda_, gradclip = None):\n",
    "    # Calculate the gradient here:\n",
    "    # X: DxN\n",
    "    # Y: dxN\n",
    "    # w: Dxd\n",
    "    D, N = X.shape\n",
    "    d, _ = Y.shape\n",
    "    \n",
    "#     dw = np.zeros((D, d))\n",
    "    dw = np.zeros((D, d))\n",
    "    \n",
    "    for i in range(N):\n",
    "#         for j in range(d):\n",
    "#             Z = Y[j,i] * np.dot(w[:,j].reshape(1,-1) , (X[:,i].reshape(D,1)).reshape(D,1)) # 1 x 1\n",
    "            \n",
    "#             dw[:,j] += (X[:,i].reshape(D,1) * Y[j,i]/(1 + np.exp(Z))).reshape(-1) # D x 1        \n",
    "        Z = Y[:,i] * np.dot(w.T , X[:,i].reshape(D,1)) # 1 x 1\n",
    "#         up = X[:,i].reshape(D,1) * Y[:,i]\n",
    "#         down = (1 + np.exp(Z[0]))\n",
    "        dw[:] += X[:,i].reshape(D,1) * Y[:,i]/(1 + np.exp(Z[0]))\n",
    "#         dw[:] += (X[:,i].reshape(D,1) * Y[:,i]/(1 + np.exp(Z[0]))).reshape(-1) # D x 1\n",
    "    c = lambda_ * w *2\n",
    "    \n",
    "    if gradclip != None:\n",
    "        c[c>=gradclip] = gradclip\n",
    "    \n",
    "    return c-dw/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "7.46468415545774\n(5231, 2336)\n(1, 2336)\n(5231, 1)\n(5231, 1)\n"
    }
   ],
   "source": [
    "y = Y_train\n",
    "x = X_train\n",
    "D,N = x.shape\n",
    "d,_ = y.shape\n",
    "lambda_ = 0.0001\n",
    "# w = np.random.rand(D,1)*0.01  # Initialization of w\n",
    "w = np.random.normal(0,0.1, D*d).reshape(D,d)\n",
    "L = cost(x,y,w,lambda_)\n",
    "print(L)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(w.shape)\n",
    "g = function_gradient(x, y, w, lambda_)\n",
    "print(g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.5979731355935242\n(5231, 1)\n"
    }
   ],
   "source": [
    "print(cost_vectorization(x,y,w,lambda_))\n",
    "\n",
    "g1 = function_gradient_vectorization(x, y, w, lambda_)\n",
    "print(g1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "3.4832110676678154e-15\n3.3306690738754696e-16\n"
    }
   ],
   "source": [
    "diff = g-g1\n",
    "rel_diff = np.abs(diff)/np.abs(g)\n",
    "print(rel_diff.max())\n",
    "print(np.abs(diff).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define solvers: GD, SGD, SVRG and SAG. \n",
    "# Setting the values here:\n",
    "\n",
    "alpha = 0.1 # change the value\n",
    "num_iters = 5 # change the value\n",
    "lambda_ = 0.0001 # change the value\n",
    "epsilon = 0.001 # change the value\n",
    "\n",
    "# ---------------------- Complete the blank definitions: --------------------------------------\n",
    "\n",
    "def solver(x,y, w, alpha, num_iters , lambda_ , epsilon , optimizer = \"GD\", batchsize = None, gradclip = None, mem=False):\n",
    "    if (optimizer == \"GD\") :\n",
    "#         print(num_iters)\n",
    "        for i in range(num_iters):\n",
    "            # update the parameter w for GD here:\n",
    "            g = function_gradient_vectorization(x, y, w, lambda_, gradclip)\n",
    "            w = w - alpha*g\n",
    "#             loss = cost(x,y,w,lambda_)\n",
    "#             print(i, loss)\n",
    "            if (i%500 == 0):\n",
    "                loss = cost_vectorization(x,y,w,lambda_)\n",
    "                print(i, loss)\n",
    "            if (i%10==0) and (mem):\n",
    "                usage=resource.getrusage(resource.RUSAGE_SELF)\n",
    "                print(\"mem for GD (mb):\", (usage[2]*resource.getpagesize())/1000000.0)\n",
    "            if (np.linalg.norm(g) <= epsilon):\n",
    "                break\n",
    "    elif (optimizer == \"SGD\"):\n",
    "        for i in range(num_iters):\n",
    "            # Complete SGD here:\n",
    "                # randomly choose NSample points for calculating the estimated gradient\n",
    "                if (batchsize == None):\n",
    "                    NSample = int(np.random.rand() * x.shape[1])\n",
    "                    while NSample == 0:\n",
    "                        NSample = int(np.random.rand() * x.shape[1])\n",
    "                else:\n",
    "                    NSample = batchsize\n",
    "                randomInd = np.arange(x.shape[1])[:NSample]\n",
    "                g = function_gradient_vectorization(x[:,randomInd], y[:,randomInd], w, lambda_, gradclip = gradclip)\n",
    "                w = w - alpha*g\n",
    "                \n",
    "                if (i%500 == 0):\n",
    "                    loss = cost_vectorization(x,y,w,lambda_)\n",
    "                    print(i, loss)\n",
    "                \n",
    "                if (i%10==0) and (mem):\n",
    "                    usage=resource.getrusage(resource.RUSAGE_SELF)\n",
    "                    print(\"mem for GD (mb):\", (usage[2]*resource.getpagesize())/1000000.0)\n",
    "                if (np.linalg.norm(g) <= epsilon):\n",
    "                    break\n",
    "                \n",
    "                \n",
    "    elif (optimizer == \"SVRG\"):\n",
    "        i = 0\n",
    "        T = 2000\n",
    "#         K = math.floor(num_iters/T)\n",
    "        K = 100\n",
    "#         Z = np.matmul(x,np.diagflat(y))\n",
    "        N = x.shape[1]\n",
    "        for k in range(K):\n",
    "#             wz = np.matmul(w.T , Z)\n",
    "#             diag = np.diagflat(1/(1+np.exp(-1*wz))-np.ones((1,N)))\n",
    "#             Ga_ = np.matmul(Z , diag)\n",
    "#             ga_ = (1/N) * np.matmul(Ga_ , np.ones((N,1)))\n",
    "            # compute all gradient and store\n",
    "            g = function_gradient_vectorization(x, y, w, lambda_, gradclip)\n",
    "            # initialize the w_previous\n",
    "            w_previous = w.copy()\n",
    "            for t in range(T):\n",
    "                # random sample\n",
    "                if (batchsize == None):\n",
    "                    NSample = int(np.random.rand() * x.shape[1])\n",
    "                    while NSample == 0:\n",
    "                        NSample = int(np.random.rand() * x.shape[1])\n",
    "                else:\n",
    "                    NSample = batchsize\n",
    "                # randomInd = np.arange(x.shape[1])[:NSample]\n",
    "                randomInd = np.arange(x.shape[1])\n",
    "                np.random.shuffle(randomInd)\n",
    "                randomInd = randomInd[:NSample]\n",
    "\n",
    "                if t==0 and k==0:\n",
    "                    print(randomInd)\n",
    "                # calculate the update term\n",
    "#                 w_previous = w.copy()\n",
    "                part1 = function_gradient_vectorization(x[:,randomInd], y[:,randomInd], w_previous, lambda_, gradclip = gradclip)\n",
    "                part2 = function_gradient_vectorization(x[:,randomInd], y[:,randomInd], w, lambda_, gradclip = gradclip)\n",
    "                part3 = g\n",
    "\n",
    "                w_previous = w_previous - alpha * (part1 - part2 + part3)\n",
    "                \n",
    "                i = i+1\n",
    "                if (i % 1000 == 0):\n",
    "                    loss = cost_vectorization(x,y,w_previous,lambda_)\n",
    "                    print(i, loss)\n",
    "            w = w_previous\n",
    "            if (np.linalg.norm((part1 - part2 + part3)) <= epsilon):\n",
    "                break\n",
    "                \n",
    "    elif (optimizer == \"SAG\"):\n",
    "        # Complete SAG here:\n",
    "        # X: DxN\n",
    "        # Y: dxN\n",
    "        # w: Dxd\n",
    "        D, N = x.shape\n",
    "        d = y.shape[0]\n",
    "        dw = np.zeros_like(w, dtype=np.float)\n",
    "        gk = np.zeros((N,D,d), dtype=np.float)\n",
    "\n",
    "\n",
    "        for k in range(num_iters):\n",
    "            index = np.random.randint(0,N)\n",
    "            # NSample = int(np.random.rand() * x.shape[1])\n",
    "            NSample  = batchsize\n",
    "\n",
    "            # randomInd = np.arange(x.shape[1])[:NSample]\n",
    "            randomInd = np.arange(x.shape[1])\n",
    "            np.random.shuffle(randomInd)\n",
    "            randomInd = randomInd[:NSample]\n",
    "\n",
    "            index_all = randomInd\n",
    "            # if k==0:\n",
    "                # print(index)\n",
    "            # for index in index_all:\n",
    "            g_i = function_gradient_vectorization(x[:,index].reshape(D,1), y[:,index].reshape(d,1), w, lambda_, gradclip = gradclip)\n",
    "            # dw = dw - gk[index,:,:] +  g_i\n",
    "\n",
    "            gk[index,:,:] = g_i\n",
    "            # dw =  ( dw + 2 * lambda_ * w)/N\n",
    "            # n = min(k+1,N)\n",
    "            # dw = dw /N\n",
    "            dw = np.sum(gk[index,:,:],axis=0)\n",
    "            w = w - alpha * dw/N\n",
    "            if (np.linalg.norm(dw) <= epsilon):\n",
    "                break\n",
    "\n",
    "            if (k % 100 == 0):\n",
    "                    loss = cost_vectorization(x,y,w,lambda_)\n",
    "                    # print(dw[:20,:])\n",
    "                    print(k, loss)\n",
    "            if (k%10==0) and (mem):\n",
    "                usage=resource.getrusage(resource.RUSAGE_SELF)\n",
    "                print(\"mem for SAG (mb):\", (usage[2]*resource.getpagesize())/1000000.0)\n",
    "            # i_t = \n",
    "        i=k\n",
    "    return w,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "initial loss 2.1006450449168645\nSAG optimization\n0 0.04689019605784502\n100 0.03973485541641171\n200 0.03559048679294316\n300 0.033648385856915944\n400 0.03223616512210999\n500 0.02962528218785076\n600 0.028562887762184835\n700 0.02766316929068601\n800 0.027010200317596263\n900 0.026430186418533952\n1000 0.025835770235811236\n1100 0.025187386774815156\n1200 0.024744251826225385\n1300 0.024253615010602424\n1400 0.02392007311331483\n1500 0.023613023528042787\n1600 0.02336055114072531\n1700 0.023106008286414993\n1800 0.02277578738498129\n1900 0.022400036537156394\n2000 0.022103534921787342\n2100 0.02182367984103336\n2200 0.021711942267968496\n2300 0.021563186968844062\n2400 0.021365476663004347\n2500 0.021191175061577784\n2600 0.02107931521279479\n2700 0.020761895733869065\n2800 0.020684472996815743\n2900 0.02052807481068481\n3000 0.02023556937808976\n3100 0.020062817459377904\n3200 0.019805018720074848\n3300 0.01956865508578081\n3400 0.019339044703369765\n3500 0.019214181213433216\n3600 0.019111139631100057\n3700 0.018960483168912228\n3800 0.018852283899600912\n3900 0.018683381010540526\n4000 0.018546808843261514\n4100 0.01840080625922209\n4200 0.0182951244111816\n4300 0.01819015124636717\n4400 0.0180752924077149\n4500 0.01802515301059564\n4600 0.01791211645867817\n4700 0.01778762211708077\n4800 0.0177300862681552\n4900 0.017653284065393984\n5000 0.017509075866263723\n5100 0.01740509428512943\n5200 0.01734960176779334\n5300 0.017321377620635257\n5400 0.017225572192268723\n5500 0.017164073744587158\n5600 0.017049796957867642\n5700 0.017001671543659592\n5800 0.01697841898533739\n5900 0.016926496245565054\n6000 0.01688647222325285\n6100 0.016786668120047837\n6200 0.01672200873455757\n6300 0.016690450647644437\n6400 0.0166381610244189\n6500 0.01659738155073198\n6600 0.016518894997469013\n6700 0.016450896444533795\n6800 0.016407546688029365\n6900 0.016350821122409438\n7000 0.01626684798987304\n7100 0.016221083504023945\n7200 0.01618037509472233\n7300 0.016129638028320868\n7400 0.016085902975896876\n7500 0.016045427406281874\n7600 0.0160085240237685\n7700 0.015954540745393157\n7800 0.015913084295801627\n7900 0.0158687041535539\n8000 0.015845357964763822\n8100 0.01580738192864814\n8200 0.015752689294540483\n8300 0.015720789217277306\n8400 0.01564615738099385\n8500 0.015616400803276888\n8600 0.015582140109241078\n8700 0.01553294647581302\n8800 0.015499225762901558\n8900 0.015440928780942347\n9000 0.015394131161821442\n9100 0.015364630113180646\n9200 0.015330406636803495\n9300 0.015294381092657162\n9400 0.01524590093433104\n9500 0.015215527093892466\n9600 0.015210326994290809\n9700 0.015159210653812082\n9800 0.015142268769472292\n9900 0.015130774367672647\n10000 0.015101666611682744\n10100 0.015083392718376117\n10200 0.01503660986500166\n10300 0.015007855062112819\n10400 0.014979481801604362\n10500 0.014933130103161295\n10600 0.014904318291446991\n10700 0.014883204415799059\n10800 0.014865712173944626\n10900 0.014836792075735034\n11000 0.0148179849900147\n11100 0.014792950915650124\n11200 0.014773403748892595\n11300 0.014738021094249727\n11400 0.014717597458524806\n11500 0.014712241625491035\n11600 0.014705322149896281\n11700 0.014684648098660175\n11800 0.014662259557645593\n11900 0.014643842632220835\n12000 0.014624828933489816\n12100 0.014597970273707926\n12200 0.01457297546297941\n12300 0.014558607045157412\n12400 0.014544998479922279\n12500 0.014533142293048557\n12600 0.014511003882779918\n12700 0.014508373177969248\n12800 0.01448693354727926\n12900 0.014467210442862855\n13000 0.014436414736209283\n13100 0.014425743686422089\n13200 0.014406639137204292\n13300 0.01438484672712613\n13400 0.014380888858929675\n13500 0.014369724670128651\n13600 0.014340445704056941\n13700 0.014315867821661967\n13800 0.014285949585769224\n13900 0.014277710583662711\n14000 0.014273592468777442\n14100 0.014271330410491155\n14200 0.014242229219169927\n14300 0.014221786807998266\n14400 0.014220084213340478\n14500 0.014200256197503317\n14600 0.014174925958814397\n14700 0.014162617393190411\n14800 0.01414441330162286\n14900 0.014133492339825455\n15000 0.014127602250005856\n15100 0.014092320272550554\n15200 0.014082956341835626\n15300 0.01406663692074909\n15400 0.014055221421938806\n15500 0.014034597042542847\n15600 0.014016952299439603\n15700 0.014013779021980264\n15800 0.014008617146738844\n15900 0.014001938818263077\n16000 0.013981894455073232\n16100 0.01396073947882816\n16200 0.01393385263259676\n16300 0.013924899642275557\n16400 0.013917689572800018\n16500 0.013908929185900108\n16600 0.013899152669206615\n16700 0.013891225331119694\n16800 0.013886875389700924\n16900 0.013875916433043405\n17000 0.013861432159317398\n17100 0.013856684582652282\n17200 0.013847313265195081\n17300 0.013843761955876083\n17400 0.013828363320867537\n17500 0.01381872818954283\n17600 0.013804981731990542\n17700 0.013795023585033307\n17800 0.013791748360032625\n17900 0.013770644160107403\n18000 0.01376189280218118\n18100 0.013753117225445533\n18200 0.01374846935056695\n18300 0.01373542745591517\n18400 0.013725477171588996\n18500 0.01372217719366984\n18600 0.013713845904396785\n18700 0.013697510330883128\n18800 0.013682306560553581\n18900 0.013679432108173515\n19000 0.013667816260550737\n19100 0.013662414274778848\n19200 0.013658073941551288\n19300 0.013648836930864886\n19400 0.01363573231487726\n19500 0.013630177262458182\n19600 0.013616254851004091\n19700 0.013609103878840852\n19800 0.013598320241972788\n19900 0.013580347116310874\nCost of SAG after convergence:  0.01357401425284767\nTraining time for SAG:  2.9099059104919434\n"
    }
   ],
   "source": [
    "## Solving the optimization problem:\n",
    "\n",
    "alpha = 0.01 # change the value\n",
    "num_iters = 50000 # change the value\n",
    "lambda_ = 0.01 # change the value\n",
    "epsilon = 0.001 # change the value\n",
    "\n",
    "y = Y_train\n",
    "x = X_train\n",
    "D,N = x.shape\n",
    "d,_ = y.shape\n",
    "\n",
    "# w = np.random.rand(D,1)*0.01  # Initialization of w\n",
    "# np.random.seed(1000)\n",
    "w = np.random.normal(0,0.1, D*d).reshape(D,d)\n",
    "# start = time.time()\n",
    "#-------------------- GD Solver -----------------------\n",
    "loss = cost_vectorization(x,y,w,lambda_)\n",
    "print(\"initial loss {}\".format(loss))\n",
    "\n",
    "#-------------------- GD Solver -----------------------\n",
    "# print(\"GD optimization\")\n",
    "# opt = \"GD\"\n",
    "# start_gd = time.time()\n",
    "# gde, iters_total = solver(x,y, w, alpha, num_iters  , lambda_ , epsilon , optimizer = opt, mem=False) # complete the command \n",
    "# end_gd = time.time()\n",
    "# cost_value = cost_vectorization(x,y,gde,lambda_)  # Calculate the cost value\n",
    "# print(\"Cost of GD after convergence: \",cost_value)\n",
    "# print(\"Training time for GD: \", end_gd-start_gd)\n",
    "\n",
    "# # #-------------------- SGD Solver -----------------------\n",
    "# # complete here :\n",
    "# opt = 'SGD'\n",
    "# print(\"SGD optimization\")\n",
    "# start_sgd = time.time()\n",
    "# gde, iters_total = solver(x,y, w, alpha, num_iters  , lambda_ , epsilon , optimizer = opt, mem=False) # complete the command \n",
    "# end_sgd = time.time()\n",
    "# cost_value = cost_vectorization(x,y,gde,lambda_)  # Calculate the cost value\n",
    "# print(\"Cost of SGD after convergence: \",cost_value)\n",
    "# print(\"Training time for SGD: \", end_sgd-start_sgd)\n",
    "\n",
    "# #-------------------- SVRG Solver -----------------------\n",
    "# # complete here :\n",
    "# alpha = 0.1 # change the value\n",
    "# num_iters = 2000 # change the value\n",
    "# lambda_ = 0.0001 # change the value\n",
    "# epsilon = 0.0000001 # change the value\n",
    "# opt = 'SVRG'\n",
    "# print(\"SVRG optimization\")\n",
    "# start_srvg = time.time()\n",
    "# gde, iters_total = solver(x,y, w, alpha, num_iters  , lambda_ , epsilon , optimizer = opt, mem=False, batchsize = 50) # complete the command \n",
    "# end_svrg = time.time()\n",
    "# cost_value = cost_vectorization(x,y,gde,lambda_)  # Calculate the cost value\n",
    "# print(\"Cost of SVRG after convergence: \",cost_value)\n",
    "# print(\"Training time for SVRG: \", end_svrg-start_srvg)\n",
    "\n",
    "\n",
    "#-------------------- SAG Solver -----------------------\n",
    "# complete here :\n",
    "alpha = 0.01 # change the value\n",
    "num_iters = 20000 # change the value\n",
    "lambda_ = 0.0001 # change the value\n",
    "epsilon = 0.0000001 # change the value\n",
    "opt = 'SAG'\n",
    "print(\"SAG optimization\")\n",
    "start_sag = time.time()\n",
    "gde, iters_total = solver(x,y, w, alpha, num_iters, lambda_ , epsilon , optimizer = opt, mem=False, batchsize = 50) # complete the command \n",
    "end_sag = time.time()\n",
    "cost_value = cost_vectorization(x,y,gde,lambda_)  # Calculate the cost value\n",
    "print(\"Cost of SAG after convergence: \",cost_value)\n",
    "print(\"Training time for SAG: \", end_sag-start_sag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''## Executing the iterations and plot the cost function here:\n",
    "\n",
    "ti= np.zeros((50,4))\n",
    "cost_= np.zeros((50,4))\n",
    "for i in range(50):\n",
    "    print(\"......\",i,\".......\")\n",
    "    #--------------GD-------------------\n",
    "    start = time.time()\n",
    "    gde = solver(x.T,y,w,num_iters=i)\n",
    "    end = time.time()\n",
    "\n",
    "    cost_[i,0] = cost(x.T,y,gde)\n",
    "\n",
    "    ti[i,0] = end-start\n",
    "\n",
    "    #---------------SGD------------------\n",
    "    #Complete for SGD solver here :\n",
    "    \n",
    "    #---------------SVRG----------------\n",
    "    #Complete for SVRG solver here :\n",
    "    \n",
    "    #---------------SAG------------------\n",
    "    #Complete for SAG solver here :\n",
    "    \n",
    "    #------------------------------------\n",
    "    \n",
    "    ## Pl the results:\n",
    "    \n",
    "\n",
    "l0 = plt.plot(cost_[:,0],color=\"red\")\n",
    "# complete other plots here: \n",
    "\n",
    "\n",
    "plt.xlabel(\"Number of Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend(['GD', 'SGD', 'SVRG', 'SAG'])\n",
    "plt.show()\n",
    "\n",
    "l0 = plt.plot(ti[:,0],color=\"red\")\n",
    "# complete other plots here:\n",
    "\n",
    "plt.xlabel(\"Number of Iteration\")\n",
    "plt.ylabel(\"Time (sec)\")\n",
    "plt.legend(['GD', 'SGD', 'SVRG', 'SAG'])\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tunning the hyper-paramter here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparing different optimizers here: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('sklearn': conda)",
   "language": "python",
   "name": "python38164bitsklearncondaab2615fd75db424388456508542e2d0a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}