{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports from libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import resource\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4905, 2336)\n",
      "(327, 2336)\n",
      "(4905, 585)\n",
      "(327, 585)\n"
     ]
    }
   ],
   "source": [
    "## Preprocessing of data\n",
    "# Load data here:\n",
    "# get data function\n",
    "def get_data(data_folder = '/Users/wzh/Downloads/EP3260-MLoNs-2020/dataset/ghg_data/'):\n",
    "    data = []\n",
    "    filelist = os.listdir(data_folder)\n",
    "    for file in filelist:\n",
    "#         print(file)\n",
    "        data_single = np.genfromtxt(data_folder+file,dtype=np.float)\n",
    "        data.append(data_single)\n",
    "#     data=np.genfromtxt('./ghg_data/ghg.gid.site2067.dat',dtype=np.float)\n",
    "    return data\n",
    "\n",
    "def splitDataset(totaldata, train = 0.8, seed = 123, normalize = True):\n",
    "    # seed\n",
    "    np.random.seed(seed)\n",
    "    # number\n",
    "    numdata = totaldata.shape[0]\n",
    "    numtrain = int(numdata*train)\n",
    "    numtest = numdata - numtrain\n",
    "    # index\n",
    "    index = np.arange(numdata)\n",
    "    np.random.shuffle(index)\n",
    "    # shuffle\n",
    "    totaldata = totaldata[index,:,:]\n",
    "    # split\n",
    "    traindata = totaldata[:numtrain, :, :]\n",
    "    testdata = totaldata[numtrain:, :, :]\n",
    "    # split X, Y\n",
    "    # train\n",
    "    X_train = traindata[:,:15, :]\n",
    "    X_train = X_train.reshape(numtrain, -1).T\n",
    "    Y_train = traindata[:,15, :].T\n",
    "#     Y_train = Y_train.reshape(numtrain, 1, -1)\n",
    "    # test\n",
    "    X_test = testdata[:,:15, :]\n",
    "    X_test = X_test.reshape(numtest, -1).T\n",
    "    Y_test = testdata[:,15, :].T\n",
    "    \n",
    "    # normalization\n",
    "    if normalize is True:\n",
    "        X_train = X_train/np.linalg.norm(X_train, axis=0)\n",
    "#         Y_train = Y_train/np.linalg.norm(Y_train, axis=0)\n",
    "        X_test = X_test/np.linalg.norm(X_test, axis=0)\n",
    "#         Y_test = Y_test/np.linalg.norm(Y_test, axis=0)\n",
    "    \n",
    "#     Y_test = Y_test.reshape(numtest, 1, -1)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "# Split train and test data here: (X_train, Y_train, X_test, Y_test)\n",
    "data = np.array(get_data())\n",
    "# Split train and test data here: (X_train, Y_train, X_test, Y_test)\n",
    "X_train, Y_train, X_test, Y_test = splitDataset(data)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic ridge regression with different optimizers\n",
    "# cost function and gradient calculation\n",
    "\n",
    "def cost(x,y,w,lambda_ = 0.01):\n",
    "    D, N = x.shape\n",
    "    value = 0\n",
    "    for i in range(N):\n",
    "        Z = -1 * y[:,i] * np.dot(w.T , (x[:,i].reshape(D,1)).reshape(D,1))\n",
    "        value += np.sum(np.log(1+np.exp(Z))) / y.shape[0]\n",
    "    norm_w = np.linalg.norm(w)\n",
    "    c = lambda_ * norm_w ** 2\n",
    "    print(\"=========================\")\n",
    "    print(\"loss part 1: {}\".format(value/N))\n",
    "    print(\"loss part 2, lambda term : {}\".format(c))\n",
    "    return value/N + c \n",
    "\n",
    "def function_gradient(X, Y, w, lambda_):\n",
    "    # Calculate the gradient here:\n",
    "    # X: DxN\n",
    "    # Y: dxN\n",
    "    # w: Dxd\n",
    "    D, N = X.shape\n",
    "    d, _ = Y.shape\n",
    "    \n",
    "    dw = np.zeros((D, d))\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(d):\n",
    "            Z = Y[j,i] * np.dot(w[:,j].reshape(1,-1) , (X[:,i].reshape(D,1)).reshape(D,1)) # 1 x 1\n",
    "            \n",
    "            dw[:,j] += (X[:,i].reshape(D,1) * Y[j,i]/(1 + np.exp(Z))).reshape(-1) # D x 1 \n",
    "    c = lambda_ * w *2\n",
    "    return c-dw/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define solvers: GD, SGD, SVRG and SAG. \n",
    "# Setting the values here:\n",
    "\n",
    "alpha = 0.1 # change the value\n",
    "num_iters = 5 # change the value\n",
    "lambda_ = 0.0001 # change the value\n",
    "epsilon = 0.001 # change the value\n",
    "\n",
    "# ---------------------- Complete the blank definitions: --------------------------------------\n",
    "\n",
    "def solver(x,y, w, alpha, num_iters , lambda_ , epsilon , optimizer = \"GD\",mem=False):\n",
    "    if (optimizer == \"GD\") :\n",
    "        print(num_iters)\n",
    "        for i in range(num_iters):\n",
    "            print(\"hello\")\n",
    "            # update the parameter w for GD here:\n",
    "            g = function_gradient(x, y, w, lambda_)\n",
    "            w = w - alpha*g\n",
    "            loss = cost(x,y,w,lambda_)\n",
    "            print(i, loss)\n",
    "            \n",
    "            if (i%10==0) and (mem):\n",
    "                usage=resource.getrusage(resource.RUSAGE_SELF)\n",
    "                print(\"mem for GD (mb):\", (usage[2]*resource.getpagesize())/1000000.0)\n",
    "            if (np.linalg.norm(g) <= epsilon):\n",
    "                break\n",
    "    elif (optimizer == \"SGD\"):\n",
    "        for i in range(num_iters):\n",
    "            # Complete SGD here:\n",
    "                pass\n",
    "    elif (optimizer == \"SVRG\"):\n",
    "        T = 100\n",
    "        K = math.floor(num_iters/T)\n",
    "        Z = np.matmul(x,np.diagflat(y))\n",
    "        N = x.shape[1]\n",
    "        for k in range(K):\n",
    "            wz = np.matmul(w.T , Z)\n",
    "            diag = np.diagflat(1/(1+np.exp(-1*wz))-np.ones((1,N)))\n",
    "            Ga_ = np.matmul(Z , diag)\n",
    "            ga_ = (1/N) * np.matmul(Ga_ , np.ones((N,1)))\n",
    "            for t in range(T):\n",
    "                # Complete SVRG here:\n",
    "                pass\n",
    "    elif (optimizer == \"SAG\"):\n",
    "        # Complete SAG here:\n",
    "        pass\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "loss part 1: 648.2212300019851\n",
      "loss part 2, lambda term : 1.6031159829466946\n",
      "initial loss 649.8243459849318\n",
      "5\n",
      "hello\n",
      "=========================\n",
      "loss part 1: 23.59700226705947\n",
      "loss part 2, lambda term : 1.6549306335762126\n",
      "0 25.251932900635683\n",
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wzh/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "loss part 1: 22.28208819749881\n",
      "loss part 2, lambda term : 1.655203666460689\n",
      "1 23.937291863959498\n",
      "hello\n",
      "=========================\n",
      "loss part 1: 21.443807726321005\n",
      "loss part 2, lambda term : 1.6554737241311346\n",
      "2 23.09928145045214\n",
      "hello\n",
      "=========================\n",
      "loss part 1: 20.824990852055944\n",
      "loss part 2, lambda term : 1.6557401305225485\n",
      "3 22.480730982578493\n",
      "hello\n",
      "=========================\n",
      "loss part 1: 20.335283291654726\n",
      "loss part 2, lambda term : 1.6560029244047667\n",
      "4 21.991286216059493\n",
      "=========================\n",
      "loss part 1: 648.2212300019851\n",
      "loss part 2, lambda term : 1.6031159829466946\n",
      "Cost of GD after convergence:  649.8243459849318\n",
      "Training time for GD:  195.14756202697754\n"
     ]
    }
   ],
   "source": [
    "## Solving the optimization problem:\n",
    "\n",
    "alpha = 0.1 # change the value\n",
    "num_iters = 5 # change the value\n",
    "lambda_ = 0.0001 # change the value\n",
    "epsilon = 0.001 # change the value\n",
    "\n",
    "y = Y_train\n",
    "x = X_train\n",
    "D,N = x.shape\n",
    "d,_ = y.shape\n",
    "\n",
    "# w = np.random.rand(D,1)*0.01  # Initialization of w\n",
    "w = np.random.normal(0,0.1, D*d).reshape(D,d)\n",
    "start = time.time()\n",
    "#-------------------- GD Solver -----------------------\n",
    "num_iters = 5 # change the value\n",
    "loss = cost(x,y,w,lambda_)\n",
    "print(\"initial loss {}\".format(loss))\n",
    "\n",
    "gde = solver(x,y, w, alpha, num_iters , lambda_ , epsilon , optimizer = \"GD\",mem=False) # complete the command \n",
    "\n",
    "end = time.time()\n",
    "# print(\"Weights of GD after convergence: \\n\",gde)\n",
    "\n",
    "cost_value = cost(x,y,gde,lambda_)  # Calculate the cost value\n",
    "print(\"Cost of GD after convergence: \",cost_value)\n",
    "\n",
    "print(\"Training time for GD: \", end-start)\n",
    "\n",
    "#-------------------- SGD Solver -----------------------\n",
    "# complete here :\n",
    "\n",
    "#-------------------- SVRG Solver -----------------------\n",
    "# complete here :\n",
    "\n",
    "#-------------------- SAG Solver -----------------------\n",
    "# complete here :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...... 0 .......\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "solver() missing 3 required positional arguments: 'alpha', 'lambda_', and 'epsilon'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f498179f26b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#--------------GD-------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mgde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: solver() missing 3 required positional arguments: 'alpha', 'lambda_', and 'epsilon'"
     ]
    }
   ],
   "source": [
    "## Executing the iterations and plot the cost function here:\n",
    "\n",
    "ti= np.zeros((50,4))\n",
    "cost_= np.zeros((50,4))\n",
    "for i in range(50):\n",
    "    print(\"......\",i,\".......\")\n",
    "    #--------------GD-------------------\n",
    "    start = time.time()\n",
    "    gde = solver(x.T,y,w,num_iters=i)\n",
    "    end = time.time()\n",
    "\n",
    "    cost_[i,0] = cost(x.T,y,gde)\n",
    "\n",
    "    ti[i,0] = end-start\n",
    "\n",
    "    #---------------SGD------------------\n",
    "    #Complete for SGD solver here :\n",
    "    \n",
    "    #---------------SVRG----------------\n",
    "    #Complete for SVRG solver here :\n",
    "    \n",
    "    #---------------SAG------------------\n",
    "    #Complete for SAG solver here :\n",
    "    \n",
    "    #------------------------------------\n",
    "    \n",
    "    ## Pl the results:\n",
    "    \n",
    "\n",
    "l0 = plt.plot(cost_[:,0],color=\"red\")\n",
    "# complete other plots here: \n",
    "\n",
    "\n",
    "plt.xlabel(\"Number of Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend(['GD', 'SGD', 'SVRG', 'SAG'])\n",
    "plt.show()\n",
    "\n",
    "l0 = plt.plot(ti[:,0],color=\"red\")\n",
    "# complete other plots here:\n",
    "\n",
    "plt.xlabel(\"Number of Iteration\")\n",
    "plt.ylabel(\"Time (sec)\")\n",
    "plt.legend(['GD', 'SGD', 'SVRG', 'SAG'])\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tunning the hyper-paramter here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparing different optimizers here: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
