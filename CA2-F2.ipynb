{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports from libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import resource\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing of data\n",
    "# Load data here:\n",
    "# get data function\n",
    "def get_data(data_folder = './ghg_data/'):\n",
    "    data = []\n",
    "    filelist = os.listdir(data_folder)\n",
    "    for file in filelist:\n",
    "        data_single = np.genfromtxt(data_folder+file,dtype=np.float)\n",
    "        data.append(data_single)\n",
    "    return data\n",
    "\n",
    "data = np.array(get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test data here: (X_train, Y_train, X_test, Y_test)\n",
    "def splitDataset_v2(totaldata, train = 0.8, seed = 123, normalize = True):\n",
    "    # seed\n",
    "    np.random.seed(seed)\n",
    "    # number\n",
    "    numdata = totaldata.shape[0]\n",
    "    numtrain = int(numdata*train)\n",
    "    numtest = numdata - numtrain\n",
    "    # index\n",
    "    index = np.arange(numdata)\n",
    "    np.random.shuffle(index)\n",
    "    # shuffle\n",
    "    totaldata = totaldata[index,:,:]\n",
    "    # split\n",
    "    traindata = totaldata[:numtrain, :, :].reshape(numtrain,-1)\n",
    "    testdata = totaldata[numtrain:, :, :].reshape(numtest,-1)\n",
    "    # split X, Y\n",
    "    # train\n",
    "    X_train = traindata[:,:-1].T\n",
    "    Y_train = traindata[:,-1].reshape(-1,1).T\n",
    "    \n",
    "    # test\n",
    "    X_test = testdata[:,:-1].T\n",
    "    Y_test = testdata[:,-1].reshape(-1,1).T\n",
    "    \n",
    "    # normalization\n",
    "    if normalize is True:\n",
    "        X_train = X_train/np.linalg.norm(X_train, axis=0)\n",
    "#         Y_train = Y_train/np.linalg.norm(Y_train, axis=0)\n",
    "        X_test = X_test/np.linalg.norm(X_test, axis=0)\n",
    "#         Y_test = Y_test/np.linalg.norm(Y_test, axis=0)\n",
    "    \n",
    "#     Y_test = Y_test.reshape(numtest, 1, -1)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "# Split train and test data here: (X_train, Y_train, X_test, Y_test)\n",
    "X_train, Y_train, X_test, Y_test = splitDataset_v2(data)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic ridge regression with different optimizers\n",
    "# cost function and gradient calculation\n",
    "def cost_vectorization(x,y,w,lambda_=0.01):\n",
    "    # x.shape (5231, 2336)\n",
    "    # y.shape (1, 2336)\n",
    "    # w.shape (5231, 1)\n",
    "    D, N = x.shape\n",
    "    value = 0\n",
    "    Z = -1 * y * np.dot(w.T , x) # (1, 2336)\n",
    "    value = np.sum(np.log(1+np.exp(Z)))\n",
    "    norm_w = np.linalg.norm(w)\n",
    "    c = lambda_ * norm_w ** 2\n",
    "    return value/N + c\n",
    "\n",
    "\n",
    "def cost(x,y,w,lambda_ = 0.01):\n",
    "    D, N = x.shape\n",
    "    value = 0\n",
    "    for i in range(N):\n",
    "        Z = -1 * y[:,i] * np.dot(w.T , (x[:,i].reshape(D,1)).reshape(D,1))\n",
    "        value += np.sum(np.log(1+np.exp(Z))) # y 1-d value\n",
    "    norm_w = np.linalg.norm(w)\n",
    "    c = lambda_ * norm_w ** 2\n",
    "#     print(\"=========================\")\n",
    "#     print(\"loss part 1: {}\".format(value/N))\n",
    "#     print(\"loss part 2, lambda term : {}\".format(c))\n",
    "    return value/N + c \n",
    "\n",
    "\n",
    "\n",
    "def function_gradient_vectorization(X, Y, w, lambda_, gradclip = None):\n",
    "    # x.shape (5231, 2336)\n",
    "    # y.shape (1, 2336)\n",
    "    # w.shape (5231, 1)\n",
    "    # dw.shape (5231,1)\n",
    "    D, N = X.shape\n",
    "    d, _ = Y.shape\n",
    "    \n",
    "    dw = np.zeros((D, d))\n",
    "    Z = Y * np.dot(w.T , X) # (1, 2336)\n",
    "    dw = np.dot(X , (Y/(1 + np.exp(Z))).T)\n",
    "\n",
    "    c = lambda_ * w *2\n",
    "    \n",
    "    if gradclip != None:\n",
    "        c[c>=gradclip] = gradclip\n",
    "    \n",
    "    return c-dw/N\n",
    "\n",
    "\n",
    "\n",
    "def function_gradient(X, Y, w, lambda_, gradclip = None):\n",
    "    # Calculate the gradient here:\n",
    "    # X: DxN\n",
    "    # Y: dxN\n",
    "    # w: Dxd\n",
    "    D, N = X.shape\n",
    "    d, _ = Y.shape\n",
    "    \n",
    "    dw = np.zeros((D, d))\n",
    "    \n",
    "    for i in range(N):\n",
    "            \n",
    "        Z = Y[:,i] * np.dot(w.T , X[:,i].reshape(D,1)) # 1 x 1\n",
    "        dw[:] += X[:,i].reshape(D,1) * Y[:,i]/(1 + np.exp(Z[0]))\n",
    "    c = lambda_ * w *2\n",
    "    \n",
    "    if gradclip != None:\n",
    "        c[c>=gradclip] = gradclip\n",
    "    \n",
    "    return c-dw/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define solvers: GD, SGD, SVRG and SAG. \n",
    "# Setting the values here:\n",
    "\n",
    "alpha = 0.1 # change the value\n",
    "num_iters = 5 # change the value\n",
    "lambda_ = 0.00001 # change the value\n",
    "epsilon = 0.001 # change the value\n",
    "\n",
    "# ---------------------- Complete the blank definitions: --------------------------------------\n",
    "\n",
    "def solver(x,y, w, alpha, num_iters , lambda_ , epsilon , optimizer = \"GD\", batchsize = None, gradclip = None, mem=False):\n",
    "    if (optimizer == \"GD\") :\n",
    "        for i in range(num_iters):\n",
    "            # update the parameter w for GD here:\n",
    "            g = function_gradient_vectorization(x, y, w, lambda_, gradclip)\n",
    "            w = w - alpha*g\n",
    "            if (i%100==0) and (mem):\n",
    "                usage=resource.getrusage(resource.RUSAGE_SELF)\n",
    "                loss = cost_vectorization(x,y,w,lambda_)\n",
    "                print(i, loss)\n",
    "                # print(\"mem for GD (mb):\", (usage[2]*resource.getpagesize())/1000000.0)\n",
    "            if (np.linalg.norm(g) <= epsilon):\n",
    "                break\n",
    "    elif (optimizer == \"SGD\"):\n",
    "        for i in range(num_iters):\n",
    "            # Complete SGD here:\n",
    "                # randomly choose NSample points for calculating the estimated gradient\n",
    "                if (batchsize == None):\n",
    "                    NSample = int(np.random.rand() * x.shape[1])\n",
    "                    while NSample == 0:\n",
    "                        NSample = int(np.random.rand() * x.shape[1])\n",
    "                else:\n",
    "                    NSample = batchsize\n",
    "                randomInd = np.arange(x.shape[1])[:NSample]\n",
    "                g = function_gradient_vectorization(x[:,randomInd], y[:,randomInd], w, lambda_, gradclip = gradclip)\n",
    "                w = w - alpha*g\n",
    "                \n",
    "                if (i%100==0) and (mem):\n",
    "                    usage=resource.getrusage(resource.RUSAGE_SELF)     \n",
    "                    loss = cost_vectorization(x,y,w,lambda_)\n",
    "                    print(i, loss)\n",
    "                    # print(\"mem for GD (mb):\", (usage[2]*resource.getpagesize())/1000000.0)\n",
    "                if (np.linalg.norm(g) <= epsilon):\n",
    "                    break\n",
    "                \n",
    "                \n",
    "    elif (optimizer == \"SVRG\"):\n",
    "        i = 0\n",
    "        T = 2000\n",
    "        K = 1000\n",
    "        N = x.shape[1]\n",
    "        for k in range(K):\n",
    "\n",
    "            # compute all gradient and store\n",
    "            g = function_gradient_vectorization(x, y, w, lambda_, gradclip)\n",
    "            # initialize the w_previous\n",
    "            w_previous = w.copy()\n",
    "            \n",
    "            for t in range(T):\n",
    "                # random sample\n",
    "                if (batchsize == None):\n",
    "                    NSample = int(np.random.rand() * x.shape[1])\n",
    "                    while NSample == 0:\n",
    "                        NSample = int(np.random.rand() * x.shape[1])\n",
    "                else:\n",
    "                    NSample = batchsize\n",
    "                randomInd = np.arange(x.shape[1])\n",
    "                np.random.shuffle(randomInd)\n",
    "                randomInd = randomInd[:NSample]\n",
    "\n",
    "                # calculate the update term\n",
    "                part1 = function_gradient_vectorization(x[:,randomInd], y[:,randomInd], w_previous, lambda_, gradclip = gradclip)\n",
    "                part2 = function_gradient_vectorization(x[:,randomInd], y[:,randomInd], w, lambda_, gradclip = gradclip)\n",
    "                part3 = g\n",
    "\n",
    "                w_previous = w_previous - alpha * (part1 - part2 + part3)\n",
    "                \n",
    "            w = w_previous\n",
    "            i = i+1\n",
    "\n",
    "            if (i%100==0) and (mem):\n",
    "                    usage=resource.getrusage(resource.RUSAGE_SELF)\n",
    "                    loss = cost_vectorization(x,y,w,lambda_)\n",
    "                    print(i, loss)\n",
    "                    # print(\"mem for GD (mb):\", (usage[2]*resource.getpagesize())/1000000.0)\n",
    "            if (np.linalg.norm((part1 - part2 + part3)) <= epsilon):\n",
    "                break\n",
    "                \n",
    "    elif (optimizer == \"SAG\"):\n",
    "        # Complete SAG here:\n",
    "        # X: DxN\n",
    "        # Y: dxN\n",
    "        # w: Dxd\n",
    "        D, N = x.shape\n",
    "        d = y.shape[0]\n",
    "        dw = np.zeros_like(w, dtype=np.float)\n",
    "        gk = np.zeros((N,D,d), dtype=np.float)\n",
    "\n",
    "\n",
    "        for k in range(num_iters):\n",
    "            NSample  = batchsize\n",
    "\n",
    "            randomInd = np.arange(x.shape[1])\n",
    "            np.random.shuffle(randomInd)\n",
    "            index_all = randomInd[:NSample]\n",
    "\n",
    "            g_i = function_gradient_vectorization(x[:,index_all].reshape(D,index_all.size), y[:,index_all].reshape(d,index_all.size), w, lambda_, gradclip = gradclip)\n",
    "\n",
    "\n",
    "            gk[index_all,:,:] = g_i\n",
    "\n",
    "            dw = np.sum(gk[:,:,:],axis=0)\n",
    "            w = w - alpha * dw/N\n",
    "            if (np.linalg.norm(dw/N) <= epsilon):\n",
    "                break\n",
    "\n",
    "            if (k%100==0) and (mem):\n",
    "                usage=resource.getrusage(resource.RUSAGE_SELF)\n",
    "                loss = cost_vectorization(x,y,w,lambda_)\n",
    "                print(k, loss)\n",
    "                # print(\"mem for SAG (mb):\", (usage[2]*resource.getpagesize())/1000000.0)\n",
    "            # i_t = \n",
    "        i=k\n",
    "    return w,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Solving the optimization problem:\n",
    "\n",
    "alpha = 0.01 # change the value\n",
    "num_iters = 100000 # change the value\n",
    "lambda_ = 0.1 # change the value\n",
    "epsilon = 0.01 # change the value\n",
    "\n",
    "w = np.random.normal(0,0.1, D*d).reshape(D,d)\n",
    "#-------------------- GD Solver -----------------------\n",
    "loss = cost_vectorization(x,y,w,lambda_)\n",
    "print(\"initial loss {}\".format(loss))\n",
    "\n",
    "# #-------------------- GD Solver -----------------------\n",
    "print(\"\\nGD optimization\")\n",
    "opt = \"GD\"\n",
    "w = np.random.normal(0,0.1, D*d).reshape(D,d)\n",
    "\n",
    "start_gd = time.time()\n",
    "gde, iters_total = solver(x,y, w, alpha, num_iters  , lambda_ , epsilon , optimizer = opt, mem=True) # complete the command \n",
    "end_gd = time.time()\n",
    "cost_value = cost_vectorization(x,y,gde,lambda_)  # Calculate the cost value\n",
    "print(\"Cost of GD after convergence: \",cost_value)\n",
    "print(\"Training time for GD: \", end_gd-start_gd)\n",
    "print(\"iters_total: \", iters_total)\n",
    "\n",
    "# # # #-------------------- SGD Solver -----------------------\n",
    "opt = 'SGD'\n",
    "print(\"\\nSGD optimization\")\n",
    "w = np.random.normal(0,0.1, D*d).reshape(D,d)\n",
    "\n",
    "start_sgd = time.time()\n",
    "gde, iters_total = solver(x,y, w, alpha, num_iters  , lambda_ , epsilon , optimizer = opt, mem=True) # complete the command \n",
    "end_sgd = time.time()\n",
    "cost_value = cost_vectorization(x,y,gde,lambda_)  # Calculate the cost value\n",
    "print(\"Cost of SGD after convergence: \",cost_value)\n",
    "print(\"Training time for SGD: \", end_sgd-start_sgd)\n",
    "print(\"iters_total: \", iters_total)\n",
    "\n",
    "# #-------------------- SVRG Solver -----------------------\n",
    "\n",
    "\n",
    "opt = 'SVRG'\n",
    "print(\"\\nSVRG optimization\")\n",
    "w = np.random.normal(0,0.1, D*d).reshape(D,d)\n",
    "\n",
    "start_srvg = time.time()\n",
    "gde, iters_total = solver(x,y, w, alpha, num_iters  , lambda_ , epsilon , optimizer = opt, mem=True, batchsize = 50) # complete the command \n",
    "end_svrg = time.time()\n",
    "cost_value = cost_vectorization(x,y,gde,lambda_)  # Calculate the cost value\n",
    "print(\"Cost of SVRG after convergence: \",cost_value)\n",
    "print(\"Training time for SVRG: \", end_svrg-start_srvg)\n",
    "print(\"iters_total: \", iters_total)\n",
    "\n",
    "\n",
    "#-------------------- SAG Solver -----------------------\n",
    "\n",
    "opt = 'SAG'\n",
    "print(\"\\nSAG optimization\")\n",
    "w = np.random.normal(0,0.1, D*d).reshape(D,d)\n",
    "\n",
    "start_sag = time.time()\n",
    "gde, iters_total = solver(x,y, w, alpha, num_iters, lambda_ , epsilon , optimizer = opt, mem=True, batchsize = 50) # complete the command \n",
    "end_sag = time.time()\n",
    "cost_value = cost_vectorization(x,y,gde,lambda_)  # Calculate the cost value\n",
    "print(\"Cost of SAG after convergence: \",cost_value)\n",
    "print(\"Training time for SAG: \", end_sag-start_sag)\n",
    "print(\"iters_total: \", iters_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('sklearn': conda)",
   "language": "python",
   "name": "python38164bitsklearnconda11bdb1c82f4e41ea847f64346c43e799"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}