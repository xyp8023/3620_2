{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports from libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import resource\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4905, 2336)\n",
      "(327, 2336)\n",
      "(4905, 585)\n",
      "(327, 585)\n"
     ]
    }
   ],
   "source": [
    "## Preprocessing of data\n",
    "# Load data here:\n",
    "# get data function\n",
    "def get_data(data_folder = '/Users/wzh/Downloads/EP3260-MLoNs-2020/dataset/ghg_data/'):\n",
    "    data = []\n",
    "    filelist = os.listdir(data_folder)\n",
    "    for file in filelist:\n",
    "#         print(file)\n",
    "        data_single = np.genfromtxt(data_folder+file,dtype=np.float)\n",
    "        data.append(data_single)\n",
    "#     data=np.genfromtxt('./ghg_data/ghg.gid.site2067.dat',dtype=np.float)\n",
    "    return data\n",
    "\n",
    "def splitDataset(totaldata, train = 0.8, seed = 123, normalize = True):\n",
    "    # seed\n",
    "    np.random.seed(seed)\n",
    "    # number\n",
    "    numdata = totaldata.shape[0]\n",
    "    numtrain = int(numdata*train)\n",
    "    numtest = numdata - numtrain\n",
    "    # index\n",
    "    index = np.arange(numdata)\n",
    "    np.random.shuffle(index)\n",
    "    # shuffle\n",
    "    totaldata = totaldata[index,:,:]\n",
    "    # split\n",
    "    traindata = totaldata[:numtrain, :, :]\n",
    "    testdata = totaldata[numtrain:, :, :]\n",
    "    # split X, Y\n",
    "    # train\n",
    "    X_train = traindata[:,:15, :]\n",
    "    X_train = X_train.reshape(numtrain, -1).T\n",
    "    Y_train = traindata[:,15, :].T\n",
    "#     Y_train = Y_train.reshape(numtrain, 1, -1)\n",
    "    # test\n",
    "    X_test = testdata[:,:15, :]\n",
    "    X_test = X_test.reshape(numtest, -1).T\n",
    "    Y_test = testdata[:,15, :].T\n",
    "    \n",
    "    # normalization\n",
    "    if normalize is True:\n",
    "        X_train = X_train/np.linalg.norm(X_train, axis=0)\n",
    "#         Y_train = Y_train/np.linalg.norm(Y_train, axis=0)\n",
    "        X_test = X_test/np.linalg.norm(X_test, axis=0)\n",
    "#         Y_test = Y_test/np.linalg.norm(Y_test, axis=0)\n",
    "    \n",
    "#     Y_test = Y_test.reshape(numtest, 1, -1)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "# Split train and test data here: (X_train, Y_train, X_test, Y_test)\n",
    "data = np.array(get_data())\n",
    "# Split train and test data here: (X_train, Y_train, X_test, Y_test)\n",
    "X_train, Y_train, X_test, Y_test = splitDataset(data)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic ridge regression with different optimizers\n",
    "# cost function and gradient calculation\n",
    "\n",
    "def cost(x,y,w,lambda_ = 0.01):\n",
    "    D, N = x.shape\n",
    "    value = 0\n",
    "    for i in range(N):\n",
    "        Z = -1 * y[:,i] * np.dot(w.T , (x[:,i].reshape(D,1)).reshape(D,1))\n",
    "        value += np.sum(np.log(1+np.exp(Z))) / y.shape[0]\n",
    "    norm_w = np.linalg.norm(w)\n",
    "    c = lambda_ * norm_w ** 2\n",
    "    print(\"=========================\")\n",
    "    print(\"loss part 1: {}\".format(value/N))\n",
    "    print(\"loss part 2, lambda term : {}\".format(c))\n",
    "    return value/N + c \n",
    "\n",
    "def function_gradient(X, Y, w, lambda_, gradclip = None):\n",
    "    # Calculate the gradient here:\n",
    "    # X: DxN\n",
    "    # Y: dxN\n",
    "    # w: Dxd\n",
    "    D, N = X.shape\n",
    "    d, _ = Y.shape\n",
    "    \n",
    "    dw = np.zeros((D, d))\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(d):\n",
    "            Z = Y[j,i] * np.dot(w[:,j].reshape(1,-1) , (X[:,i].reshape(D,1)).reshape(D,1)) # 1 x 1\n",
    "            \n",
    "            dw[:,j] += (X[:,i].reshape(D,1) * Y[j,i]/(1 + np.exp(Z))).reshape(-1) # D x 1 \n",
    "    c = lambda_ * w *2\n",
    "    \n",
    "    if gradclip != None:\n",
    "        c[c>=gradclip] = gradclip\n",
    "    \n",
    "    return c-dw/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define solvers: GD, SGD, SVRG and SAG. \n",
    "# Setting the values here:\n",
    "\n",
    "alpha = 0.1 # change the value\n",
    "num_iters = 5 # change the value\n",
    "lambda_ = 0.0001 # change the value\n",
    "epsilon = 0.001 # change the value\n",
    "\n",
    "# ---------------------- Complete the blank definitions: --------------------------------------\n",
    "\n",
    "def solver(x,y, w, alpha, num_iters , lambda_ , epsilon , optimizer = \"GD\", batchsize = None, gradclip = None, mem=False):\n",
    "    if (optimizer == \"GD\") :\n",
    "        print(num_iters)\n",
    "        for i in range(num_iters):\n",
    "            # update the parameter w for GD here:\n",
    "            g = function_gradient(x, y, w, lambda_, gradclip)\n",
    "            w = w - alpha*g\n",
    "            loss = cost(x,y,w,lambda_)\n",
    "            print(i, loss)\n",
    "            \n",
    "            if (i%10==0) and (mem):\n",
    "                usage=resource.getrusage(resource.RUSAGE_SELF)\n",
    "                print(\"mem for GD (mb):\", (usage[2]*resource.getpagesize())/1000000.0)\n",
    "            if (np.linalg.norm(g) <= epsilon):\n",
    "                break\n",
    "    elif (optimizer == \"SGD\"):\n",
    "        for i in range(num_iters):\n",
    "            # Complete SGD here:\n",
    "                # randomly choose NSample points for calculating the estimated gradient\n",
    "                if (batchsize == None):\n",
    "                    NSample = int(np.random.rand() * x.shape[1])\n",
    "                    while NSample == 0:\n",
    "                        NSample = int(np.random.rand() * x.shape[1])\n",
    "                else:\n",
    "                    NSample = batchsize\n",
    "                randomInd = np.arange(x.shape[1])[:NSample]\n",
    "                g = function_gradient(x[:,randomInd], y[:,randomInd], w, lambda_, gradclip = gradclip)\n",
    "                w = w - alpha*g\n",
    "                loss = cost(x,y,w,lambda_)\n",
    "                print(i, loss)\n",
    "                if (i%10==0) and (mem):\n",
    "                    usage=resource.getrusage(resource.RUSAGE_SELF)\n",
    "                    print(\"mem for GD (mb):\", (usage[2]*resource.getpagesize())/1000000.0)\n",
    "                if (np.linalg.norm(g) <= epsilon):\n",
    "                    break\n",
    "                \n",
    "                \n",
    "    elif (optimizer == \"SVRG\"):\n",
    "        T = 100\n",
    "#         K = math.floor(num_iters/T)\n",
    "        K = 1\n",
    "#         Z = np.matmul(x,np.diagflat(y))\n",
    "        N = x.shape[1]\n",
    "        for k in range(K):\n",
    "#             wz = np.matmul(w.T , Z)\n",
    "#             diag = np.diagflat(1/(1+np.exp(-1*wz))-np.ones((1,N)))\n",
    "#             Ga_ = np.matmul(Z , diag)\n",
    "#             ga_ = (1/N) * np.matmul(Ga_ , np.ones((N,1)))\n",
    "            # compute all gradient and store\n",
    "            g = function_gradient(x, y, w, lambda_, gradclip)\n",
    "            # initialize the w_previous\n",
    "            w_previous = w.copy()\n",
    "            for t in range(T):\n",
    "                # random sample\n",
    "                if (batchsize == None):\n",
    "                    NSample = int(np.random.rand() * x.shape[1])\n",
    "                    while NSample == 0:\n",
    "                        NSample = int(np.random.rand() * x.shape[1])\n",
    "                else:\n",
    "                    NSample = batchsize\n",
    "                randomInd = np.arange(x.shape[1])[:NSample]\n",
    "                # calculate the update term\n",
    "#                 w_previous = w.copy()\n",
    "                part1 = function_gradient(x[:,randomInd], y[:,randomInd], w_previous, lambda_, gradclip = gradclip)\n",
    "                part2 = function_gradient(x[:,randomInd], y[:,randomInd], w, lambda_, gradclip = gradclip)\n",
    "                part3 = g\n",
    "\n",
    "                w_previous = w_previous - alpha * (part1 - part2 + part3)\n",
    "            w = w_previous\n",
    "            loss = cost(x,y,w,lambda_)\n",
    "            print(k, loss)\n",
    "    elif (optimizer == \"SAG\"):\n",
    "        # Complete SAG here:\n",
    "        pass\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "loss part 1: 665.5757992441225\n",
      "loss part 2, lambda term : 16.03470561214139\n",
      "initial loss 681.6105048562639\n",
      "=========================\n",
      "loss part 1: 83.74640882057007\n",
      "loss part 2, lambda term : 16.697125423839157\n",
      "0 100.44353424440922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wzh/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "loss part 1: 72.29033639462165\n",
      "loss part 2, lambda term : 16.694416370583635\n",
      "1 88.98475276520529\n",
      "=========================\n",
      "loss part 1: 65.28615175855205\n",
      "loss part 2, lambda term : 16.691714284624688\n",
      "2 81.97786604317673\n",
      "=========================\n",
      "loss part 1: 60.44703280655829\n",
      "loss part 2, lambda term : 16.688976922426452\n",
      "3 77.13600972898475\n",
      "=========================\n",
      "loss part 1: 56.838483494714474\n",
      "loss part 2, lambda term : 16.68619051306239\n",
      "4 73.52467400777687\n",
      "=========================\n",
      "loss part 1: 54.00584840907755\n",
      "loss part 2, lambda term : 16.68335064577735\n",
      "5 70.6891990548549\n",
      "=========================\n",
      "loss part 1: 51.71262817774921\n",
      "loss part 2, lambda term : 16.680456914303267\n",
      "6 68.39308509205247\n",
      "=========================\n",
      "loss part 1: 49.81375726987277\n",
      "loss part 2, lambda term : 16.67751067487187\n",
      "7 66.49126794474464\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-479eb083099b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# gde = solver(x,y, w, alpha, num_iters , lambda_ , epsilon , optimizer = \"SGD\",mem=False) # complete the command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mgde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iters\u001b[0m  \u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# complete the command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-14cb5471cb48>\u001b[0m in \u001b[0;36msolver\u001b[0;34m(x, y, w, alpha, num_iters, lambda_, epsilon, optimizer, batchsize, gradclip, mem)\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mNSample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrandomInd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mNSample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandomInd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandomInd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-32066ebc3903>\u001b[0m in \u001b[0;36mfunction_gradient\u001b[0;34m(X, Y, w, lambda_, gradclip)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1 x 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mdw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# D x 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Solving the optimization problem:\n",
    "\n",
    "alpha = 0.1 # change the value\n",
    "num_iters = 50 # change the value\n",
    "lambda_ = 0.001 # change the value\n",
    "epsilon = 0.001 # change the value\n",
    "\n",
    "y = Y_train\n",
    "x = X_train\n",
    "D,N = x.shape\n",
    "d,_ = y.shape\n",
    "opt = \"SGD\"\n",
    "\n",
    "# w = np.random.rand(D,1)*0.01  # Initialization of w\n",
    "w = np.random.normal(0,0.1, D*d).reshape(D,d)\n",
    "start = time.time()\n",
    "#-------------------- GD Solver -----------------------\n",
    "loss = cost(x,y,w,lambda_)\n",
    "print(\"initial loss {}\".format(loss))\n",
    "\n",
    "# gde = solver(x,y, w, alpha, num_iters , lambda_ , epsilon , optimizer = \"SGD\",mem=False) # complete the command \n",
    "gde = solver(x,y, w, alpha, num_iters  , lambda_ , epsilon , optimizer = opt, batchsize = 5, mem=False) # complete the command \n",
    "\n",
    "end = time.time()\n",
    "# print(\"Weights of GD after convergence: \\n\",gde)\n",
    "\n",
    "cost_value = cost(x,y,gde,lambda_)  # Calculate the cost value\n",
    "print(\"Cost of GD after convergence: \",cost_value)\n",
    "\n",
    "print(\"Training time for GD: \", end-start)\n",
    "\n",
    "#-------------------- SGD Solver -----------------------\n",
    "# complete here :\n",
    "\n",
    "#-------------------- SVRG Solver -----------------------\n",
    "# complete here :\n",
    "\n",
    "#-------------------- SAG Solver -----------------------\n",
    "# complete here :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Executing the iterations and plot the cost function here:\n",
    "\n",
    "ti= np.zeros((50,4))\n",
    "cost_= np.zeros((50,4))\n",
    "for i in range(50):\n",
    "    print(\"......\",i,\".......\")\n",
    "    #--------------GD-------------------\n",
    "    start = time.time()\n",
    "    gde = solver(x.T,y,w,num_iters=i)\n",
    "    end = time.time()\n",
    "\n",
    "    cost_[i,0] = cost(x.T,y,gde)\n",
    "\n",
    "    ti[i,0] = end-start\n",
    "\n",
    "    #---------------SGD------------------\n",
    "    #Complete for SGD solver here :\n",
    "    \n",
    "    #---------------SVRG----------------\n",
    "    #Complete for SVRG solver here :\n",
    "    \n",
    "    #---------------SAG------------------\n",
    "    #Complete for SAG solver here :\n",
    "    \n",
    "    #------------------------------------\n",
    "    \n",
    "    ## Pl the results:\n",
    "    \n",
    "\n",
    "l0 = plt.plot(cost_[:,0],color=\"red\")\n",
    "# complete other plots here: \n",
    "\n",
    "\n",
    "plt.xlabel(\"Number of Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend(['GD', 'SGD', 'SVRG', 'SAG'])\n",
    "plt.show()\n",
    "\n",
    "l0 = plt.plot(ti[:,0],color=\"red\")\n",
    "# complete other plots here:\n",
    "\n",
    "plt.xlabel(\"Number of Iteration\")\n",
    "plt.ylabel(\"Time (sec)\")\n",
    "plt.legend(['GD', 'SGD', 'SVRG', 'SAG'])\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tunning the hyper-paramter here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparing different optimizers here: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
