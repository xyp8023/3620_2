{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##imports from libraries\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import resource\n",
    "import time\n",
    "from datetime import datetime\n",
    "import math\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "import cvxpy\n",
    "from multiprocessing import Process, Pipe\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import copy\n",
    "\n",
    "###------------------- keras imports for the dataset and neural network --------------------##\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "# from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "from keras.layers.normalization import BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing of data\n",
    "# Load data from keras:\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "# Reshape data for building the input vector from the 28x28 pixels\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalizing the data to help with the training\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# one-hot encoding using keras' numpy-related utilities\n",
    "n_classes = 10 #complete\n",
    "Y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "Y_test = keras.utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------Deep Nural Network (DNN) -------------------------------------------\n",
    "#----- Part (a) --------------------------------------------------------------------------\n",
    "\n",
    "# Building a linear stack of layers with the sequential model\n",
    "\n",
    "modelori = Sequential()\n",
    "modelori.add(Dense(10, input_shape=(784,)))\n",
    "modelori.add(Activation('relu'))                            \n",
    "\n",
    "modelori.add(Dense(10))\n",
    "modelori.add(Activation('relu'))\n",
    "\n",
    "modelori.add(Dense(10))\n",
    "modelori.add(Activation('softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question a\n",
    "Train the DNN with SGD (batchsize=1). Record and show the performance. Visulize 9 correct/incorrect predicted samples respectively.\n",
    "\n",
    "> lr= 0.001 , decay= 0.01 , momentum= 0.9 , nesterov= True , epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zehang/anaconda3/envs/mlon/lib/python3.6/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d0b857795100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m           validation_data=(X_test, Y_test))\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlon/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/mlon/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    199\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlon/lib/python3.6/site-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         if (self._delta_t_batch > 0. and\n\u001b[1;32m     90\u001b[0m            \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.95\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_t_batch\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlon/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3500\u001b[0m     \"\"\"\n\u001b[1;32m   3501\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3502\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3503\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlon/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3408\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3410\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3411\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlon/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   3548\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3549\u001b[0m         \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3550\u001b[0;31m     \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3552\u001b[0m     \u001b[0;31m# Check if the array contains any nan's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### -------------------------- Compiling the sequential model ---------------------------------\n",
    "# sgd\n",
    "sgd = optimizers.SGD(lr= 0.001 , decay= 0.0001 , momentum= 0.9 , nesterov= True ) # complete the command  \n",
    "model = copy.copy(modelori)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer= sgd) # complete the command  \n",
    "\n",
    "epochnum = 100\n",
    "\n",
    "##-------------------------- Training and saving the model ----------------------------------------------\n",
    "start = time.time()\n",
    "\n",
    "## Saving metrics in history\n",
    "# Complete the history: \n",
    "history = model.fit(X_train, Y_train,\n",
    "          epochs= epochnum,\n",
    "          verbose= 0 ,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test, Y_test))\n",
    "end = time.time()\n",
    "\n",
    "# saving the model\n",
    "save_dir = \"output/result/\" # Complete\n",
    "model_name = 'm1.h5' # Complete\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the metrics\n",
    "fig = plt.figure()\n",
    "# ----------------------- Plot your result here ---------------------\n",
    "\n",
    "#Plot here\n",
    "#--------------------------------------------------------------------\n",
    "fig = plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "# print(history.history.keys())\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "## Evaluate the model\n",
    "mnist_model = load_model(model_path)\n",
    "loss_and_metrics = mnist_model.evaluate(X_test, Y_test, verbose=2) #complete  \n",
    "\n",
    "print(\"Test Loss\", loss_and_metrics[0])\n",
    "print(\"Test Accuracy\", loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Load the model and create predictions on the test set --------------------\n",
    "mnist_model = load_model(model_path)\n",
    "predicted_classes = mnist_model.predict_classes(X_test)\n",
    "\n",
    "## --------- See which we predicted correctly and which not-----------\n",
    "correct_indices = np.where(predicted_classes == y_test)[0]\n",
    "incorrect_indices = np.where(predicted_classes != y_test)[0]\n",
    "\n",
    "print(len(correct_indices),\" classified correctly\")\n",
    "print(len(incorrect_indices),\" classified incorrectly\")\n",
    "\n",
    "# Adapt figure size to accomodate 18 subplots\n",
    "plt.rcParams['figure.figsize'] = (7,6)\n",
    "\n",
    "figure_evaluation_correct = plt.figure()\n",
    "####----------------------------------------- Plot the results----------------\n",
    "# plot 9 correct predictions\n",
    "## plot here....\n",
    "\n",
    "# sample 9 correct predicted images\n",
    "X_correct_id = np.random.choice(correct_indices, 9)\n",
    "X_correct = X_test[X_correct_id]\n",
    "# 9 correct predicted samples\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_correct[i].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Predicted: {}, Truth: {}\".format(predicted_classes[X_correct_id[i]], y_test[X_correct_id[i]]))\n",
    "\n",
    "figure_evaluation_incorrect = plt.figure()\n",
    "# 9 incorrect predicted samples\n",
    "X_incorrect_id = np.random.choice(incorrect_indices, 9)\n",
    "X_incorrect = X_test[X_incorrect_id]\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_incorrect[i].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Predicted: {}, Truth: {}\".format(predicted_classes[X_incorrect_id[i]], y_test[X_incorrect_id[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question b (1)\n",
    "Train the DNN with SGD (batchsize=256). Record and show the performance. Visulize 9 correct/incorrect predicted samples respectively.\n",
    "\n",
    "> lr= 0.001 , decay= 0.01 , momentum= 0.9 , nesterov= True , epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zehang/anaconda3/envs/mlon/lib/python3.6/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "##------------------------- Part (b) -----------------------------------\n",
    "# Repeat part (a) with mini-batch GD and compare the results:\n",
    "##### -------------------------- Compiling the sequential model ---------------------------------\n",
    "sgd = optimizers.SGD(lr= 0.001 , decay= 0.0001, momentum= 0.9 , nesterov= True ) # complete the command\n",
    "model = copy.copy(modelori)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer= sgd) # complete the command  \n",
    "\n",
    "epochnum = 100\n",
    "batchsize = 256\n",
    "\n",
    "##-------------------------- Training and saving the model ----------------------------------------------\n",
    "start = time.time()\n",
    "\n",
    "## Saving metrics in history\n",
    "\n",
    "# Complete the history: \n",
    "history = model.fit(X_train, Y_train,\n",
    "          epochs= epochnum,\n",
    "          batch_size=batchsize,\n",
    "          verbose= 0 ,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test, Y_test))\n",
    "end = time.time()\n",
    "\n",
    "# saving the model\n",
    "save_dir = \"output/result/\" # Complete\n",
    "model_name = 'm1_minibatch_{}_{}.h5'.format(batchsize, \"sgd\") # Complete\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "\n",
    "# plotting the metrics\n",
    "fig = plt.figure()\n",
    "# ----------------------- Plot your result here ---------------------\n",
    "\n",
    "#Plot here\n",
    "#--------------------------------------------------------------------\n",
    "fig = plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "# print(history.history.keys())\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "## Evaluate the model\n",
    "mnist_model = load_model(model_path)\n",
    "loss_and_metrics = mnist_model.evaluate(X_test, Y_test, verbose=2) #complete  \n",
    "\n",
    "print(\"Test Loss\", loss_and_metrics[0])\n",
    "print(\"Test Accuracy\", loss_and_metrics[1])\n",
    "\n",
    "# ------------------------- Load the model and create predictions on the test set --------------------\n",
    "mnist_model = load_model(model_path)\n",
    "predicted_classes = mnist_model.predict_classes(X_test)\n",
    "\n",
    "## --------- See which we predicted correctly and which not-----------\n",
    "correct_indices = np.where(predicted_classes == y_test)[0]\n",
    "incorrect_indices = np.where(predicted_classes != y_test)[0]\n",
    "\n",
    "print(len(correct_indices),\" classified correctly\")\n",
    "print(len(incorrect_indices),\" classified incorrectly\")\n",
    "\n",
    "# Adapt figure size to accomodate 18 subplots\n",
    "plt.rcParams['figure.figsize'] = (7,6)\n",
    "\n",
    "figure_evaluation_correct = plt.figure()\n",
    "####----------------------------------------- Plot the results----------------\n",
    "# plot 9 correct predictions\n",
    "## plot here....\n",
    "\n",
    "# sample 9 correct predicted images\n",
    "X_correct_id = np.random.choice(correct_indices, 9)\n",
    "X_correct = X_test[X_correct_id]\n",
    "# 9 correct predicted samples\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_correct[i].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Predicted: {}, Truth: {}\".format(predicted_classes[X_correct_id[i]], y_test[X_correct_id[i]]))\n",
    "\n",
    "figure_evaluation_incorrect = plt.figure()\n",
    "# 9 incorrect predicted samples\n",
    "X_incorrect_id = np.random.choice(incorrect_indices, 9)\n",
    "X_incorrect = X_test[X_incorrect_id]\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_incorrect[i].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Predicted: {}, Truth: {}\".format(predicted_classes[X_incorrect_id[i]], y_test[X_incorrect_id[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question b (2)\n",
    "Compare the training performance among constant lr, diminishing, AdaGrad, RMSProp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_constant = optimizers.SGD(lr= 0.001 , momentum= 0.9 , nesterov= True ) # complete the command  \n",
    "sgd_diminishing = optimizers.SGD(lr= 0.001 , decay= 0.0001 , momentum= 0.9 , nesterov= True ) # complete the command \n",
    "adagrad = optimizers.Adagrad(lr=0.001, epsilon=1e-6)\n",
    "rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-6)\n",
    "epochnum = 200\n",
    "batchsize = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgd constant\n",
    "model = copy.copy(modelori)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer= sgd_constant) # complete the command  \n",
    "\n",
    "##-------------------------- Training and saving the model ----------------------------------------------\n",
    "start = time.time()\n",
    "\n",
    "## Saving metrics in history\n",
    "\n",
    "# Complete the history: \n",
    "history_sgdconstant = model.fit(X_train, Y_train,\n",
    "          epochs= epochnum,\n",
    "          batch_size=batchsize,\n",
    "          verbose= 0 ,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test, Y_test))\n",
    "end = time.time()\n",
    "print(\"time 1: {}\".format(end-start))\n",
    "# saving the model\n",
    "save_dir = \"output/result/\" # Complete\n",
    "model_name = 'm1_minibatch_{}_sgdconstant.h5'.format(batchsize) # Complete\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##sgd diminishing\n",
    "model = copy.copy(modelori)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer= sgd_diminishing) # complete the command  \n",
    "start = time.time()\n",
    "history_sgddiminishing = model.fit(X_train, Y_train,\n",
    "          epochs= epochnum,\n",
    "          batch_size=batchsize,\n",
    "          verbose= 0 ,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test, Y_test))\n",
    "end = time.time()\n",
    "print(\"time 2: {}\".format(end-start))\n",
    "# saving the model\n",
    "save_dir = \"output/result/\" # Complete\n",
    "model_name = 'm1_minibatch_{}_sgd_diminishing.h5'.format(batchsize) # Complete\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adagrad\n",
    "model = copy.copy(modelori)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer= adagrad) # complete the command  \n",
    "start = time.time()\n",
    "history_adagrad = model.fit(X_train, Y_train,\n",
    "          epochs= epochnum,\n",
    "          batch_size=batchsize,\n",
    "          verbose= 0 ,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test, Y_test))\n",
    "end = time.time()\n",
    "print(\"time 3: {}\".format(end-start))\n",
    "# saving the model\n",
    "save_dir = \"output/result/\" # Complete\n",
    "model_name = 'm1_minibatch_{}_adagrad.h5'.format(batchsize) # Complete\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmsprop\n",
    "model = copy.copy(modelori)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer= rmsprop) # complete the command  \n",
    "start = time.time()\n",
    "history_rmsprop = model.fit(X_train, Y_train,\n",
    "          epochs= epochnum,\n",
    "          batch_size=batchsize,\n",
    "          verbose= 0 ,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test, Y_test))\n",
    "end = time.time()\n",
    "print(\"time 4: {}\".format(end-start))\n",
    "# saving the model\n",
    "save_dir = \"output/result/\" # Complete\n",
    "model_name = 'm1_minibatch_{}_rmsprop.h5'.format(batchsize) # Complete\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training performance wrt epoch\n",
    "train_performance_q2 = plt.figure()\n",
    "plt.plot(history_sgdconstant.history['accuracy'])\n",
    "plt.plot(history_sgddiminishing.history['accuracy'])\n",
    "plt.plot(history_adagrad.history['accuracy'])\n",
    "plt.plot(history_rmsprop.history['accuracy'])\n",
    "plt.title('accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['sgd_constant', 'sgd_dimi','adagrad','rmsprop'], loc='lower right')\n",
    "# plt.ylim([0.5,1.0])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how the loss\n",
    "# plot the training loss wrt epoch\n",
    "train_performance_q2_loss = plt.figure()\n",
    "plt.plot(history_sgdconstant.history['loss'])\n",
    "plt.plot(history_sgddiminishing.history['loss'])\n",
    "plt.plot(history_adagrad.history['loss'])\n",
    "plt.plot(history_rmsprop.history['loss'])\n",
    "plt.title('loss')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['sgd_constant', 'sgd_dimi','adagrad','rmsprop'], loc='lower right')\n",
    "# plt.ylim([0.5,1.0])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem c\n",
    "Examine shallow network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------- Part (c) ------------------------------------\n",
    "# Repeat part (a) by fixing \\sum_j N_j and discuss the results\n",
    "# Building a shallow network\n",
    "model_shallow = Sequential()\n",
    "model_shallow.add(Dense(20, input_shape=(784,)))\n",
    "model_shallow.add(Activation('relu'))                            \n",
    "model_shallow.add(Dense(10))\n",
    "model_shallow.add(Activation('softmax'))\n",
    "# building a deeper network\n",
    "model_deeper = Sequential()\n",
    "model_deeper.add(Dense(5, input_shape=(784,)))\n",
    "model_deeper.add(Activation('relu'))                            \n",
    "model_deeper.add(Dense(5))\n",
    "model_deeper.add(Activation('relu'))\n",
    "model_deeper.add(Dense(5))\n",
    "model_deeper.add(Activation('relu'))\n",
    "model_deeper.add(Dense(5))\n",
    "model_deeper.add(Activation('relu'))\n",
    "model_deeper.add(Dense(10))\n",
    "model_deeper.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##sgd diminishing\n",
    "sgd_diminishing = optimizers.SGD(lr= 0.001 , decay= 0.0001 , momentum= 0.9 , nesterov= True ) # complete the command \n",
    "model_shallow.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer= sgd_diminishing) # complete the command  \n",
    "start = time.time()\n",
    "history_sgddiminishing_shallow = model_shallow.fit(X_train, Y_train,\n",
    "          epochs= epochnum,\n",
    "          batch_size=batchsize,\n",
    "          verbose= 0 ,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test, Y_test))\n",
    "end = time.time()\n",
    "print(\"time 2: {}\".format(end-start))\n",
    "# saving the model\n",
    "save_dir = \"output/result/\" # Complete\n",
    "model_name = 'm1_minibatch_{}_history_sgddiminishing_shallow.h5'.format(batchsize) # Complete\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##sgd diminishing\n",
    "sgd_diminishing = optimizers.SGD(lr= 0.001 , decay= 0.0001 , momentum= 0.9 , nesterov= True ) # complete the command \n",
    "model_deeper.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer= sgd_diminishing) # complete the command  \n",
    "start = time.time()\n",
    "history_sgddiminishing_deeper = model_deeper.fit(X_train, Y_train,\n",
    "          epochs= epochnum,\n",
    "          batch_size=batchsize,\n",
    "          verbose= 0 ,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test, Y_test))\n",
    "end = time.time()\n",
    "print(\"time: {}\".format(end-start))\n",
    "# saving the model\n",
    "save_dir = \"output/result/\" # Complete\n",
    "model_name = 'm1_minibatch_{}_history_sgddiminishing_deeper.h5'.format(batchsize) # Complete\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training performance wrt epoch\n",
    "train_performance_q3 = plt.figure()\n",
    "plt.plot(history_sgddiminishing.history['accuracy'])\n",
    "plt.plot(history_sgddiminishing_shallow.history['accuracy'])\n",
    "plt.plot(history_sgddiminishing_deeper.history['accuracy'])\n",
    "plt.title('accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['sgd_dimi_ori', 'sgd_dimi_shallow', 'sgd_dimi_deeper'])#, loc='lower right')\n",
    "# plt.ylim([0.5,1.0])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how the loss\n",
    "# plot the training loss wrt epoch\n",
    "train_performance_q3_loss = plt.figure()\n",
    "plt.plot(history_sgddiminishing.history['loss'])\n",
    "plt.plot(history_sgddiminishing_shallow.history['loss'])\n",
    "plt.plot(history_sgddiminishing_deeper.history['loss'])\n",
    "plt.title('loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['sgd_dimi_ori', 'sgd_dimi_shallow', 'sgd_dimi_deeper'])#, loc='lower right')\n",
    "# plt.ylim([0.5,1.0])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quetstion 4\n",
    "6 random disjoint workers, master worker computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "numworker = 6\n",
    "randind = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(randind)\n",
    "randset = np.split(randind, numworker)\n",
    "print(X_train[randset[0]].shape)\n",
    "print(Y_train[randset[0]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------- Part (d) -------------------------------------\n",
    "\n",
    "# Split the dataset to 6 random disjoint subsets, each for one worker, and repeat part (a) on master-worker computational graph.\n",
    "numworker = 6\n",
    "batchsize = 256\n",
    "randind = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(randind)\n",
    "randset = np.split(randind, numworker)\n",
    "\n",
    "## Prepare the data here:\n",
    "X_train_split = []\n",
    "Y_train_split = []\n",
    "for i in range(numworker):\n",
    "    X_train_split.append(X_train[randset[i]])\n",
    "    Y_train_split.append(Y_train[randset[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 1s - loss: 2.2435 - accuracy: 0.1368\n",
      "Epoch 1/1\n",
      " - 1s - loss: 2.1859 - accuracy: 0.1788\n",
      "Epoch 1/1\n",
      " - 1s - loss: 2.1945 - accuracy: 0.2006\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.1795 - accuracy: 0.2311\n",
      "Epoch 1/1\n",
      " - 1s - loss: 2.1238 - accuracy: 0.1932\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.2319 - accuracy: 0.1839\n"
     ]
    }
   ],
   "source": [
    "## Compute the weights and DNN layers here:\n",
    "models = []\n",
    "initial_weight_1 = np.zeros((784,10))\n",
    "initial_weight_2 = np.zeros((10,10))\n",
    "initial_weight_3 = np.zeros((10,10))\n",
    "epochnum = 1\n",
    "for i in range(numworker):\n",
    "    model_param = dict()\n",
    "    model = Sequential()\n",
    "    l1 = Dense(10, input_shape=(784,), activation='relu')\n",
    "    model.add(l1)\n",
    "    l1_weights = l1.get_weights()\n",
    "    model_param['model'] = model\n",
    "    model_param['l1_weight'] = l1_weights\n",
    "    initial_weight_1 += l1_weights[0]\n",
    "\n",
    "    l2 = Dense(10, activation = 'relu')\n",
    "    model.add(l2)\n",
    "    l2_weights = l2.get_weights()\n",
    "    model_param['l2_weight'] = l2_weights\n",
    "    initial_weight_2 += l2_weights[0]\n",
    "\n",
    "    l3 = Dense(10, activation = 'softmax')\n",
    "    model.add(l3)\n",
    "    l3_weights = l3.get_weights()\n",
    "    model_param['l3_weight'] = l3_weights\n",
    "    initial_weight_3 += l3_weights[0]\n",
    "\n",
    "    # compiling the sequential model\n",
    "    sgd = optimizers.SGD(lr=0.001, decay=0.01, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "\n",
    "    history = model.fit(X_train_split[i], Y_train_split[i],\n",
    "          epochs= epochnum,\n",
    "          verbose= 2 ,\n",
    "          shuffle=True)\n",
    "    \n",
    "    models.append(model_param)\n",
    "\n",
    "## Complile the sequential model here:\n",
    "\n",
    "\n",
    "## Plotting the metrics here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##--------------------------- Part (e) ---------------------------------\n",
    "\n",
    "# Building a linear stack of layers with the sequential model here:\n",
    "## You can use this command: model.add(Dropout(...))\n",
    "# Building a linear stack of layers with the sequential model\n",
    "\n",
    "batchsize = 256\n",
    "epochnum = 200\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_shape=(784,)))\n",
    "model.add(Activation('relu'))                            \n",
    "model.add(Dense(10))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compiling the sequential model\n",
    "sgd_diminishing = optimizers.SGD(lr=0.001, decay=0.0001, momentum=0.9, nesterov=True)\n",
    "# Training the model and saving metrics in history here:\n",
    "##sgd diminishing\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer= sgd_diminishing) # complete the command  \n",
    "start = time.time()\n",
    "history_sgddiminishing_dropout = model.fit(X_train, Y_train,\n",
    "          epochs= epochnum,\n",
    "          batch_size=batchsize,\n",
    "          verbose= 0 ,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test, Y_test))\n",
    "end = time.time()\n",
    "print(\"time 2: {}\".format(end-start))\n",
    "# saving the model\n",
    "save_dir = \"output/result/\" # Complete\n",
    "model_name = 'm1_minibatch_{}_sgd_diminishing_dropout.h5'.format(batchsize) # Complete\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training performance wrt epoch\n",
    "# compare with the original network\n",
    "train_performance_q5 = plt.figure()\n",
    "plt.plot(history_sgddiminishing.history['accuracy'])\n",
    "plt.plot(history_sgddiminishing_dropout.history['accuracy'])\n",
    "plt.title('accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['sgd_dimi_ori', 'sgd_dimi_drop'])#, loc='lower right')\n",
    "# plt.ylim([0.5,1.0])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "Using batchnorm to improve smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##--------------------------- Part (f) ---------------------------------\n",
    "\n",
    "# Building a linear stack of layers with the sequential model here:\n",
    "## You can use this command: model.add(BatchNormalization())\n",
    "\n",
    "# # no bn\n",
    "# batchsize = 256\n",
    "# epochnum = 200\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(10, input_shape=(784,)))\n",
    "# model.add(Activation('relu'))                            \n",
    "# model.add(Dense(10))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(10))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "# # compiling the sequential model\n",
    "# sgd_diminishing = optimizers.SGD(lr=0.001, decay=0.01, momentum=0.9, nesterov=True)\n",
    "# model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer= sgd_diminishing) # complete the command  \n",
    "# start = time.time()\n",
    "# history_sgddiminishing_nobn = model.fit(X_train, Y_train,\n",
    "#           epochs= epochnum,\n",
    "#           batch_size=batchsize,\n",
    "#           verbose= 0 ,\n",
    "#           shuffle=True,\n",
    "#           validation_data=(X_test, Y_test))\n",
    "# end = time.time()\n",
    "# print(\"time 2: {}\".format(end-start))\n",
    "# # saving the model\n",
    "# save_dir = \"output/result/\" # Complete\n",
    "# model_name = 'm1_minibatch_{}_sgd_diminishing_nobn.h5'.format(batchsize) # Complete\n",
    "# model_path = os.path.join(save_dir, model_name)\n",
    "# model.save(model_path)\n",
    "\n",
    "\n",
    "# bn\n",
    "\n",
    "\n",
    "batchsize = 128\n",
    "epochnum = 200\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_shape=(784,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))                            \n",
    "model.add(Dense(10))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compiling the sequential model\n",
    "sgd_diminishing = optimizers.SGD(lr=0.001, decay=0.0001, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer= sgd_diminishing) # complete the command  \n",
    "start = time.time()\n",
    "history_sgddiminishing_bn = model.fit(X_train, Y_train,\n",
    "          epochs= epochnum,\n",
    "          batch_size=batchsize,\n",
    "          verbose= 0 ,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test, Y_test))\n",
    "end = time.time()\n",
    "print(\"time 2: {}\".format(end-start))\n",
    "# saving the model\n",
    "save_dir = \"output/result/\" # Complete\n",
    "model_name = 'm1_minibatch_{}_sgd_diminishing_bn.h5'.format(batchsize) # Complete\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training performance wrt epoch\n",
    "# compare with the original network\n",
    "train_performance_q6 = plt.figure()\n",
    "plt.plot(history_sgddiminishing_nobn.history['accuracy'])\n",
    "plt.plot(history_sgddiminishing_bn.history['accuracy'])\n",
    "plt.title('accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['sgd_dimi_nobn', 'sgd_dimi_bn'])#, loc='lower right')\n",
    "# plt.ylim([0.5,1.0])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlon",
   "language": "python",
   "name": "mlon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
