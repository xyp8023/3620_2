{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing of data\n",
    "# Function to load data\n",
    "\n",
    "def get_power_data():\n",
    "    \"\"\"\n",
    "    Read the Individual household electric power consumption dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assume that the dataset is located on folder \"data\"\n",
    "    data = pd.read_csv('./household_power_consumption.txt',\n",
    "                       sep=';', low_memory=False)\n",
    "#     data = pd.read_csv('../../dataset/household_power_consumption.txt',\n",
    "#                        sep=';', low_memory=False)\n",
    "\n",
    "    # Drop some non-predictive variables\n",
    "    data = data.drop(columns=['Date', 'Time'], axis=1)\n",
    "\n",
    "    #print(data.head())\n",
    "\n",
    "    # Replace missing values\n",
    "    data = data.replace('?', np.nan)\n",
    "\n",
    "    # Drop NA\n",
    "    data = data.dropna(axis=0)\n",
    "\n",
    "    # Normalize\n",
    "    standard_scaler = preprocessing.StandardScaler()\n",
    "    np_scaled = standard_scaler.fit_transform(data)\n",
    "    data = pd.DataFrame(np_scaled)\n",
    "\n",
    "    # Goal variable assumed to be the first\n",
    "    X = data.values[:, 1:].astype('float32')\n",
    "    y = data.values[:, 0].astype('float32')\n",
    "\n",
    "    # Create categorical y for binary classification with balanced classes\n",
    "    y = np.sign(y+0.46)\n",
    "\n",
    "    # Split train and test data here: (X_train, Y_train, X_test, Y_test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    no_class = 2                 #binary classification\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, no_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X,y types: <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "X size (1536960, 6)\n",
      "Y size (1536960,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, no_class = get_power_data()\n",
    "print(\"X,y types: {} {}\".format(type(X_train), type(y_train)))\n",
    "print(\"X size {}\".format(X_train.shape))\n",
    "print(\"Y size {}\".format(y_train.shape))\n",
    "\n",
    "# Create a binary variable from one of the columns.\n",
    "# You can use this OR not\n",
    "\n",
    "idx = y_train >= 0\n",
    "notidx = y_train < 0\n",
    "y_train[idx] = 1\n",
    "y_train[notidx] = -1\n",
    "\n",
    "\n",
    "# X_test = X_test/np.linalg.norm(X_test)\n",
    "# X_train = X_train/np.linalg.norm(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_train.shape)\n",
    "# #print(X_train.min())\n",
    "# print(X_test.max(), X_test.min())\n",
    "\n",
    "\n",
    "# print(X_test.max(), X_test.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(x, derivative=False):\n",
    "    sigm = 1. / (1. + np.exp(-x)) \n",
    "    if derivative:\n",
    "        return sigm * (1. - sigm)\n",
    "    return sigm\n",
    "\n",
    "# Define weights initialization\n",
    "def initialize_w(N, d):\n",
    "    return 2*np.random.random((N,d)) - 1\n",
    "\n",
    "# Fill in feed forward propagation\n",
    "def feed_forward_propagation(X, y, w_1, w_2, w_3, lmbda):\n",
    "    # Fill in\n",
    "    # X (N,d)\n",
    "    # w_1 (d,h)\n",
    "    # w_2 (h,g)\n",
    "    # w_3 (g,1)\n",
    "    N,d = X.shape\n",
    "    layer_0 = X # (N,d)\n",
    "    layer_1 = sigmoid(np.dot(layer_0, w_1)) # (N, h)\n",
    "    layer_2 = sigmoid(np.dot(layer_1, w_2)) # (N, g)\n",
    "    layer_3 = np.dot(layer_2, w_3) # (N, 1)\n",
    "    \n",
    "    return layer_0, layer_1, layer_2, layer_3\n",
    "def back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda):\n",
    "    N = y.shape[0]\n",
    "    y = y.reshape((-1,1))\n",
    "    layer_3_delta = np.zeros_like(w_3) # (g,1)\n",
    "    layer_2_delta = np.zeros_like(w_2) # (h, g)\n",
    "    layer_1_delta = np.zeros_like(w_1) # (d, h)\n",
    "    layer_3_delta = 2 * np.dot(layer_2.T, (layer_3 - y)) # (g,1)\n",
    "    # print(\"np.dot(w_3,(layer_3-y)) shape \", np.dot(w_3,(layer_3-y).T).shape)\n",
    "    # print(\" sigmoid(np.dot(layer_1,w_2), derivative=True).T shape\", sigmoid(np.dot(layer_1,w_2), derivative=True).T.shape)\n",
    "    dJ_dl2 = 2 * np.dot(w_3,(layer_3-y).T) # # (g,N)\n",
    "    dl2_ds2 = sigmoid(np.dot(layer_1,w_2), derivative=True).T # (g,N)\n",
    "\n",
    "    layer_2_delta  = np.dot(dJ_dl2 * dl2_ds2, layer_1).T    \n",
    "    # layer_2_delta = 2 * np.dot(np.dot(w_3,(layer_3-y).T)*sigmoid(np.dot(layer_1,w_2), derivative=True).T, layer_1).T\n",
    "    ds2_dl1 = w_2 # (h,g)\n",
    "    dl1_ds1 =  sigmoid(np.dot(layer_0,w_1), derivative=True).T # (h,N)\n",
    "    ds1_dw1 = layer_0 # (N,d)\n",
    "\n",
    "    layer_1_delta = np.dot(np.dot(ds2_dl1, dJ_dl2 * dl2_ds2) * dl1_ds1, ds1_dw1).T # (d,h)\n",
    "    return layer_1_delta/N, layer_2_delta/N, layer_3_delta/N\n",
    "\n",
    "def back_propagation_blocklayer(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda, activelayer = 3):\n",
    "    N = y.shape[0]\n",
    "    y = y.reshape((-1,1))\n",
    "    layer_3_delta = np.zeros_like(w_3) # (g,1)\n",
    "    layer_2_delta = np.zeros_like(w_2) # (h, g)\n",
    "    layer_1_delta = np.zeros_like(w_1) # (d, h)\n",
    "    layer_3_delta = 2 * np.dot(layer_2.T, (layer_3 - y)) # (g,1)\n",
    "    \n",
    "    if activelayer == 3:\n",
    "        return layer_1_delta/N, layer_2_delta/N, layer_3_delta/N\n",
    "    \n",
    "    # print(\"np.dot(w_3,(layer_3-y)) shape \", np.dot(w_3,(layer_3-y).T).shape)\n",
    "    # print(\" sigmoid(np.dot(layer_1,w_2), derivative=True).T shape\", sigmoid(np.dot(layer_1,w_2), derivative=True).T.shape)\n",
    "    dJ_dl2 = 2 * np.dot(w_3,(layer_3-y).T) # # (g,N)\n",
    "    dl2_ds2 = sigmoid(np.dot(layer_1,w_2), derivative=True).T # (g,N)\n",
    "\n",
    "    layer_2_delta  = np.dot(dJ_dl2 * dl2_ds2, layer_1).T    \n",
    "    if activelayer == 2:\n",
    "        return layer_1_delta/N, layer_2_delta/N, layer_3_delta/N\n",
    "    \n",
    "    # layer_2_delta = 2 * np.dot(np.dot(w_3,(layer_3-y).T)*sigmoid(np.dot(layer_1,w_2), derivative=True).T, layer_1).T\n",
    "    ds2_dl1 = w_2 # (h,g)\n",
    "    dl1_ds1 =  sigmoid(np.dot(layer_0,w_1), derivative=True).T # (h,N)\n",
    "    ds1_dw1 = layer_0 # (N,d)\n",
    "\n",
    "    layer_1_delta = np.dot(np.dot(ds2_dl1, dJ_dl2 * dl2_ds2) * dl1_ds1, ds1_dw1).T # (d,h)\n",
    "    if activelayer == 1:\n",
    "        return layer_1_delta/N, layer_2_delta/N, layer_3_delta/N\n",
    "\n",
    "# Cost function\n",
    "def cost(X, y, w_1, w_2, w_3, lmbda):\n",
    "    N, d = X.shape\n",
    "    a1,a2,a3,a4 = feed_forward_propagation(X,y,w_1,w_2,w_3,lmbda)\n",
    "\n",
    "#     return np.linalg.norm(a4[:,0] - y,2) ** 2 / N + lmbda * (np.linalg.norm(w_1)**2 + np.linalg.norm(w_2)**2 + np.linalg.norm(w_3)**2)\n",
    "    return np.linalg.norm(a4[:,0] - y,2) ** 2 / N\n",
    "\n",
    "# Define SGD\n",
    "def SGD(X, y, w_1, w_2, w_3, lmbda, learning_rate, batch_size, iterations):\n",
    "    # Complete here:\n",
    "    loss_lst = []\n",
    "    time_lst = []\n",
    "    start = time.time()\n",
    "    y = y.reshape((-1,1))\n",
    "    for i in range(iterations):\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        loss_lst.append(loss)\n",
    "        \n",
    "        randomInd = np.arange(X.shape[0])\n",
    "        np.random.shuffle(randomInd)\n",
    "        randomInd = randomInd[:batch_size]\n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X[randomInd,:],y[randomInd,:],w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y[randomInd,:], w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda)\n",
    "        \n",
    "        g = (np.linalg.norm(layer_1_delta) + np.linalg.norm(layer_2_delta) + np.linalg.norm(layer_3_delta))/3\n",
    "        if i%20==0:\n",
    "            print(\"i, g, loss is \", i, g, loss)   \n",
    "        if (g <= epsilon):\n",
    "#             loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "            time_lst.append(time.time()-start)\n",
    "            print(\"converge, break! current i: \", i, loss)\n",
    "            break\n",
    "        \n",
    "        w_1 = w_1 - learning_rate*layer_1_delta\n",
    "        w_2 = w_2 - learning_rate*layer_2_delta\n",
    "        w_3 = w_3 - learning_rate*layer_3_delta\n",
    "    \n",
    "        \n",
    "#         loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        \n",
    "#         if i%20==0:\n",
    "#             print(i,loss)\n",
    "        time_lst.append(time.time()-start)\n",
    "    return w_1, w_2, w_3, loss_lst, time_lst\n",
    "\n",
    "# Define SVRG here:\n",
    "def SVRG(X, y, w_1, w_2, w_3, lmbda, learning_rate, T, batch_size, iterations):\n",
    "    # Complete here:\n",
    "    y = y.reshape((-1,1))\n",
    "    N = X.shape[0]\n",
    "    loss_lst = []\n",
    "    time_lst = []\n",
    "    start = time.time()\n",
    "    for i in range(iterations):\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        loss_lst.append(loss)\n",
    "        # compute all gradient and store\n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X,y,w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda)\n",
    "            \n",
    "        \n",
    "        g = (np.linalg.norm(layer_1_delta) + np.linalg.norm(layer_2_delta) + np.linalg.norm(layer_3_delta))/3\n",
    "        if i%20==0:\n",
    "            print(\"i, g, loss is \", g, loss)   \n",
    "        if (g <= epsilon):\n",
    "#             loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)  \n",
    "            time_lst.append(time.time()-start)\n",
    "            \n",
    "            print(\"converge, break! current i: \", i, loss)\n",
    "            break\n",
    "        \n",
    "        # initialize the w_previous\n",
    "        # w_previous = w.copy()\n",
    "        w_1_previous, w_2_previous, w_3_previous = w_1.copy(), w_2.copy(), w_3.copy()\n",
    "        for t in range(T//batch_size):\n",
    "            # random sample\n",
    "            # randomInd = int(np.random.rand() * N)\n",
    "            randomInd = np.arange(N)\n",
    "            np.random.shuffle(randomInd)\n",
    "            randomInd = randomInd[:batch_size]\n",
    "            # randomInd = np.random.randint(0,N)\n",
    "            layer_0_p1, layer_1_p1, layer_2_p1, layer_3_p1 = feed_forward_propagation(X[randomInd,:],y[randomInd,:],w_1_previous,w_2_previous,w_3_previous,lmbda)\n",
    "            layer_1_delta_p1, layer_2_delta_p1, layer_3_delta_p1 = back_propagation(y[randomInd,:], w_1_previous, w_2_previous, w_3_previous, layer_0_p1, layer_1_p1, layer_2_p1, layer_3_p1, lmbda)\n",
    "\n",
    "            layer_0_p2, layer_1_p2, layer_2_p2, layer_3_p2 = feed_forward_propagation(X[randomInd,:],y[randomInd,:],w_1,w_2,w_3,lmbda)\n",
    "            layer_1_delta_p2, layer_2_delta_p2, layer_3_delta_p2 = back_propagation(y[randomInd,:], w_1, w_2, w_3, layer_0_p2, layer_1_p2, layer_2_p2, layer_3_p2, lmbda)\n",
    "            \n",
    "            # calculate the update term\n",
    "            # part1 = function_gradient_vectorization(x[:,randomInd], y[:,randomInd], w_previous, lambda_, gradclip = gradclip)\n",
    "            # part2 = function_gradient_vectorization(x[:,randomInd], y[:,randomInd], w, lambda_, gradclip = gradclip)\n",
    "            # part3 = g\n",
    "\n",
    "            w_1_previous = w_1_previous - learning_rate * (layer_1_delta_p1 - layer_1_delta_p2 + layer_1_delta)\n",
    "            w_2_previous = w_2_previous - learning_rate * (layer_2_delta_p1 - layer_2_delta_p2 + layer_2_delta)\n",
    "            w_3_previous = w_3_previous - learning_rate * (layer_3_delta_p1 - layer_3_delta_p2 + layer_3_delta)\n",
    "\n",
    "            # w_previous = w_previous - alpha * (part1 - part2 + part3)\n",
    "            \n",
    "        # w = w_previous\n",
    "        \n",
    "        w_1, w_2, w_3 = w_1_previous, w_2_previous, w_3_previous\n",
    "        time_lst.append(time.time()-start)\n",
    "        \n",
    "#         loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "#         if i%20==0:\n",
    "#             print(i,loss)\n",
    "    \n",
    "    return w_1, w_2, w_3, loss_lst, time_lst\n",
    "\n",
    "# Define GD here:\n",
    "def GD(X, y, w_1,w_2,w_3, lmbda, learning_rate, iterations):\n",
    "    N = X.shape[0]\n",
    "    loss_lst = []\n",
    "    time_lst = []\n",
    "    start = time.time()\n",
    "    for i in range(iterations):\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        loss_lst.append(loss)\n",
    "        \n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda)\n",
    "        \n",
    "        g = (np.linalg.norm(layer_1_delta) + np.linalg.norm(layer_2_delta) + np.linalg.norm(layer_3_delta))/3\n",
    "        if i%20==0:\n",
    "            print(\"i, g, loss is \", i, g, loss)   \n",
    "        if (g <= epsilon):\n",
    "#             loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "            time_lst.append(time.time()-start)\n",
    "            \n",
    "            print(\"converge, break! current i: \", i, loss)\n",
    "            break\n",
    "#         w_1 = w_1 -  learning_rate * layer_1_delta + (lmbda / N * w_1)\n",
    "#         w_2 = w_2 - learning_rate * layer_2_delta + (lmbda / N * w_2)\n",
    "#         w_3 = w_3 - learning_rate * layer_3_delta + (lmbda / N * w_3)\n",
    "        w_1 = w_1 -  learning_rate * layer_1_delta\n",
    "        w_2 = w_2 - learning_rate * layer_2_delta\n",
    "        w_3 = w_3 - learning_rate * layer_3_delta\n",
    "#         loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "#         if i%20==0:\n",
    "#             print(i,loss)\n",
    "        time_lst.append(time.time()-start)\n",
    "\n",
    "    return w_1, w_2, w_3, loss_lst, time_lst\n",
    "\n",
    "# Define projected GD here:\n",
    "def PGD(X, y, w_1,w_2,w_3, lmbda, learning_rate, iterations, noise = None):\n",
    "    # Complete here:\n",
    "    N = X.shape[0]\n",
    "    loss_lst = []\n",
    "    time_lst = []\n",
    "    start = time.time()\n",
    "    for i in range(iterations):\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        loss_lst.append(loss)\n",
    "        \n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda)\n",
    "        \n",
    "        g = (np.linalg.norm(layer_1_delta) + np.linalg.norm(layer_2_delta) + np.linalg.norm(layer_3_delta))/3\n",
    "        if i%20==0:\n",
    "            print(\"i, g, loss is \", i, g, loss)   \n",
    "        if (g <= epsilon) or (loss<0.2):\n",
    "#             loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "            time_lst.append(time.time()-start)\n",
    "            \n",
    "            print(\"converge, break! current i: \", i,loss)\n",
    "            break\n",
    "            \n",
    "        if noise == None:\n",
    "            # the noise level is determined by the delta\n",
    "            noise = []\n",
    "            noise.append((np.random.random((w_1.shape)) * 2 - 1) *np.mean(layer_1_delta))\n",
    "            noise.append((np.random.random((w_2.shape)) * 2 - 1) *np.mean(layer_2_delta))\n",
    "            noise.append((np.random.random((w_3.shape)) * 2 - 1) *np.mean(layer_3_delta))\n",
    "        \n",
    "        w_1 = w_1 -  learning_rate * layer_1_delta + noise[0]*0.01\n",
    "        w_2 = w_2 - learning_rate * layer_2_delta + noise[1]*0.01\n",
    "        w_3 = w_3 - learning_rate * layer_3_delta + noise[2]*0.01\n",
    "#         loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        \n",
    "#         if i%20==0:\n",
    "#             print(i,loss)\n",
    "        time_lst.append(time.time()-start)\n",
    "\n",
    "    return w_1, w_2, w_3, loss_lst, time_lst\n",
    "\n",
    "# Define BCD here:\n",
    "def BCD(X, y, w_1,w_2,w_3, lmbda, learning_rate, iterations, strategy = 0, blockrate = 0.5):\n",
    "    loss_lst = []\n",
    "    time_lst = []\n",
    "    start = time.time()\n",
    "    # Complete here:\n",
    "    # 2 strategies:\n",
    "    # a. randomly select weight by a random mask (random coordinate selection)\n",
    "    # b. we train the layers asynchronously to archieve the goal of blocking coordinate (cyclic update rule) \n",
    "    for i in range(iterations):\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        loss_lst.append(loss)\n",
    "        \n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda)\n",
    "        \n",
    "        g = (np.linalg.norm(layer_1_delta) + np.linalg.norm(layer_2_delta) + np.linalg.norm(layer_3_delta))/3\n",
    "        if i%20==0:\n",
    "            print(\"i, g, loss is \", i, g, loss)   \n",
    "        if (g <= epsilon):\n",
    "            time_lst.append(time.time()-start)\n",
    "            \n",
    "#             loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "            print(\"converge, break! current i: \", i,loss)\n",
    "            break\n",
    "            \n",
    "        if strategy == 0:\n",
    "            layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "            # generate random mask for each layer\n",
    "            l1_mask = np.random.choice([1, 0], size=layer_1_delta.shape, p=[blockrate, 1 - blockrate])\n",
    "            l2_mask = np.random.choice([1, 0], size=layer_2_delta.shape, p=[blockrate, 1 - blockrate])\n",
    "            l3_mask = np.random.choice([1, 0], size=layer_3_delta.shape, p=[blockrate, 1 - blockrate])\n",
    "            w_1 = w_1 - learning_rate * layer_1_delta * l1_mask\n",
    "            w_2 = w_2 - learning_rate * layer_2_delta * l2_mask\n",
    "            w_3 = w_3 - learning_rate * layer_3_delta * l3_mask\n",
    "        else:\n",
    "            # cyclic\n",
    "            # layer 1\n",
    "            layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "            layer_1_delta, _, _ = back_propagation_blocklayer(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda, 1)\n",
    "            w_1 = w_1 - learning_rate * layer_1_delta\n",
    "            # layer 2\n",
    "            layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "            _, layer_2_delta, _ = back_propagation_blocklayer(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda, 2)\n",
    "            w_2 = w_2 - learning_rate * layer_2_delta\n",
    "            # layer 3\n",
    "            layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "            _, _, layer_3_delta = back_propagation_blocklayer(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda, 3)\n",
    "            w_3 = w_3 - learning_rate * layer_3_delta\n",
    "#         loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "#         if i%20==0:\n",
    "#             print(i,loss)\n",
    "        time_lst.append(time.time()-start)\n",
    "\n",
    "    return w_1, w_2, w_3, loss_lst, time_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nw_size = 10\\n\\n# Initialize weights for debug\\nw_1 = initialize_w(X_train.shape[1], w_size)\\n\\nw_2 = initialize_w(w_size,w_size+1)\\n\\nw_3 = initialize_w(w_size+1, 1)\\n\\nlmbda = 0\\n\\nlayer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X_test, y_test, w_1, w_2, w_3, lmbda)\\n\\nprint(\"layer_0 shape \", layer_0.shape)\\nprint(\"layer_1 shape \",layer_1.shape)\\nprint(\"layer_2 shape \",layer_2.shape)\\nprint(\"layer_3 shape \",layer_3.shape)\\n\\nlayer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y_test, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda)\\n\\n\\nprint(\"w_3 shape \",w_3.shape)\\nprint(\"w_2 shape \",w_2.shape)\\nprint(\"w_1 shape \",w_1.shape)\\n\\nprint(\"layer_3_delta shape\", layer_3_delta.shape)\\nprint(\"layer_2_delta shape\", layer_2_delta.shape)\\nprint(\"layer_1_delta shape\", layer_1_delta.shape)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "w_size = 10\n",
    "\n",
    "# Initialize weights for debug\n",
    "w_1 = initialize_w(X_train.shape[1], w_size)\n",
    "\n",
    "w_2 = initialize_w(w_size,w_size+1)\n",
    "\n",
    "w_3 = initialize_w(w_size+1, 1)\n",
    "\n",
    "lmbda = 0\n",
    "\n",
    "layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X_test, y_test, w_1, w_2, w_3, lmbda)\n",
    "\n",
    "print(\"layer_0 shape \", layer_0.shape)\n",
    "print(\"layer_1 shape \",layer_1.shape)\n",
    "print(\"layer_2 shape \",layer_2.shape)\n",
    "print(\"layer_3 shape \",layer_3.shape)\n",
    "\n",
    "layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y_test, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda)\n",
    "\n",
    "\n",
    "print(\"w_3 shape \",w_3.shape)\n",
    "print(\"w_2 shape \",w_2.shape)\n",
    "print(\"w_1 shape \",w_1.shape)\n",
    "\n",
    "print(\"layer_3_delta shape\", layer_3_delta.shape)\n",
    "print(\"layer_2_delta shape\", layer_2_delta.shape)\n",
    "print(\"layer_1_delta shape\", layer_1_delta.shape)\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGD\tinitial loss is : 2.2892894984202625\n",
      "i, g, loss is  0 5.284163179436155 2.2892894984202625\n",
      "i, g, loss is  20 0.4833716362254423 0.4580153892252612\n",
      "i, g, loss is  40 0.35103067554787587 0.3805720560188617\n",
      "i, g, loss is  60 0.31458724850344444 0.35762694828334346\n",
      "i, g, loss is  80 0.29691419970679 0.3489391142626257\n",
      "i, g, loss is  100 0.2833167939304973 0.3458258561892211\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-02e0036bc26c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PGD\\tinitial loss is :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmbda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mstart_pgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mw_1_star\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_2_star\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_3_star\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_pgd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_pgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmbda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mend_pgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training time for PGD: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_pgd\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_pgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-953f3fad5a1f>\u001b[0m in \u001b[0;36mPGD\u001b[0;34m(X, y, w_1, w_2, w_3, lmbda, learning_rate, iterations, noise)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmbda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0mloss_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-953f3fad5a1f>\u001b[0m in \u001b[0;36mcost\u001b[0;34m(X, y, w_1, w_2, w_3, lmbda)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlmbda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_forward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlmbda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m#     return np.linalg.norm(a4[:,0] - y,2) ** 2 / N + lmbda * (np.linalg.norm(w_1)**2 + np.linalg.norm(w_2)**2 + np.linalg.norm(w_3)**2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-953f3fad5a1f>\u001b[0m in \u001b[0;36mfeed_forward_propagation\u001b[0;34m(X, y, w_1, w_2, w_3, lmbda)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mlayer_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;31m# (N,d)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mlayer_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (N, h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mlayer_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (N, g)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mlayer_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (N, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-953f3fad5a1f>\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(x, derivative)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sigmoid function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderivative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msigm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mderivative\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msigm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msigm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Should be a hyperparameter that you tune, not an argument - Fill in the values\n",
    "lmbda =0.001\n",
    "w_size = 50\n",
    "lr = 0.02\n",
    "iterations = 500 # 100\n",
    "T = 2000\n",
    "batch_size = 100\n",
    "epsilon = 0.1\n",
    "\n",
    "# Initialize weights\n",
    "w_1 = initialize_w(X_train.shape[1], w_size)\n",
    "\n",
    "w_2 = initialize_w(w_size,w_size)\n",
    "\n",
    "w_3 = initialize_w(w_size, 1)\n",
    "\n",
    "# print(\"GD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "# start_gd = time.time()\n",
    "# w_1_star,w_2_star,w_3_star,loss_gd,time_gd = GD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations)\n",
    "# end_gd = time.time()\n",
    "# print(\"Training time for GD: \", end_gd-start_gd)\n",
    "\n",
    "print(\"PGD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_pgd = time.time()\n",
    "w_1_star,w_2_star,w_3_star,loss_pgd,time_pgd = PGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations)\n",
    "end_pgd = time.time()\n",
    "print(\"Training time for PGD: \", end_pgd-start_pgd)\n",
    "\n",
    "\n",
    "# print(\"SGD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "# start_sgd = time.time()\n",
    "# w_1_star,w_2_star,w_3_star,loss_sgd,time_sgd = SGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, batch_size, iterations)\n",
    "# end_sgd = time.time()\n",
    "# print(\"Training time for SGD: \", end_sgd-start_sgd)\n",
    "\n",
    "\n",
    "\n",
    "# print(\"SVRG\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "# start_svrg = time.time()\n",
    "# w_1_star1,w_2_star1,w_3_star1,loss_svrg,time_svrg = SVRG(X_train, y_train, w_1, w_2, w_3, lmbda, lr, T, batch_size, iterations)\n",
    "# end_svrg = time.time()\n",
    "# print(\"Training time for SVRG: \", end_svrg-start_svrg)\n",
    "\n",
    "# print(\"BCD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "# strategy = 0 # 0 for random block mask, 1 for layer block\n",
    "# blockrate = 0.5\n",
    "# start_bcd = time.time()\n",
    "# w_1_star,w_2_star,w_3_star ,loss_bcd,time_bcd = BCD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations, strategy, blockrate)\n",
    "# end_bcd = time.time()\n",
    "# print(\"Training time for BCD: \", end_bcd-start_bcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbda =0.01\n",
    "w_size = 50\n",
    "lr = 0.01\n",
    "iterations = 500 # 100\n",
    "T = 2000\n",
    "batch_size = 100\n",
    "epsilon = 0.1\n",
    "\n",
    "# Initialize weights\n",
    "w_1 = initialize_w(X_train.shape[1], w_size)\n",
    "\n",
    "w_2 = initialize_w(w_size,w_size)\n",
    "\n",
    "w_3 = initialize_w(w_size, 1)\n",
    "\n",
    "# print(\"GD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "# start_gd = time.time()\n",
    "# w_1_star,w_2_star,w_3_star, loss_gd1,time_gd1 = GD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations)\n",
    "# end_gd = time.time()\n",
    "# print(\"Training time for GD: \", end_gd-start_gd)\n",
    "\n",
    "print(\"PGD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_pgd = time.time()\n",
    "w_1_star,w_2_star,w_3_star, loss_pgd1,time_pgd1 = PGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations)\n",
    "end_pgd = time.time()\n",
    "print(\"Training time for PGD: \", end_pgd-start_pgd)\n",
    "\n",
    "\n",
    "# print(\"SGD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "# start_sgd = time.time()\n",
    "# w_1_star,w_2_star,w_3_star, loss_sgd1,time_sgd1 = SGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, batch_size, iterations)\n",
    "# end_sgd = time.time()\n",
    "# print(\"Training time for SGD: \", end_sgd-start_sgd)\n",
    "\n",
    "\n",
    "\n",
    "# print(\"SVRG\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "# start_svrg = time.time()\n",
    "# w_1_star1,w_2_star1,w_3_star1, loss_svrg1,time_svrg1 = SVRG(X_train, y_train, w_1, w_2, w_3, lmbda, lr, T, batch_size, iterations)\n",
    "# end_svrg = time.time()\n",
    "# print(\"Training time for SVRG: \", end_svrg-start_svrg)\n",
    "\n",
    "# print(\"BCD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "# strategy = 0 # 0 for random block mask, 1 for layer block\n",
    "# blockrate = 0.5\n",
    "# start_bcd = time.time()\n",
    "# w_1_star,w_2_star,w_3_star, loss_bcd1,time_bcd1 = BCD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations, strategy, blockrate)\n",
    "# end_bcd = time.time()\n",
    "# print(\"Training time for BCD: \", end_bcd-start_bcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGD\tinitial loss is : 20.176253373106913\n",
      "i, g, loss is  0 18.402618930288615 20.176253373106913\n",
      "i, g, loss is  20 1.4032849326877506 1.3583449577411986\n",
      "i, g, loss is  40 1.2965252490048045 1.066252280797918\n",
      "i, g, loss is  60 0.9676234840573926 0.6407774338500488\n",
      "i, g, loss is  80 0.5948413480640112 0.36996117792611144\n"
     ]
    }
   ],
   "source": [
    "lmbda =0.001\n",
    "w_size = 50\n",
    "lr = 0.01\n",
    "iterations = 500 # 100\n",
    "T = 2000\n",
    "batch_size = 100\n",
    "epsilon = 0.1\n",
    "\n",
    "# Initialize weights\n",
    "w_1 = initialize_w(X_train.shape[1], w_size)\n",
    "\n",
    "w_2 = initialize_w(w_size,w_size)\n",
    "\n",
    "w_3 = initialize_w(w_size, 1)\n",
    "\n",
    "# print(\"GD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "# start_gd = time.time()\n",
    "# w_1_star,w_2_star,w_3_star, loss_gd2, time_gd2 = GD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations)\n",
    "# end_gd = time.time()\n",
    "# print(\"Training time for GD: \", end_gd-start_gd)\n",
    "\n",
    "print(\"PGD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_pgd = time.time()\n",
    "w_1_star,w_2_star,w_3_star, loss_pgd2,time_pgd2 = PGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations)\n",
    "end_pgd = time.time()\n",
    "print(\"Training time for PGD: \", end_pgd-start_pgd)\n",
    "\n",
    "\n",
    "# print(\"SGD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "# start_sgd = time.time()\n",
    "# w_1_star,w_2_star,w_3_star, loss_sgd2,time_sgd2 = SGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, batch_size, iterations)\n",
    "# end_sgd = time.time()\n",
    "# print(\"Training time for SGD: \", end_sgd-start_sgd)\n",
    "\n",
    "\n",
    "\n",
    "# print(\"SVRG\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "# start_svrg = time.time()\n",
    "# w_1_star1,w_2_star1,w_3_star2, loss_svrg1,time_svrg2 = SVRG(X_train, y_train, w_1, w_2, w_3, lmbda, lr, T, batch_size, iterations)\n",
    "# end_svrg = time.time()\n",
    "# print(\"Training time for SVRG: \", end_svrg-start_svrg)\n",
    "\n",
    "# print(\"BCD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "# strategy = 0 # 0 for random block mask, 1 for layer block\n",
    "# blockrate = 0.5\n",
    "# start_bcd = time.time()\n",
    "# w_1_star,w_2_star,w_3_star, loss_bcd2,time_bcd2 = BCD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations, strategy, blockrate)\n",
    "# end_bcd = time.time()\n",
    "# print(\"Training time for BCD: \", end_bcd-start_bcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be a hyperparameter that you tune, not an argument - Fill in the values\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lambda', type=float, default=0., dest='lmbda') \n",
    "parser.add_argument('--w_size', type=int, default=10, dest='w_size')\n",
    "parser.add_argument('--lr', type=float, default=0.01)\n",
    "parser.add_argument('--iterations', type=int, default=10)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# Initialize weights\n",
    "w_1 = initialize_w(X_train.shape[1], args.w_size)\n",
    "\n",
    "w_2 = initialize_w(args.w_size,args.w_size)\n",
    "\n",
    "w_3 = initialize_w(args.w_size, 1)\n",
    "\n",
    "# Get iterations\n",
    "iterations = args.iterations\n",
    "# Define plotting variables\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "# Define the optimizers for the loop\n",
    "optimizers = [\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": SGD(X_train, y_train, w_1, w_2, w_3, args.lmbda, args.lr, batch_size),\n",
    "            \"name\": \"SGD\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": SVRG(X_train, y_train, w_1, w_2, w_3, args.lmbda, args.lr),\n",
    "            \"name\": \"SVRG\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": GD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations),\n",
    "            \"name\": \"GD\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": PGD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations, noise=),\n",
    "            \"name\": \"PGD\",\n",
    "            \"inner\": # Fill in\n",
    "        },\n",
    "        {# Fill in the hyperparameters\n",
    "            \"opt\": BCD(\n",
    "                X_train, y_train, w_1, w_2, w_3, learning_rate=args.lr,\n",
    "                lmbda=args.lmbda, iterations=iterations),\n",
    "            \"name\": \"BCD\",\n",
    "            \"inner\": # Fill in\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the iterates over the algorithms above\n",
    "\n",
    "for opt in optimizers:\n",
    "    #\n",
    "    # Fill in\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7UAAAH1CAYAAAApntlwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nOzdfZxcZXnw8d/FZjErbwsxvmRDJAoNoojRiC+IUrQNoC0p1QJqFZ62FBVbqk8q1KpYS6GNVbFieRCVWgRqNY1U0VjlAYqIEoyPvBlJAcluQEIwoBAkwPX8cc6S2cns7kwyOy87v+/ns5/MOeeeM9e8nM1ce9/3dUdmIkmSJElSN9qp3QFIkiRJkrS9TGolSZIkSV3LpFaSJEmS1LVMaiVJkiRJXcukVpIkSZLUtUxqJUmSJEldy6RWkiYQEW+JiG81u20rRcRhETHc7jjaKSLOiIiL2h3HjoiIEyLimjrbRkR8PiJ+ERE/aHc86j4R8auIeE6745CkepjUSmqaiHhzRKwqvwzdHRHfiIhXVbU5ISIyIv6gav8B5X1/Uf58OyIO2M44zitj+FVEPBoRWyq2v9HIuTLzi5n5281uq+mhg/9g8Crgt4C5mXnwjp4sIvYpr9sZOx6aOk1EXBkRf1y5LzN3zczbp+Cx7oyIzRHxy4jYFBHXRsTJEVHXd9JWfRb9zEvdxaRWUlNExHuATwB/BzwDmAd8Gji6qunbgfvLfyutB94I7AU8DbgMuHR7YsnMk8svZLuW8fzb6HZmHlkRs19WpoFeeB+34zk+G7gzMx9qwWNNO74GU+53MnM3is/p2cD7gM+2NyRJ3cykVtIOi4g9gL8B3pWZyzPzoczckpn/mZlLK9o9G3gNcBKwOCKeMXosMzdl5p2ZmUAAjwP7TkGsd0bE+yLix8BDETEjIk6LiP8pew5uiYjfq2g/Zohl+Zf7kyPitrJH+dyIiO1o2xcR/xgR90XEHRFxykS9AhPFWKPtQERcWD7mLcBLq47PiYivRMSG8rH/rOLYThWPtTEivhQRe5XHRnsuToqI9WVv/HsniOPC8jl/vYz7+xHx3Irj+0fEf0XE/RGxprL3vrrnaJzX9l0RcRtwW7nvnIhYFxEPRsQNEXHoeLFVxXlYRAxHxHsj4t7yeZ1YcfwpEfHRiLgrIn4exUiAgYjYBfgGMKdiJMCcshfqaeV9/zoiHouI3cvtv42IT5S394iIL5Tvw8/KtjtVPN/vRsTHI+J+4IwacS+LiGvK669y/x8BFwCvKGP6cLn/TyJibfl6XxYRcyZ6PatcXf67qTznKyru+9Hys3ZHRFT+0WiPiPhs+XqOlM+9b5z34IyI+HJE/Fv5WflhRBxUcfx55WdiU0TcHBG/W+6fX+4bfd0uiIh7K+53UUScOlk8db7efRHxV7H1OrwhIvYuj70yIq6PiAfKf19Zcb8rI+Ij5fl/GRHfqvh8zCxj3Fg+j+uj/L0Yxe+q11W9RheVt0evxRPLz/wvovhd89KI+HF5rk9V3Hf0+f1TGeNPIuK15bEzgUOBT5Xv7acqPhP7Vrx2E31WrxnvczCRzHwgMy8DjgXeHhEvKM/5+ohYHcW1vC4iKt+PbT6LEfHciLiifB3vi4gvRsRgxfN/X/me/zKK3zWjz33c33e1Hqee5ySpTTLTH3/88WeHfoAjgMeAGZO0+wDwg/L2jcB7arTZVJ7rCeCvmxDbGcBFFdt3Aj8C9gYGyn1vAuZQ/KHvWOAh4FnlsROAayrun8DXgEGK3ugNwBHb0fZk4BZgLrAn8O2yfc3XcKIYa7Q9G/hvil7vvYGbgOHy2E7ADcAHgZ2B5wC3A4vL46cC15VxPQX4P8Al5bF9yhgvAXYBDiyf0+vGieNCil75g4EZwBeBS8tjuwDrgBPLYy8G7gOeXx6/EvjjinPVem3/q3yOo+/jW4FZ5fneC9wDzKz1OaiK8zCKz9zfAP3AUcDDwJ7l8U9QjBzYC9gN+E/grIr7Dled72rg98vb3wL+Bziy4tjvlbe/AHy1POc+wE+BP6p4vo8B7y6fz8Doa1C+h58BVgJPHec5Vb9eh5ev74vL9/WfgKsnej2rzjf63s+oeowtwJ8AfcA7KEZcRHl8BcXnZxfg6cAPgD+d4DrdQjFaox/438Ad5e1+YC3wVxSf2cOBXwILyvveBbykvL2G4vP8vIpjCyeLp9brXSPGpRS/txZQ/OHtIIrP217AL4A/LO97fLk9q+Kz/D/Ab5Tv45XA2eWxP6X4PD21fA1fAuxe8bvqdVWv0UVV78d5wEzgt4FHyuf4dGAIuBd4TdXz+4vy9TwWeADYq9b1VvGZ2LfOz+q4n4Mar+OY51Wx/y7gHRXX1YEUn/UXAj8HlkzwWdyXYrj9U4DZFNfZJ8pjCyh+18ypuP9zG/h9N+H/a/74409n/LQ9AH/88af7f4C3APfU0e424NTy9unA/xun3S7AO4HXNyG2J78Iltt3Av9rkvv8CDi6vH0C2yZTr6rY/hJw2na0vYKKL/jA6xr5AlUZY41jt1Mmz+X2SWxNal8G3FXV/nTg8+XtW4HXVhx7VvmFdUbFl7z9K47/A/DZceK4ELigYvso4Cfl7WOB/65q/3+AD5W3r2TypPbwSV6jXwAH1focVLU7DNjM2C/J9wIvp0heHqL8ElweewVwR8V9q5PajwCfLF+ze4A/p/hDw8zycZ5G8eX/18ABFff7U+DKiudb/T6dAHwf+DfgK8DOEzz36tfrs8A/VGzvWr6v+9TzejJ+Uru2YvupZZtnUkxB+DUVySFFsvd/J7hOr6vY3gm4m6IH8dDyddyp4vglwBnl7X8F3lM+7hqKz+TJwHyKP5LtNFk8tV7vGjGuocY1R5HM/qBq3/eAEyo+y39dceydwDfL2/8LuBZ4YY3z3snkSe1QxfGNwLEV219h6+/bE6hKNCmS+j+sdb1VfCb2pb7Pas3PwTiv45jnVbH/OuD949znE8DHx/ss1mi/BFhd3t6X4np+HdBf1a6e33cmtf740wU/zhmR1AwbgadFxIzMfKxWg4g4hOJL5ug82YuBMyPiRZn5o8q2mflQRJwHbIiI52XmvVXnOpRi2CfAzzLz+Q3Gu67qfG+j+FK8T7lrV4rEYzz3VNx+uGzfaNs5VXGMialagzFWn/tnFbefTTFcdlPFvj6Knt3R4/8REU9UHH+cIimoFevPKHpUxjPe83828LKqOGZQJCj1qn4f3wv8McXzT2B3Jn4fK22s+uyOxjqb4kv6DVGMHIci0a05jLZ0FfAxil7RGyl6QD9LkSSvzcz7yiGmOzP2vfkZRQ9bzedX2peih/DgzHy0vqcGFK/JD0c3MvNXEbGxfLw7J3i8yTz5/mbmw+VrtCtF72U/cHfF67bTJI/x5LHMfCKKAlyjQ6TXZWblZ7LytboK+F1gmKKH7kqKRPMRij+cPBHF1IfJ4pns+e9N0eNabQ5j38fq+GD86+Bfy/NeWg6XvYgisdsySSyjfl5xe3ON7crfTSOZmVUxzmFyT2Pyz+p4n4NGDFGM7CAiXkbxh6AXlI/9FODfx7tjRDyd4g9Jh1L0Ju9E8UctMnNtOQT9DOD5EbGSYpTQeur7fSepCzinVlIzfI/iC+SSCdq8nSIZ+FFE3EPR4wTwtnHa70SRTAxVH8jM/86thZ8aTWihSHiAJ+f5fgY4hWK44CDFcN0Y577NcjfFkLdRe4/XcDtivLvqfPMqbq+j6GUcrPjZLTOPqjh+ZNXxmZk5Mk6s8yh6gBq1Driq6nF2zcx3lMcfonj/Rz2zxjkq38dDKYrN/AHFsOFBiuGVO/o+3keRHDy/Is49sihCNiaGCtdSDHn8PYrneAvF6/R6igRs9LxbKL5Uj5oHVL7Otc59K8WQ7W9ExIIGnsfoF3gAopgPPKuOx6vnWC3rKHr3nlbxuu0+yfX65OeqnK85t4x7PbB3jK2OW/laXUWRzBxW3r4GOIRi/v7o611PPJM9x3XAc2vsH/Pa1ohvXFnUHvhwZh4AvBJ4A1t/J9ZzDTRiKCoyesZeuxM993o+qzskIl5K8bt+dN78xRRD/vfOzD0ohlmPxl4r1rPK/S/MzN0ppiI8+Vwz8+LMfFX5HBL4+/LQRL/vGv3MS2ojk1pJOywzH6CYo3luRCyJiKdGRH9EHBkR/xARMymSjZOAF1X8vBt4SxTFmn4rIhZGUYxld4qerl9QfImfSrtQfHnZABBFgaAXTPFjQjEU+c8jYqjsoXnfBG0bjfFLwOkRsWdEzKV4nUf9AHiwLJwyUL7eLyi/VELx5fHMMpEmImZHRHUF6w+U7/HzKRKsf6vvKY/xNeA3IuIPy89Kf1nk5nnl8R8Bx5SPsy/wR5OcbzeKOYMbgBkR8UGKntodUvYOfgb4eNkbRPmeLS6b/ByYFRXFmjLzYYp5y+9ia1J1LcWQzavKNo9TvE9nRsRu5ev9HoqeusliuoRifum3o6Lw1iQuBk6MiBdFxFMoqoJ/PzPvrPP+Gyjmude1bmlm3k0xn/gfI2L3siDPcyPiNRPc7SURcUwUxdJOpUhCr6P4A9hDwF+Wn5PDgN+hHPWRmbdR/OHhrRTzhB+keF9+n62v9/bEU+0C4CMRsV8UXhgRs4DLKT7Lby5/lx0LHEDxGZ9QRPxmRBwYRcGqBymSx8fLwz8Cjiuf8yKK+cY74unAn5XnexPwvDJ2KF6vmu/tjnxWJ1O+F2+geC8vyswby0O7Afdn5iMRcTDw5oq71fos7gb8iqKo0xDF/OfRx1gQEYeXn/tHKD4ro6/xRL/vGvrMS2ovk1pJTZGZH6P4ovPXFF8G1lH0LK6g6MHdDHwhM+8Z/aEYktlHUWhqkGKe3AMUQ/z2pZgX+sgUx30L8I8Uvc0/pxhK+92pfMzSZyi+ZP8YWE3x5fIxtn7Z2pEYP0wxPPCO8jGeHNJbfkH9HYo/KtxB0QtzATCalJ1D0UPyrYj4JUVS8bKq819FUbjnO8BHM/NbdT7nyuf0S4riNsdR9BbdQ9F78pSyyceBR8vn+y8URaYmspJiSPpPKZ77I2zfcNpa3kfxfK+LiAcpinotKJ/HTyg+t7dHUXF2dDjnVRTDXX9Qsb0bWyuqQvHHhoco5kBfQ5F4fq6egDLzXygKW10REfvU0f47FIXavkLRk/9cite+LmWifibw3fJ5vryOu72NYujoLRR/oPoyxZzF8XyVYq71aNGlY8qezEcphhcfSfF5/TTwtvK1H3UVxRDyuyq2g+La2t54qn2MIrn7FkUC+lmKObobKXpY30sxFeMvgTdk5n11nPOZZRwPUvwB7yq2JosfoHiffkFxTV/cQKy1fB/Yj+I1PBN4Yxk7FNf9G6OoXvzJGvfd7s/qOP6z/P2yDng/xWt7YsXxdwJ/U7b5IMXrDoz7WfwwxXD/B4CvA8srzvUUiqHM91H8nnk6xR+FRp93zd932/mZl9QmoxUKJUltFMUSGOdlZvUwxo5RJk93UBRbqTl3WtoeUSzZsm9mvrXdsUxHEXECRSGoV7U7FkmaCvbUSlIblEN/jyqHKw4BHwL+o91xSZIkdZuWJrUR8bkoFra/aZzjERGfjGJx+B9HxItbGZ8ktVBQDJn7BcUQyVsphtlJkiSpAS0dfhwRr6aYyP+FzNymyElEHEUxb+MoijkN52Rm9VwuSZIkSZKAFvfUZubVlGuQjeNoioQ3M/M6YDAiGiniIEmSJEnqIZ02p3aIsdUqh6mxRqUkSZIkSQAz2h1Alaixr+b46Ig4iWLNS3bZZZeX7L///lMZl9R2P7nnl2x5/Ilt9vf37cT+z9ytDRFJkiRJrXHDDTfcl5mzax3rtKR2GNi7YnsuxfqF28jM84HzARYtWpSrVq2a+uikNpp/2tdr/oUngFVnv77V4UiSJEktExE/G+9Ypw0/vgx4W1kF+eXAA5l5d7uDkjrBnMGBhvZLkiRJvaDVS/pcAnwPWBARwxHxRxFxckScXDa5HLgdWAt8BnhnK+OTOtnSxQsY6O8bs2+gv4+lixe0KSJJkiSp/Vo6/Dgzj5/keALvalE4UldZsrCombZs5RrWb9rMnMEBli5e8OR+SZIkqRd12pxaSRNYsnDIJFaSJEldbcuWLQwPD/PII49sc2zmzJnMnTuX/v7+us9nUitJkiRJapnh4WF222039tlnHyK2LoCTmWzcuJHh4WHmz59f9/k6rVCUJEmSJGkae+SRR5g1a9aYhBYgIpg1a1bNHtyJmNRKkiRJklqqOqGdbP9EHH4sTUMrVo9YUEqSJEk9waRWmmZWrB7h9OU3snnL4wCMbNrM6ctvBDCxlSRJ0rTj8GNpmlm2cs2TCe2ozVseZ9nKNW2KSJIkSRqrWM21/v0TMamVppn1mzY3tF+SJElqpZkzZ7Jx48ZtEtjR6sczZ85s6HwOP5ammTmDA4zUSGDnDA60IRpJkiRprLlz5zI8PMyGDRu2OTa6Tm0jTGqlaWbp4gVj5tQCDPT3sXTxgjZGJUmSJBX6+/sbWod2Mia10jQzWgzK6seSJEnqBSa10jS0ZOGQSawkSZJ6goWiJEmSJEldy6RWkiRJktS1HH4s9bAVq0eceytJkqSuZlIr9agVq0fGVEke2bSZ05ffCGBiK0mSpK7h8GOpRy1buWbMsj8Am7c8zrKVa9oUkSRJktQ4k1qpR63ftLmh/ZIkSVInMqmVetScwYGG9kuSJEmdyKRW6lFLFy9goL9vzL6B/j6WLl7QpogkSZKkxlkoSupRo8WgrH4sSZKkbmZSK/WwJQuHTGIlSZLU1Rx+LEmSJEnqWia1kiRJkqSu5fBjSZNasXrEubeSJEnqSCa1kia0YvUIpy+/kc1bHgdgZNNmTl9+I4CJrSRJktrO4ceSJrRs5ZonE9pRm7c8zrKVa9oUkSRJkrSVSa2kCa3ftLmh/ZIkSVIrmdRKmtCcwYGG9kuSJEmtZFIraUJLFy9goL9vzL6B/j6WLl7QpogkSZKkrSwUJWlCo8WgrH4sSZKkTtTypDYijgDOAfqACzLz7KrjewAXAfPK+D6amZ9vdZyStlqycMgkVpIkSR2ppUltRPQB5wK/BQwD10fEZZl5S0WzdwG3ZObvRMRsYE1EfDEzH21lrJIa53q2kiRJarVWz6k9GFibmbeXSeqlwNFVbRLYLSIC2BW4H3istWFKatToerYjmzaTbF3PdsXqkXaHJkmSpGms1UntELCuYnu43FfpU8DzgPXAjcCfZ+YT1SeKiJMiYlVErNqwYcNUxSupTq5nK0mSpHZodVIbNfZl1fZi4EfAHOBFwKciYvdt7pR5fmYuysxFs2fPbn6kkhrieraSJElqh1YntcPA3hXbcyl6ZCudCCzPwlrgDmD/FsUnaTu5nq0kSZLaodVJ7fXAfhExPyJ2Bo4DLqtqcxfwWoCIeAawALi9pVFKapjr2UqSJKkdWlr9ODMfi4hTgJUUS/p8LjNvjoiTy+PnAR8BLoyIGymGK78vM+9rZZySGud6tpIkSWqHyKye0tp9Fi1alKtWrWp3GJIkSZKkKRARN2TmolrHWtpTK0ngeraSJElqHpNaSS01up7t6PI/o+vZAia2kiRJalirC0VJ6nGuZytJkqRmMqmV1FKuZytJkqRmMqmV1FKuZytJkqRmMqmV1FKuZytJkqRmslCUpJZyPVtJkiQ1k0mtpJZbsnDIJFaSJElNYVIrqWO5nq0kSZImY1IrqSO5nq0kSZLqYaEoSR3J9WwlSZJUD5NaSR3J9WwlSZJUD5NaSR3J9WwlSZJUD5NaSR3J9WwlSZJUDwtFSepIrmcrSZKkepjUSupY9a5n69I/kiRJvcukVlJXc+kfSZKk3uacWkldzaV/JEmSeptJraSu5tI/kiRJvc2kVlJXc+kfSZKk3mZSK6mrufSPJElSb7NQlKSuVu/SP1ZIliRJmp5MaiV1vcmW/rFCsiRJ0vTl8GNJ054VkiVJkqYvk1pJ054VkiVJkqYvk1pJ054VkiVJkqYvk1pJ054VkiVJkqYvC0VJmvbqrZAMVkmWJEnqNia1knrCZBWSwSrJkiRJ3cjhx5JUskqyJElS92l5UhsRR0TEmohYGxGnjdPmsIj4UUTcHBFXtTpGSb3JKsmSJEndp6VJbUT0AecCRwIHAMdHxAFVbQaBTwO/m5nPB97Uyhgl9S6rJEuSJHWfVvfUHgyszczbM/NR4FLg6Ko2bwaWZ+ZdAJl5b4tjlNSjGqmSvGL1CIecfQXzT/s6h5x9BStWj7QqTEmSJFVodVI7BKyr2B4u91X6DWDPiLgyIm6IiLe1LDpJPW3JwiHOOuZAhgYHCGBocICzjjlwmyJRowWlRjZtJtlaUMrEVpIkqfVaXf04auzLqu0ZwEuA1wIDwPci4rrM/OmYE0WcBJwEMG/evCkIVVIvqqdK8kQFpaySLEmS1Fqt7qkdBvau2J4LrK/R5puZ+VBm3gdcDRxUfaLMPD8zF2XmotmzZ09ZwJJUzYJSkiRJnaPVSe31wH4RMT8idgaOAy6ravNV4NCImBERTwVeBtza4jglaVwWlJIkSeocLU1qM/Mx4BRgJUWi+qXMvDkiTo6Ik8s2twLfBH4M/AC4IDNvamWckjSRRgpKSZIkaWpFZvWU1u6zaNGiXLVqVbvDkNRDVqweYdnKNazftJk5gwMsXbyg5nzaettJkiRpfBFxQ2YuqnWsKYWiImJWZm5sxrkkqRvUU1BqtEryaFGp0SrJo/eXJEnSjmto+HFE/ElELK3YPjAihoF7I2JVRDyz6RFKUpeaqEqyJEmSmqPRObXvBirLe34M2AScCuwB/E2T4pKkrmeVZEmSpKnX6PDjecBPACJiD+A1wJLMvDwiNgJnNTk+SepacwYHGKmRwNaqkuzcW0mSpO3TaE9tH/BEeftVQAJXltvrgKc3JyxJ6n71VkkenXs7smkzyda5tytWj7QwWkmSpO7UaFJ7G/D68vZxwLWZ+XC5PQe4v1mBSVK3W7JwiLOOOZChwQECGBoc4KxjDtymB9a5t5IkSduv0eHHHwX+NSLeDuwJvKni2G9SrC0rSSrVUyXZubeSJEnbr6GkNjMvjoi7gJcB12fm1RWHfw5c1szgJKkXNDL3VpIkSWM1vE5tZl4DXFNj/4eaEpEk9ZilixeMWc8Was+9BQtKSZIkVWt0ndpXRsQbKrZnRcQlEXFjRHw0Ivomur8kaVv1zr21oJQkSdK2Gu2pPRv4DvC1cnsZcBTwbeAdwAPAR5oWnST1iHrm3k5UUMreWkmS1KsarX78PGAVQET0A28E/iIzfx94P/Dm5oYnSRplQSlJkqRtNdpTuyvwYHn7YGAXtvba/hCY16S4JElVGiko5dxbSZLUKxrtqR0BDipvHwnclJn3ltt7Ag/XvJckaYctXbyAgf6xpQtqFZRy7q0kSeoljSa1lwB/FxFfBt4DXFRx7MXAbc0KTJI0Vr0FpSaaeytJkjTdNDr8+AzgEeDlFEWjPl5x7CDg35sTliSplnoKSjUy99ZhypIkqds1lNRm5uPAmeMcW9KUiCRJO6Teubejw5RHe3VHhykDJraSJKlrNDr8GICIeEFEvCsiPhAR74yIFzQ7MEnS9ql37q3DlCVJ0nTQUE9tRMwALgSOB6LiUEbExcAJZW+uJKlNRntZJxtW7BJBkiRpOmh0Tu2HgD8APkhRJOoe4JnAW8tjt5f/SpLaqJ65ty4RJEmSpoNGhx+/FfhIZp6ZmT/LzF+X/54J/C3wtuaHKEmaCi4RJEmSpoNGk9o5wPfGOXZteVyS1AVcIkiSJE0HjQ4/Xg8cAny7xrFXlsclSV3CJYIkSVK3azSp/SLw/oh4orx9N8Wc2uOA9wN/39zwJEnt5hJBkiSpkzU6/PgM4MvAh4HbgF8BaynWrv33cr8kaRpxiSBJktTJGuqpzczHgDdHxJnAq4G9gPuBqyjm064GXtjsICVJ7dPsJYIcoixJkpqp0eHHAGTmzcDNlfsi4nnA85sRlCSpszRriSCHKEuSpGZrdPixJEk11TNM2SHKkiSp2barp1aSpGr1DFO2krIkSWo2k1pJUtNMNkzZSsqSJKnZJh1+HBHPqeeHYmkfSZLGZSVlSZLUbPX01K4Fso52UU+7iDgCOAfoAy7IzLPHafdS4Drg2Mz8ch2PL0nqcM2upAwOU5YkqdfVk9Se2KwHi4g+4Fzgt4Bh4PqIuCwzb6nR7u+Blc16bElSZ2hWJWVwmLIkSaojqc3Mf2ni4x0MrM3M2wEi4lLgaOCWqnbvBr4CvLSJjy1J6hJLFy8Yk6xC48OUq5Nae3QlSZqeWl0oaghYV7E9DLysskFEDAG/BxyOSa0k9aRmD1O2R1eSpOmr1Ult1NhXPQ/3E8D7MvPxiFrNyxNFnAScBDBv3rymBShJ6gzNHKZsj64kSdPXpNWPm2wY2Ltiey6wvqrNIuDSiLgTeCPw6YhYUn2izDw/Mxdl5qLZs2dPVbySpA5WbzXlRnt0RzZtJtnao7ti9UhT45YkSc3T6p7a64H9ImI+MAIcB7y5skFmzh+9HREXAl/LzBWtDFKS1B3qHaZsj64kSdNXS5PazHwsIk6hqGrcB3wuM2+OiJPL4+e1Mh5JUverZ5hyvYWnnKMrSVL3aXVPLZl5OXB51b6ayWxmntCKmCRJ05s9upIkTV8tT2olSWoHe3QlSZqeWl0oSpKkjrVk4RBnHXMgQ4MDBDA0OMBZxxxYs0e3lkZ6dKutWD3CIWdfwfzTvs4hZ19hcSpJkupkT60kSRU6vUfX4cySJI1lT60kSQ1qV4+uSw5JkrQte2olSdoO7ejRrbdAlb25kqReYlIrSdIUaXbV5XqSX4cyS5J6jUmtJElTqJk9uvUkv4305lqZWZI0HTinVpKkNqt3ju7SxQsY6O8bs686+W3GUOZqVmaWJHUye2olSeoA9fTo1jOcuZlDmcHhzJKkzmdSK0lSF5ks+W3mUGaYmuHMJr+SpGZy+LEkSdNIM4cyQ/OHMzeyLJHDniVJ9bCnVpKkaaZZQ5mh+cOZ7fmVJDWbSa0kST2q1ZWZweRXktR8Dj+WJEnjavZw5uokd7z9DnuWJNXLnlpJkjShZg5n7rWeX3uHJWnqmdRKkqSmMPkdm6hxym0AACAASURBVPw6NFqSWsOkVpIktVSvJL/tnBdskiypl5jUSpKkjtTtyW+7hkabJEvqNSa1kiSpq3Vq8tuuodHTKUk2mZZUD5NaSZLUE1qd/LZraPR0SZIt2iWpXia1kiRJFZqV/LZraPR0SZI7vWiXvdJS5zCplSRJ2g71Jr+tHho9XZLkTi7a1em90u1sJ7WDSa0kSVKbNTP5nS5JcicX7erkXul2thtt26yh4CbmqpdJrSRJUpeoJ/mtt12nJ8mdXLSrk3ul29mumUPBOz0xny7tpkuib1IrSZLUozo5Se7kol2d3CvdznbNHAreyYn5dGnXSKLf6XZqdwCSJEmaHpYsHOK7px3OHWe/nu+edvi4X4yb2W7JwiHOOuZAhgYHCGBocICzjjlwm6Jdk7WBIvkd6O8bs2+8XuR2tKtOcsfb3652zRwK3omJ+XRrV++5uoE9tZIkSepqrS7a1cm90u1s18yh4PaGT327es/VDUxqJUmSpFIzh2Q3u910SLqnQ2I+XdrVe65uEJnZ7hh22KJFi3LVqlXtDkOSJEnqad1e/bh6nikUyW/1cPXp0K7ec3WKiLghMxfVPGZSK0mSJEmFTk66m92um6ofd1RSGxFHAOcAfcAFmXl21fG3AO8rN38FvCMz/99E5zSplSRJkqTpa6KktqXVjyOiDzgXOBI4ADg+Ig6oanYH8JrMfCHwEeD8VsYoSZIkSeoerV7S52BgbWbenpmPApcCR1c2yMxrM/MX5eZ1wNwWxyhJkiRJ6hKtTmqHgHUV28PlvvH8EfCNKY1IkiRJktS1Wr2kT9TYV3NSb0T8JkVS+6pxjp8EnAQwb968ZsUnSZIkSeoire6pHQb2rtieC6yvbhQRLwQuAI7OzI21TpSZ52fmosxcNHv27CkJVpIkSZLU2Vqd1F4P7BcR8yNiZ+A44LLKBhExD1gO/GFm/rTF8UmSJEmSukhLhx9n5mMRcQqwkmJJn89l5s0RcXJ5/Dzgg8As4NMRAfDYeKWbJUmSJEm9reXr1E4F16mVJEmSpOmrY9aplSRJkiSpmUxqJUmSJEldy6RWkiRJktS1TGolSZIkSV3LpFaSJEmS1LVMaiVJkiRJXcukVpIkSZLUtUxqJUmSJEldy6RWkiRJktS1TGolSZIkSV3LpFaSJEmS1LVMaiVJkiRJXcukVpIkSZLUtUxqJUmSJEldy6RWkiRJktS1TGolSZIkSV3LpFaSJEmS1LVMaiVJkiRJXcukVpIkSZLUtUxqJUmSJEldy6RWkiRJktS1TGolSZIkSV3LpFaSJEmS1LVMaiVJkiRJXcukVpIkSZLUtUxqJUmSJEldy6RWkiRJktS1TGolSZIkSV3LpFaSJEmS1LVMaiVJkiRJXcukVpIkSZLUtVqe1EbEERGxJiLWRsRpNY5HRHyyPP7jiHhxq2OUJEmSJHWHlia1EdEHnAscCRwAHB8RB1Q1OxLYr/w5CfjnVsYoSZIkSeoere6pPRhYm5m3Z+ajwKXA0VVtjga+kIXrgMGIeFaL45QkSZIkdYFWJ7VDwLqK7eFyX6NtJEmSJEliRosfL2rsy+1oQ0ScRDE8GeBXEbFmB2Obak8D7mt3EFKH8bqQtuV1IW3L60LaVq9dF88e70Crk9phYO+K7bnA+u1oQ2aeD5zf7ACnSkSsysxF7Y5D6iReF9K2vC6kbXldSNvyutiq1cOPrwf2i4j5EbEzcBxwWVWby4C3lVWQXw48kJl3tzhOSZIkSVIXaGlPbWY+FhGnACuBPuBzmXlzRJxcHj8PuBw4ClgLPAyc2MoYJUmSJEndo9XDj8nMyykS18p951XcTuBdrY6rBbpmqLTUQl4X0ra8LqRteV1I2/K6KEWRQ0qSJEmS1H1aPadWkiRJkqSmMamdYhFxRESsiYi1EXFau+OR2iEi9o6I/xsRt0bEzRHx5+X+vSLivyLitvLfPdsdq9RqEdEXEasj4mvltteFelpEDEbElyPiJ+X/G6/wulCvi4i/KL9D3RQRl0TETK+LrUxqp1BE9AHnAkcCBwDHR8QB7Y1KaovHgPdm5vOAlwPvKq+F04DvZOZ+wHfKbanX/Dlwa8W214V63TnANzNzf+AgiuvD60I9KyKGgD8DFmXmCygK7h6H18WTTGqn1sHA2sy8PTMfBS4Fjm5zTFLLZebdmfnD8vYvKb6gDFFcD/9SNvsXYEl7IpTaIyLmAq8HLqjY7XWhnhURuwOvBj4LkJmPZuYmvC6kGcBARMwAngqsx+viSSa1U2sIWFexPVzuk3pWROwDLAS+DzxjdB3q8t+nty8yqS0+Afwl8ETFPq8L9bLnABuAz5fD8i+IiF3wulAPy8wR4KPAXcDdwAOZ+S28Lp5kUju1osY+y02rZ0XErsBXgFMz88F2xyO1U0S8Abg3M29odyxSB5kBvBj458xcCDxEDw+plADKubJHA/OBOcAuEfHW9kbVWUxqp9YwsHfF9lyKoQJSz4mIfoqE9ouZubzc/fOIeFZ5/FnAve2KT2qDQ4DfjYg7KaanHB4RF+F1od42DAxn5vfL7S9TJLleF+plrwPuyMwNmbkFWA68Eq+LJ5nUTq3rgf0iYn5E7EwxofuyNscktVxEBMX8qFsz82MVhy4D3l7efjvw1VbHJrVLZp6emXMzcx+K/x+uyMy34nWhHpaZ9wDrImJBueu1wC14Xai33QW8PCKeWn6nei1FfRKvi1JkOhp2KkXEURRzpvqAz2XmmW0OSWq5iHgV8N/AjWydO/hXFPNqvwTMo/iF/abMvL8tQUptFBGHAf87M98QEbPwulAPi4gXURRP2xm4HTiRoiPG60I9KyI+DBxLsaLEauCPgV3xugBMaiVJkiRJXczhx5IkSZKkrmVSK0mSJEnqWia1kiRJkqSuZVIrSZIkSepaJrWSJEmSpK5lUitJUoMi4oSIyIjYt9w+NSKOaWM8gxFxRkS8uMaxKyPiyjaEJUlSS8xodwCSJE0DpwLXAMvb9PiDwIeAYeCHVcfe2fpwJElqHZNaSZI6UEQ8JTN/vaPnycxbmhGPJEmdyuHHkiTtgIi4E3g28JZySHJGxIUVxw+KiMsi4hcRsTkivhsRh1ad48KIGI6IV0TEtRGxGfiH8thxEXFFRGyIiF9FxOqIeHvFffcB7ig3P1MRwwnl8W2GH0fEgoj4j4jYVMZ0XUQcUdXmjPI8+0XE18vH/llEfDAi/P4gSeoY/qckSdKO+T3gHmAl8Iry5yMA5RzXa4G9gD8Bfh/YCHw7Il5SdZ49gEuBS4AjgYvL/c8Bvgy8BVgC/CdwQUScXB6/Gxidz3tWRQxfrxVsRMyhGCp9EHAK8AfAJuDrEXFkjbv8B3BF+dgrgA8Db6/RTpKktnD4sSRJOyAzV0fEr4H7MvO6qsPLgLuAwzPzUYCIWAncBHyAIlEctSvw1sz8atX5/270dtlDeiXwLOAdwHmZ+euIWF02ub1GDNXeA+wJvCIz15bnvRy4BTgT+EZV+3/MzM+Xt78dEYcDxwOfR5KkDmBPrSRJUyAiBoDXAP8OPBERMyJiBhDAt4FXV93lMeBrNc6zX0RcEhEjwJby54+BBdsZ2quB60YTWoDMfJyih/hFEbF7VfvqHt+bgHnb+diSJDWdSa0kSVNjL6CPokd2S9XPKcCeVXNT7y2TyydFxK7Af1EMFT4NOBR4KfA54Ck7ENfdNfbfQ5Fw71m1//6q7V8DM7fzsSVJajqHH0uSNDU2AU8A5wJfqNUgM5+o3KzR5BUURagOzcxrRneWPb7b637gmTX2P7OMoTqJlSSpo5nUSpK0434NDFTuyMyHIuK/KXpZf1iVwNbrqeW/W0Z3RMSewNE1Hp/qGMZxFXBqROyTmXeW5+wDjgVWZ+YvtyNOSZLaxqRWkqQddwtwaES8gWIY731lwvge4GpgZUR8lmLY79OAFwN9mXnaJOe9FngQODciPgTsAvw1cB9FteRRP6eoqnxcRPwYeAi4IzM31jjnx4ETgP8qz/kg8E7gN4DXN/i8JUlqO+fUSpK0404H1gBfAq4HzgDIzB9SzIHdCHwS+BZwDnAgRbI7oczcQLFkUB/Fsj5nARcAF1W1e4KieNSeFEWorgd+Z5xzrgdeBdwM/HN53r2A12fmN+t+xpIkdYjIrDWFR5IkSZKkzmdPrSRJkiSpa7U0qY2Iz0XEvRFx0zjHIyI+GRFrI+LHEfHiVsYnSZIkSeoure6pvRA4YoLjRwL7lT8nUcz1kSRJkiSpppYmtZl5NROvf3c08IUsXAcMRsSzWhOdJEmSJKnbdNqc2iFgXcX2cLlPkiRJkqRtdNo6tVFjX83yzBFxEsUQZXbZZZeX7L///lMZl9R2mx7ewsimzTxRUbF8pwiGBgcYfGp/GyOTJEmSptYNN9xwX2bOrnWs05LaYWDviu25wPpaDTPzfOB8gEWLFuWqVaumPjqpjQ45+woe27R5m/3PGBzgu6cd3oaIJEmSpNaIiJ+Nd6zThh9fBrytrIL8cuCBzLy73UFJnWB9jYR2ov2SJElSL2hpT21EXAIcBjwtIoaBDwH9AJl5HnA5cBSwFngYOLGV8UmdbM7gACM1Etg5gwNtiEaSJEnqDC1NajPz+EmOJ/CuFoUjdZWlixdw+vIb2bzl8Sf3DfT3sXTxgjZGJUmSJLVXp82plTSOJQuLQuDLVq5h/abNzBkcYOniBU/ulyRJkrrBli1bGB4e5pFHHtnm2MyZM5k7dy79/fUXQjWplbrIkoVDJrGSJEnqasPDw+y2227ss88+RGxdACcz2bhxI8PDw8yfP7/u83VaoShJkiRJ0jT2yCOPMGvWrDEJLUBEMGvWrJo9uBMxqZUkSZIktVR1QjvZ/ok4/FiahlasHnHurSRJknqCSa00zaxYPTKmSvLIps2cvvxGABNbSZIkTTsOP5ammWUr14xZ9gdg85bHWbZyTZsikiRJksYqVnOtf/9ETGqlaWb9ps0N7ZckSZJaaebMmWzcuHGbBHa0+vHMmTMbOp/Dj6VpZs7gACM1Etg5gwNtiEaSJEkaa+7cuQwPD7Nhw4Ztjo2uU9sIk1ppmlm6eMGYObUAA/19LF28oI1RSZIkSYX+/v6G1qGdjEmtNM2MFoOy+rEkSZJ6gUmtNA0tWThkEitJkqSeYKEoSZIkSVLXMqmVJEmSJHUtk1pJkiRJUtdyTq3Uw1asHrGglCRJkrqaSa3Uo1asHhmz9M/Ips2cvvxGABNbSZIkdQ2HH0s9atnKNWPWsgXYvOVxlq1c06aIJEmSpMaZ1Eo9av2mzQ3tlyRJkjqRSa3Uo+YMDjS0X5IkSepEJrVSj1q6eAED/X1j9g3097F08YI2RSRJkiQ1zkJRUo8aLQZl9WNJkiR1M5NaqYctWThkEitJkqSu5vBjSZIkSVLXsqdW0qRWrB5xmLIkSZI6kkmtpAmtWD3C6ctvfHJN25FNmzl9+Y0AJraSJElqO4cfS5rQspVrnkxoR23e8jjLVq5pU0SSJEnSVia1kia0ftPmhvZLkiRJrWRSK2lCcwYHGtovSZIktZJJraQJLV28gIH+vjH7Bvr7WLp4QZsikiRJkrayUJSkCY0Wg7L6sSRJkjpRy5PaiDgCOAfoAy7IzLOrju8BXATMK+P7aGZ+vtVxStpqycIhk1hJkiR1pJYOP46IPuBc4EjgAOD4iDigqtm7gFsy8yDgMOAfI2LnVsYpSZIkSeoOre6pPRhYm5m3A0TEpcDRwC0VbRLYLSIC2BW4H3isxXFK2g4rVo84TFmSJEkt1eqkdghYV7E9DLysqs2ngMuA9cBuwLGZ+URrwpO0vVasHuH05Tc+uabtyKbNnL78RgATW0mSJE2ZVlc/jhr7smp7MfAjYA7wIuBTEbH7NieKOCkiVkXEqg0bNjQ/UkkNWbZyzZMJ7ajNWx5n2co1bYpIkiRJvaDVSe0wsHfF9lyKHtlKJwLLs7AWuAPYv/pEmXl+Zi7KzEWzZ8+esoAl1Wf9ps0N7ZckSZKaodVJ7fXAfhExvyz+dBzFUONKdwGvBYiIZwALgNtbGqWkhs0ZHGhovyRJktQMLU1qM/Mx4BRgJXAr8KXMvDkiTo6Ik8tmHwFeGRE3At8B3peZ97UyTkmNW7p4AQP9fWP2DfT3sXTxgjZFJEmSpF7Q8nVqM/Ny4PKqfedV3F4P/Har45K0Y0aLQVn9WJIkSa3U8qRW0vS1ZOGQSawkSZJayqRWUsu5nq0kSZKaxaRWUku5nq0kSZKaqdXVjyX1ONezlSRJUjOZ1EpqKdezlSRJUjOZ1EpqKdezlSRJUjOZ1EpqKdezlSRJUjNZKEpSS7merSRJkprJpFZSy9W7nq1L/0iSJGkyJrWSOpJL/0iSJKkezqmV1JFc+keSJEn1MKmV1JFc+keSJEn1MKmV1JFc+keSJEn1MKmV1JFc+keSJEn1sFCUpI7k0j+SJEmqh0mtpI7l0j+SJEmajEmtpK7m0j+SJEm9zTm1krqaS/9IkiT1NpNaSV3NpX8kSZJ6m0mtpK7m0j+SJEm9zaRWUldz6R9JkqTeZqEoSV2t3qV/rJAsSZI0PZnUSup6ky39Y4VkSZKk6cvhx5KmPSskS5IkTV8mtZKmPSskS5IkTV8mtZKmPSskS5IkTV8mtZKmPSskS5IkTV8WipI07dVbIRmskixJktRtTGol9YTJKiSDVZIlSZK6kcOPJalklWRJkqTu0/KkNiKOiIg1EbE2Ik4bp81hEfGjiLg5Iq5qdYySepNVkiVJkrpPS5PaiOgDzgWOBA4Ajo+IA6raDAKfBn43M58PvKmVMUrqXVZJliRJ6j6t7qk9GFibmbdn5qPApcDRVW3eDCzPzLsAMvPeFscoqUdZJVmSJKn7tLpQ1BCwrmJ7GHhZVZvfAPoj4kpgN+CczPxCa8KT1MuskixJktR9Wp3URo19WbU9A3gJ8FpgAPheRFyXmT8dc6KIk4CTAObNmzcFoUrqRVZJliRJ6i6tHn48DOxdsT0XWF+jzTcz86HMvA+4Gjio+kSZeX5mLsrMRbNnz56ygCWpmlWSJUmSOkerk9rrgf0iYn5E7AwcB1xW1earwKERMSMinkoxPPnWFscpSeOySrIkSVLnaOnw48x8LCJOAVYCfcDnMvPmiDi5PH5eZt4aEd8Efgw8AVyQmTe1Mk5JmsicwQFGaiSwVkmWJElqvVbPqSUzLwcur9p3XtX2MmBZK+OSpHotXbxgzJxaGL9KsgWlJEmSplbLk1pJ6nb1Vkm2oJQkSdLUa0pSGxGzMnNjM84lSd2gnirJExWUMqmVJElqjoYKRUXEn0TE0ortAyNiGLg3IlZFxDObHqEkdSkLSkmSJE29Rqsfvxuo/Db2MWATcCqwB/A3TYpLkrreeIWjau1fsXqEQ86+gvmnfZ1Dzr6CFatHpjo8SZKkaaHRpHYe8BOAiNgDeA3wl5n5T8CHgMXNDU+SutfSxQsY6O8bs69WQanRubcjmzaTbJ17a2IrSZI0uUaT2j6KZXYAXgUkcGW5vQ54enPCkqTut2ThEGcdcyBDgwMEMDQ4wFnHHLjNfNqJ5t5KkiRpYo0WiroNeD1wBXAccG1mPlwemwPc38TYJKnr1VNQyrm3kiRJ26/RntqPAqdGxH3Am4F/qjj2m8CPmxWYJPWKRubeSpIkaayGemoz8+KIuAt4GXB9Zl5dcfjnwGXNDE6SesHSxQvGrGcLtefeQjH/drL1cSVJknpJw+vUZuY1wDU19n+oKRFJUo8ZTUonS1ZHC0qNJr+jBaUqzyFJktRrGkpqI+KVwF6Z+bVyexbwKeAFwErgfZn5+ASnkCTVUM/c24kKSpnUSpKkXtXonNqzgZdUbC8DjgJ+CrwD+KsmxSVJqmJBKUmSpG01mtQ+D1gFEBH9wBuBv8jM3wfeT1E8SpI0BRopKLVi9QiHnH0F80/7OoecfYVr3kqSpGmr0aR2V+DB8vbBwC7A18rtHwLzmhSXJKnK0sULGOjvG7OvVkGp0bm3I5s2k2yde2tiK0mSpqNGk9oR4KDy9pHATZl5b7m9J/BwzXtJknbYkoVDnHXMgQwNDhDA0OAAZx1z4DbzaSeaeytJkjTdNFr9+BLg7yLiMIq5tJUVj18M3NakuCRJNdRTUMq5t5IkqZc0mtSeATwCvJyiaNTHK44dBPx7c8KSJG2vOYMDjNRIYMebe+u6t5IkqZs1lNSWy/WcOc6xJU2JSJK0Q5YuXjBmPVuYeO6t695KkqRu1mhPLQAR8QLgNcBewEbg6sy8qZmBSZK2z2hCOlkPrOveSpKk6aChpDYiZgAXAscDUXEoI+Ji4ISyN1eS1EbNnnvrMGVJktSpGq1+/CHgD4APAvOBgfLfDwLHlv9KkrpAveveukSQJEnqZI0mtW8FPpKZZ2bmzzLz1+W/ZwJ/C7yt+SFKkqZCveveukSQJEnqZI3OqZ0DfG+cY9cC79+xcCRJrVLv3FuHKUuSpE7WaFK7HjgE+HaNY68sj0uSukQ9c2/rXSLIasqSJKkdGh1+/EXg/RHxgYh4TkQMRMT8iDidopf2X5sfoiSpnRymLEmSOlmjPbVnAM8BPlzeHhXAxeV+SdI00uxhyg5RliRJzdRQUpuZjwFvjogzgVdTrFN7P3AVxXzb1cALmx2kJKm9mjVM2SHKkiSp2RodfgxAZt6cmf9cVkH+58y8BdgDeH5zw5MkdYt6hik7RFmSJDVbo8OPJUmqqZ5hylZSliRJzWZSK0lqmsmGKVtJWZIkNdt2DT+WJGl7WElZkiQ126Q9tRHxnDrP9cx6GkXEEcA5QB9wQWaePU67lwLXAcdm5pfrjEGS1MGaXUkZHKYsSVKvq2f48Vog62gXk7WLiD7gXOC3gGHg+oi4rCw0Vd3u74GVdTyuJKmLNKuSMjhMWZIk1ZfUntjExzsYWJuZtwNExKXA0cAtVe3eDXwFeGkTH1uS/n979x9rd13fcfz5sgXt/LFuWg1pYdSkY3aiwBqmIf7CbVB1wpjbyvw9E4YBo9nigLm5Hy6BJZvRJSAhiOJ0EoeKRJn4A42ZTqUIqEzqKrDRVgV/IOJYu+J7f5xvw+Hulp57e+73nM+5z0dyc8/38/2cc9/NO/fHq5/P93vUiDeedNRDwiosfJvy3FDriq4kSbPpgKG2qi4f49dbC9w5dLwD+NXhCUnWAr8FnIihVpKWpXFvU3ZFV5Kk2dX33Y8zz9jcLctvA86pqgeS+aZ3L5ScAZwBcMQRR4ytQEnSdBjnNmVXdCVJml193/14B3D40PE6YNecOZuAK5LcAbwEuCjJqXNfqKouqapNVbVpzZo1S1WvJGmKjXo35YWu6O68536KB1d0r7px51jrliRJ49P3Su31wIYk64GdwBbg94cnVNX6fY+TvBv4aFVd1WeRkqQ2jLpN2RVdSZJmV6+htqr2JjmbwV2NVwCXVdUtSc7szl/cZz2SpPaNsk151BtPeY2uJEnt6Xullqq6Brhmzti8YbaqXtVHTZKk2eaKriRJs6v3UCtJ0iS4oitJ0mzq+0ZRkiRNrVOPXcv5px3N2tWrCLB29SrOP+3oeVd057OQFd25rrpxJydccB3rz/0YJ1xwnTenkiRpRK7USpI0xBVdSZLa4kqtJEkL5IquJEnTw5VaSZIWYdpXdL1BlSRpuXClVpKkJTKpFd194XfnPfdTPBh+XdWVJM0iV2olSVpCk1jRHfUth1zNlSTNAkOtJEkTNu730R0l/LqVWZI0Kwy1kiRNgXGu6I4Sfheymmv4lSRNM6+plSSpEaNeo/vGk45i1SErHjI2N/yOYyvzsIVcx+sdnCVJ4+RKrSRJDRllRXeU7czj3Mq872u58itJmgRDrSRJM+hA4XecW5nB8CtJmhy3H0uStAyNcyszjP62RG57liSNmyu1kiQtU+Paygyu/EqSJsdQK0mSHpbhd/Hh14AsSUvPUCtJksbC8PvQ8OvqsCT1w1ArSZJ6tVzC7yS3RhuSJS0nhlpJkjSVWg+/k9oabUiWtNwYaiVJUtOmNfxOamu0IVnScmOolSRJy0Lf4XdSW6MNyd60S1puDLWSJElDxhV+J7U12pDcz027XJWWpoehVpIkaRFGDb99b402JC/9TbumfVV6KeZJ08xQK0mSNGHjDL+G5P8/PqnV5mkP09O+FdxgrlEZaiVJkhoxSvgddd5yCsmTWm2e9jA9zVvBpz2Yz9K8WWColSRJWqaWS0ie1GrztIfpad4KPs3BfNbmzULwfcSkC5AkSdJsOPXYtXz+3BO5/YIX8vlzT9zvH8ejzDv12LWcf9rRrF29igBrV6/i/NOOnjckH2jeqK/1xpOOYtUhKx4ytr8gPc55c0Pu/sYnNW8SW8H7DObLdd6+4LvznvspHgy+V924k9a4UitJkqSpNO6V5L5Xmye1dXsWtoIvt9XwScwbdZW7BYZaSZIkqTPOID3qvEmF6WneCj7NwXxW5o0akFtgqJUkSZImbBJhetR54wzJsxDMZ2XeqAG5BamqSddw0DZt2lRbt26ddBmSJEmSptS034W473lzbyYFg+A73/Xm0yDJDVW1ad5zhlpJkiRJWn5auvvxw4Xa3rcfJzkZeDuwAri0qi6Yc/6lwDnd4X3Aa6vq5n6rlCRJkqTZNuo29WnX61v6JFkBXAhsBjYCpyfZOGfa7cBzquppwFuAS/qsUZIkSZLUjr7fp/Z4YHtV3VZVe4ArgFOGJ1TVF6rqh93hF4F1PdcoSZIkSWpE36F2LXDn0PGObmx/XgP8y3wnkpyRZGuSrXffffcYS5QkSZIktaLvUJt5xua9U1WS5zEItefMd76qLqmqTVW1ac2aNWMsUZIkSZLUir5vFLUDOHzoeB2wa+6kJE8DLgU2V9X3e6pNkiRJktSYvldqrwc2JFmf5FBgC3D18IQkRwAfAl5eVd/suT5JkiRJUkN6Xamtqr1JzgauZfCWPpdV1S1J9lkKtQAACfpJREFUzuzOXwy8GXg8cFESgL37ez8iSZIkSdLylqp5L2ltyqZNm2rr1q2TLkOSJEmStASS3LC/xc6+tx9LkiRJkjQ2hlpJkiRJUrMMtZIkSZKkZhlqJUmSJEnNMtRKkiRJkpplqJUkSZIkNctQK0mSJElqlqFWkiRJktQsQ60kSZIkqVmGWkmSJElSswy1kiRJkqRmGWolSZIkSc0y1EqSJEmSmmWolSRJkiQ1y1ArSZIkSWqWoVaSJEmS1CxDrSRJkiSpWYZaSZIkSVKzDLWSJEmSpGYZaiVJkiRJzTLUSpIkSZKaZaiVJEmSJDXLUCtJkiRJapahVpIkSZLULEOtJEmSJKlZhlpJkiRJUrMMtZIkSZKkZhlqJUmSJEnNMtRKkiRJkpplqJUkSZIkNav3UJvk5CTbkmxPcu4855PkH7rzX01yXN81SpIkSZLa0GuoTbICuBDYDGwETk+ycc60zcCG7uMM4B191ihJkiRJakffK7XHA9ur6raq2gNcAZwyZ84pwHtq4IvA6iSH9VynJEmSJKkBfYfatcCdQ8c7urGFzpEkSZIkiZU9f73MM1aLmEOSMxhsTwa4L8m2g6xtsZ4AfG9CX1vjZS9nh72cLfZzdtjL2WEvZ4e9nB2z3stf2N+JvkPtDuDwoeN1wK5FzKGqLgEuGXeBC5Vka1VtmnQdOnj2cnbYy9liP2eHvZwd9nJ22MvZsZx72ff24+uBDUnWJzkU2AJcPWfO1cArursgPwP4UVV9u+c6JUmSJEkN6HWltqr2JjkbuBZYAVxWVbckObM7fzFwDfACYDvw38Cr+6xRkiRJktSOvrcfU1XXMAiuw2MXDz0u4Ky+6zoIE98CrbGxl7PDXs4W+zk77OXssJezw17OjmXbywwypCRJkiRJ7en7mlpJkiRJksbGULtISU5Osi3J9iTnTroeHViSy5LcleTrQ2M/n+STSf6j+/xzQ+fO6/q7LclJk6lacyU5PMlnknwjyS1JXt+N28sGJXlUki8nubnr51914/azUUlWJLkxyUe7Y3vZoCR3JPlakpuSbO3G7GWDkqxOcmWSW7vfnc+0l21KclT3Pbnv494kb7CfhtpFSbICuBDYDGwETk+ycbJVaQTvBk6eM3Yu8Omq2gB8ujum6+cW4Je751zU9V2Ttxf446p6CvAM4KyuX/ayTbuBE6vq6cAxwMndne/tZ7teD3xj6Nhetut5VXXM0FuE2Ms2vR34eFX9EvB0Bt+f9rJBVbWt+548BvgVBjfV/TD201C7SMcD26vqtqraA1wBnDLhmnQAVfU54Adzhk8BLu8eXw6cOjR+RVXtrqrbGdyN+/heCtXDqqpvV9VXusc/ZvDLeS32skk1cF93eEj3UdjPJiVZB7wQuHRo2F7ODnvZmCSPA54NvBOgqvZU1T3Yy1nwfOBbVfWf2E9D7SKtBe4cOt7Rjak9T9r3Psjd5yd24/a4AUmOBI4FvoS9bFa3XfUm4C7gk1VlP9v1NuBPgJ8OjdnLNhXwiSQ3JDmjG7OX7XkycDfwru6ygEuTPBp7OQu2AO/vHi/7fhpqFyfzjHkb6dlij6dckscAHwTeUFX3PtzUecbs5RSpqge6rVTrgOOTPPVhptvPKZXkRcBdVXXDqE+ZZ8xeTo8Tquo4BpdanZXk2Q8z115Or5XAccA7qupY4Cd0W1P3w142IMmhwIuBfz7Q1HnGZrKfhtrF2QEcPnS8Dtg1oVp0cL6b5DCA7vNd3bg9nmJJDmEQaN9XVR/qhu1l47otcZ9lcN2P/WzPCcCLk9zB4LKcE5O8F3vZpKra1X2+i8E1e8djL1u0A9jR7YABuJJByLWXbdsMfKWqvtsdL/t+GmoX53pgQ5L13f+UbAGunnBNWpyrgVd2j18JfGRofEuSRyZZD2wAvjyB+jRHkjC4NugbVfXWoVP2skFJ1iRZ3T1eBfwacCv2szlVdV5VrauqIxn8Xryuql6GvWxOkkcneey+x8BvAF/HXjanqr4D3JnkqG7o+cC/Yy9bdzoPbj0G+8nKSRfQoqram+Rs4FpgBXBZVd0y4bJ0AEneDzwXeEKSHcBfABcAH0jyGuC/gN8BqKpbknyAwQ/+vcBZVfXARArXXCcALwe+1l2HCfCn2MtWHQZc3t2N8RHAB6rqo0n+Dfs5K/zebM+TgA8P/g+RlcA/VdXHk1yPvWzR64D3dQsxtwGvpvt5ay/bk+RngF8H/nBoeNn/nE3VTG6rliRJkiQtA24/liRJkiQ1y1ArSZIkSWqWoVaSJEmS1CxDrSRJkiSpWYZaSZIkSVKzDLWSJC1Qkhrh444kR3aPXzUFNR85p77nLuC5fzb0vB1LWKYkSQvm+9RKkrRwz5xz/GHgZuAvh8Z2A9/u5n6rn7JG8jfAxxi8b+Go3gV8Cvhz4OlLUZQkSYtlqJUkaYGq6ovDx0l2A9+bO96Zb2ySvrWfOverqnYCO5PcvUQ1SZK0aG4/liRpicy3/TjJu5PsSLIpyReS3J9kW5IXduf/qNu6fG+SjyRZM+c1VyY5L8mtSXYn2ZXk75M86iDqPKmr5UdJ7uvqefOi/+GSJPXIlVpJkvr3OOA9wN8Bu4A3AR9MciHwi8BZwJOAtwEXAr879Nz3Ar8J/C3wBeApwFuAI4HfXmghSZ4MXA1cCfw1sAfYADx54f8sSZL6Z6iVJKl/jwXOrKrPASTZxeCa3BcBG6vqgW78qcDrkqyoqgeSPAv4PeCVVfWe7rU+leQHwHuTHFNVNy2wluOAQ4HXVtW93dh1B/WvkySpR24/liSpfz/ZF2g7t3afP7Uv0A6NrwQO645PZrCS+sFuG/LKJCuBT3Tnn72IWm4C/he4IslLkjxxEa8hSdLEGGolSerfPcMHVbWne/jDOfP2je+7XvaJDFZV72MQRPd93NWdf/xCC6mq7cBJDP4m+EfgO0m+lOQ5C30tSZImwe3HkiS14/vA/wDP2s/5XYt50ar6DPCZJI8ETmBwbe3HkhxZVd9bVKWSJPXEUCtJUjs+DpwD/GxVfXrcL15Vu4HrkjwG+AiwHjDUSpKmmqFWkqRGVNVnk7wfuDLJW4EvAz9lcOfjFwDnVNU3F/KaSc5kcC3uNcCdwBOA8xis+n59fNVLkrQ0DLWSJLXlZcDrgD9g8FZAu4E7gGuB7y7i9W4GNgPnM7hm9wfAvwIvrar7x1CvJElLKlU16RokSdISS3IkcDvwGgbvkftAjfhHQJIAK4B3As+vqnVLVKYkSQvm3Y8lSVpe3sngjskLubvxm7rnvGJJKpIk6SC4UitJ0jKQ5FDgaUND26rqxyM+9zBgbXe4p6q+Ou76JElaLEOtJEmSJKlZbj+WJEmSJDXLUCtJkiRJapahVpIkSZLULEOtJEmSJKlZhlpJkiRJUrMMtZIkSZKkZv0fgVHZq3qPmY8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "# Plot results\n",
    "ax[0].legend(loc=\"upper right\")\n",
    "ax[0].set_xlabel(r\"Iteration\", fontsize=16)\n",
    "ax[0].set_ylabel(\"Loss\", fontsize=16)\n",
    "ax[0].set_title(\"CA3 - Training a deep neural network for the power consumption Dataset\")\n",
    "ax[0].set_ylim(ymin=0)\n",
    "ax[0].scatter(np.arange(len(loss_gd)), loss_gd)\n",
    "\n",
    "\n",
    "ax[1].legend(loc=\"upper right\")\n",
    "ax[1].set_xlabel(r\"Time [s]\", fontsize=16)\n",
    "ax[1].set_ylabel(\"Loss\", fontsize=16)\n",
    "ax[1].set_ylim(ymin=0)\n",
    "ax[1].scatter(time_gd, loss_gd)\n",
    "\n",
    "plt.savefig(\"power.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('sklearn': conda)",
   "language": "python",
   "name": "python38164bitsklearncondaab2615fd75db424388456508542e2d0a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
