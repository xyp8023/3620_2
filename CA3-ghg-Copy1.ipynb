{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing of data\n",
    "# Function to load data\n",
    "\n",
    "def get_ghg_data():\n",
    "    def splitDataset(totaldata, train = 0.8, seed = 123, normalize = True):\n",
    "        # seed\n",
    "        np.random.seed(seed)\n",
    "        # number\n",
    "        numdata = totaldata.shape[0]\n",
    "        numtrain = int(numdata*train)\n",
    "        numtest = numdata - numtrain\n",
    "        # index\n",
    "        index = np.arange(numdata)\n",
    "        np.random.shuffle(index)\n",
    "        # shuffle\n",
    "        totaldata = totaldata[index,:,:]\n",
    "        \n",
    "        totaldata = totaldata.reshape(numdata, -1)\n",
    "        totaldata = totaldata.T\n",
    "        totaldata = totaldata/np.linalg.norm(totaldata, axis=0)\n",
    "        totaldata = totaldata.T\n",
    "        \n",
    "        # split\n",
    "        traindata = totaldata[:numtrain, :]\n",
    "        testdata = totaldata[numtrain:, :]\n",
    "        # split X, Y\n",
    "        # train\n",
    "        X_train = traindata[:,:-1]\n",
    "        Y_train = traindata[:,-1].reshape(-1)\n",
    "\n",
    "        # test\n",
    "        X_test = testdata[:,:-1]\n",
    "        Y_test = testdata[:,-1].reshape(-1)\n",
    "        return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "    data = []\n",
    "    data_folder = '/home/zehang/Downloads/dataset/ghg_data/'\n",
    "    filelist = os.listdir(data_folder)\n",
    "    for file in filelist:\n",
    "        data_single = np.genfromtxt(data_folder+file,dtype=np.float)\n",
    "        data.append(data_single)\n",
    "        \n",
    "    data = np.array(data)\n",
    "    X_train, Y_train, X_test, Y_test = splitDataset(data)\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_ghg_data()\n",
    "print(\"X,y types: {} {}\".format(type(X_train), type(y_train)))\n",
    "print(\"X size {}\".format(X_train.shape))\n",
    "print(\"Y size {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x, derivative=False):\n",
    "    sigm = 1. / (1. + np.exp(-x)) \n",
    "    if derivative:\n",
    "        return sigm * (1. - sigm)\n",
    "    return sigm\n",
    "\n",
    "# Define weights initialization\n",
    "def initialize_w(N, d):\n",
    "    return 2*np.random.random((N,d)) - 1\n",
    "\n",
    "# Fill in feed forward propagation\n",
    "def feed_forward_propagation(X, y, w_1, w_2, w_3, lmbda):\n",
    "    # Fill in\n",
    "    # X (N,d)\n",
    "    # w_1 (d,h)\n",
    "    # w_2 (h,g)\n",
    "    # w_3 (g,1)\n",
    "    N,d = X.shape\n",
    "    layer_0 = X # (N,d)\n",
    "    layer_1 = sigmoid(np.dot(layer_0, w_1)) # (N, h)\n",
    "    layer_2 = sigmoid(np.dot(layer_1, w_2)) # (N, g)\n",
    "    layer_3 = np.dot(layer_2, w_3) # (N, 1)\n",
    "    \n",
    "    return layer_0, layer_1, layer_2, layer_3\n",
    "def back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda):\n",
    "    N = y.shape[0]\n",
    "    y = y.reshape((-1,1))\n",
    "    layer_3_delta = np.zeros_like(w_3) # (g,1)\n",
    "    layer_2_delta = np.zeros_like(w_2) # (h, g)\n",
    "    layer_1_delta = np.zeros_like(w_1) # (d, h)\n",
    "    layer_3_delta = 2 * np.dot(layer_2.T, (layer_3 - y)) # (g,1)\n",
    "    # print(\"np.dot(w_3,(layer_3-y)) shape \", np.dot(w_3,(layer_3-y).T).shape)\n",
    "    # print(\" sigmoid(np.dot(layer_1,w_2), derivative=True).T shape\", sigmoid(np.dot(layer_1,w_2), derivative=True).T.shape)\n",
    "    dJ_dl2 = 2 * np.dot(w_3,(layer_3-y).T) # # (g,N)\n",
    "    dl2_ds2 = sigmoid(np.dot(layer_1,w_2), derivative=True).T # (g,N)\n",
    "\n",
    "    layer_2_delta  = np.dot(dJ_dl2 * dl2_ds2, layer_1).T    \n",
    "    # layer_2_delta = 2 * np.dot(np.dot(w_3,(layer_3-y).T)*sigmoid(np.dot(layer_1,w_2), derivative=True).T, layer_1).T\n",
    "    ds2_dl1 = w_2 # (h,g)\n",
    "    dl1_ds1 =  sigmoid(np.dot(layer_0,w_1), derivative=True).T # (h,N)\n",
    "    ds1_dw1 = layer_0 # (N,d)\n",
    "\n",
    "    layer_1_delta = np.dot(np.dot(ds2_dl1, dJ_dl2 * dl2_ds2) * dl1_ds1, ds1_dw1).T # (d,h)\n",
    "    return layer_1_delta/N, layer_2_delta/N, layer_3_delta/N\n",
    "\n",
    "def back_propagation_blocklayer(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda, activelayer = 3):\n",
    "    N = y.shape[0]\n",
    "    y = y.reshape((-1,1))\n",
    "    layer_3_delta = np.zeros_like(w_3) # (g,1)\n",
    "    layer_2_delta = np.zeros_like(w_2) # (h, g)\n",
    "    layer_1_delta = np.zeros_like(w_1) # (d, h)\n",
    "    layer_3_delta = 2 * np.dot(layer_2.T, (layer_3 - y)) # (g,1)\n",
    "    \n",
    "    if activelayer == 3:\n",
    "        return layer_1_delta/N, layer_2_delta/N, layer_3_delta/N\n",
    "    \n",
    "    # print(\"np.dot(w_3,(layer_3-y)) shape \", np.dot(w_3,(layer_3-y).T).shape)\n",
    "    # print(\" sigmoid(np.dot(layer_1,w_2), derivative=True).T shape\", sigmoid(np.dot(layer_1,w_2), derivative=True).T.shape)\n",
    "    dJ_dl2 = 2 * np.dot(w_3,(layer_3-y).T) # # (g,N)\n",
    "    dl2_ds2 = sigmoid(np.dot(layer_1,w_2), derivative=True).T # (g,N)\n",
    "\n",
    "    layer_2_delta  = np.dot(dJ_dl2 * dl2_ds2, layer_1).T    \n",
    "    if activelayer == 2:\n",
    "        return layer_1_delta/N, layer_2_delta/N, layer_3_delta/N\n",
    "    \n",
    "    # layer_2_delta = 2 * np.dot(np.dot(w_3,(layer_3-y).T)*sigmoid(np.dot(layer_1,w_2), derivative=True).T, layer_1).T\n",
    "    ds2_dl1 = w_2 # (h,g)\n",
    "    dl1_ds1 =  sigmoid(np.dot(layer_0,w_1), derivative=True).T # (h,N)\n",
    "    ds1_dw1 = layer_0 # (N,d)\n",
    "\n",
    "    layer_1_delta = np.dot(np.dot(ds2_dl1, dJ_dl2 * dl2_ds2) * dl1_ds1, ds1_dw1).T # (d,h)\n",
    "    if activelayer == 1:\n",
    "        return layer_1_delta/N, layer_2_delta/N, layer_3_delta/N\n",
    "\n",
    "# Cost function\n",
    "def cost(X, y, w_1, w_2, w_3, lmbda):\n",
    "    N, d = X.shape\n",
    "    a1,a2,a3,a4 = feed_forward_propagation(X,y,w_1,w_2,w_3,lmbda)\n",
    "\n",
    "#     return np.linalg.norm(a4[:,0] - y,2) ** 2 / N + lmbda * (np.linalg.norm(w_1)**2 + np.linalg.norm(w_2)**2 + np.linalg.norm(w_3)**2)\n",
    "    return np.linalg.norm(a4[:,0] - y,2) ** 2 / N\n",
    "\n",
    "# Define SGD\n",
    "def SGD(X, y, w_1, w_2, w_3, lmbda, learning_rate, batch_size, iterations):\n",
    "    # Complete here:\n",
    "    loss_lst = []\n",
    "    time_lst = []\n",
    "    start = time.time()\n",
    "    y = y.reshape((-1,1))\n",
    "    for i in range(iterations):\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        loss_lst.append(loss)\n",
    "        \n",
    "        randomInd = np.arange(X.shape[0])\n",
    "        np.random.shuffle(randomInd)\n",
    "        randomInd = randomInd[:batch_size]\n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X[randomInd,:],y[randomInd,:],w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y[randomInd,:], w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda)\n",
    "        \n",
    "        g = (np.linalg.norm(layer_1_delta) + np.linalg.norm(layer_2_delta) + np.linalg.norm(layer_3_delta))/3\n",
    "        if i%20==0:\n",
    "            print(\"g is \", g, loss)   \n",
    "        if (g <= epsilon):\n",
    "#             loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "            time_lst.append(time.time()-start)\n",
    "            print(\"converge, break! current i: \", i, loss)\n",
    "            break\n",
    "        \n",
    "        w_1 = w_1 - learning_rate*layer_1_delta\n",
    "        w_2 = w_2 - learning_rate*layer_2_delta\n",
    "        w_3 = w_3 - learning_rate*layer_3_delta\n",
    "    \n",
    "        \n",
    "#         loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        \n",
    "#         if i%20==0:\n",
    "#             print(i,loss)\n",
    "        time_lst.append(time.time()-start)\n",
    "    return w_1, w_2, w_3, loss_lst, time_lst\n",
    "\n",
    "# Define SVRG here:\n",
    "def SVRG(X, y, w_1, w_2, w_3, lmbda, learning_rate, T, batch_size, iterations):\n",
    "    # Complete here:\n",
    "    y = y.reshape((-1,1))\n",
    "    N = X.shape[0]\n",
    "    loss_lst = []\n",
    "    time_lst = []\n",
    "    start = time.time()\n",
    "    for i in range(iterations):\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        loss_lst.append(loss)\n",
    "        # compute all gradient and store\n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X,y,w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3, lmbda)\n",
    "            \n",
    "        \n",
    "        g = (np.linalg.norm(layer_1_delta) + np.linalg.norm(layer_2_delta) + np.linalg.norm(layer_3_delta))/3\n",
    "        if i%20==0:\n",
    "            print(\"g is \", g, loss)   \n",
    "        if (g <= epsilon):\n",
    "#             loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)  \n",
    "            time_lst.append(time.time()-start)\n",
    "            \n",
    "            print(\"converge, break! current i: \", i, loss)\n",
    "            break\n",
    "        \n",
    "        # initialize the w_previous\n",
    "        # w_previous = w.copy()\n",
    "        w_1_previous, w_2_previous, w_3_previous = w_1.copy(), w_2.copy(), w_3.copy()\n",
    "        for t in range(T//batch_size):\n",
    "            # random sample\n",
    "            # randomInd = int(np.random.rand() * N)\n",
    "            randomInd = np.arange(N)\n",
    "            np.random.shuffle(randomInd)\n",
    "            randomInd = randomInd[:batch_size]\n",
    "            # randomInd = np.random.randint(0,N)\n",
    "            layer_0_p1, layer_1_p1, layer_2_p1, layer_3_p1 = feed_forward_propagation(X[randomInd,:],y[randomInd,:],w_1_previous,w_2_previous,w_3_previous,lmbda)\n",
    "            layer_1_delta_p1, layer_2_delta_p1, layer_3_delta_p1 = back_propagation(y[randomInd,:], w_1_previous, w_2_previous, w_3_previous, layer_0_p1, layer_1_p1, layer_2_p1, layer_3_p1, lmbda)\n",
    "\n",
    "            layer_0_p2, layer_1_p2, layer_2_p2, layer_3_p2 = feed_forward_propagation(X[randomInd,:],y[randomInd,:],w_1,w_2,w_3,lmbda)\n",
    "            layer_1_delta_p2, layer_2_delta_p2, layer_3_delta_p2 = back_propagation(y[randomInd,:], w_1, w_2, w_3, layer_0_p2, layer_1_p2, layer_2_p2, layer_3_p2, lmbda)\n",
    "            \n",
    "            # calculate the update term\n",
    "            # part1 = function_gradient_vectorization(x[:,randomInd], y[:,randomInd], w_previous, lambda_, gradclip = gradclip)\n",
    "            # part2 = function_gradient_vectorization(x[:,randomInd], y[:,randomInd], w, lambda_, gradclip = gradclip)\n",
    "            # part3 = g\n",
    "\n",
    "            w_1_previous = w_1_previous - learning_rate * (layer_1_delta_p1 - layer_1_delta_p2 + layer_1_delta)\n",
    "            w_2_previous = w_2_previous - learning_rate * (layer_2_delta_p1 - layer_2_delta_p2 + layer_2_delta)\n",
    "            w_3_previous = w_3_previous - learning_rate * (layer_3_delta_p1 - layer_3_delta_p2 + layer_3_delta)\n",
    "\n",
    "            # w_previous = w_previous - alpha * (part1 - part2 + part3)\n",
    "            \n",
    "        # w = w_previous\n",
    "        \n",
    "        w_1, w_2, w_3 = w_1_previous, w_2_previous, w_3_previous\n",
    "        time_lst.append(time.time()-start)\n",
    "        \n",
    "#         loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "#         if i%20==0:\n",
    "#             print(i,loss)\n",
    "    \n",
    "    return w_1, w_2, w_3, loss_lst, time_lst\n",
    "\n",
    "# Define GD here:\n",
    "def GD(X, y, w_1,w_2,w_3, lmbda, learning_rate, iterations):\n",
    "    N = X.shape[0]\n",
    "    loss_lst = []\n",
    "    time_lst = []\n",
    "    start = time.time()\n",
    "    for i in range(iterations):\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        loss_lst.append(loss)\n",
    "        \n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda)\n",
    "        \n",
    "        g = (np.linalg.norm(layer_1_delta) + np.linalg.norm(layer_2_delta) + np.linalg.norm(layer_3_delta))/3\n",
    "        if i%20==0:\n",
    "            print(\"g is \", g, loss)   \n",
    "        if (g <= epsilon):\n",
    "#             loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "            time_lst.append(time.time()-start)\n",
    "            \n",
    "            print(\"converge, break! current i: \", i, loss)\n",
    "            break\n",
    "#         w_1 = w_1 -  learning_rate * layer_1_delta + (lmbda / N * w_1)\n",
    "#         w_2 = w_2 - learning_rate * layer_2_delta + (lmbda / N * w_2)\n",
    "#         w_3 = w_3 - learning_rate * layer_3_delta + (lmbda / N * w_3)\n",
    "        w_1 = w_1 -  learning_rate * layer_1_delta\n",
    "        w_2 = w_2 - learning_rate * layer_2_delta\n",
    "        w_3 = w_3 - learning_rate * layer_3_delta\n",
    "#         loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "#         if i%20==0:\n",
    "#             print(i,loss)\n",
    "        time_lst.append(time.time()-start)\n",
    "\n",
    "    return w_1, w_2, w_3, loss_lst, time_lst\n",
    "\n",
    "# Define projected GD here:\n",
    "def PGD(X, y, w_1,w_2,w_3, lmbda, learning_rate, iterations, noise = None):\n",
    "    # Complete here:\n",
    "    N = X.shape[0]\n",
    "    loss_lst = []\n",
    "    time_lst = []\n",
    "    start = time.time()\n",
    "    for i in range(iterations):\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        loss_lst.append(loss)\n",
    "        \n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda)\n",
    "        \n",
    "        g = (np.linalg.norm(layer_1_delta) + np.linalg.norm(layer_2_delta) + np.linalg.norm(layer_3_delta))/3\n",
    "        if i%20==0:\n",
    "            print(\"g is \", g, loss)   \n",
    "        if (g <= epsilon):\n",
    "#             loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "            time_lst.append(time.time()-start)\n",
    "            \n",
    "            print(\"converge, break! current i: \", i,loss)\n",
    "            break\n",
    "            \n",
    "        if noise == None:\n",
    "            # the noise level is determined by the delta\n",
    "            noise = []\n",
    "            noise.append((np.random.random((w_1.shape)) * 2 - 1) * 0.0001)\n",
    "            noise.append((np.random.random((w_2.shape)) * 2 - 1) * 0.0001)\n",
    "            noise.append((np.random.random((w_3.shape)) * 2 - 1) * 0.0001)\n",
    "        \n",
    "        w_1 = w_1 -  learning_rate * layer_1_delta + noise[0]\n",
    "        w_2 = w_2 - learning_rate * layer_2_delta + noise[1]\n",
    "        w_3 = w_3 - learning_rate * layer_3_delta + noise[2]\n",
    "#         loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        \n",
    "#         if i%20==0:\n",
    "#             print(i,loss)\n",
    "        time_lst.append(time.time()-start)\n",
    "\n",
    "    return w_1, w_2, w_3, loss_lst, time_lst\n",
    "\n",
    "# Define BCD here:\n",
    "def BCD(X, y, w_1,w_2,w_3, lmbda, learning_rate, iterations, strategy = 0, blockrate = 0.5):\n",
    "    loss_lst = []\n",
    "    time_lst = []\n",
    "    start = time.time()\n",
    "    # Complete here:\n",
    "    # 2 strategies:\n",
    "    # a. randomly select weight by a random mask (random coordinate selection)\n",
    "    # b. we train the layers asynchronously to archieve the goal of blocking coordinate (cyclic update rule) \n",
    "    for i in range(iterations):\n",
    "        loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "        loss_lst.append(loss)\n",
    "        \n",
    "        layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "        layer_1_delta, layer_2_delta, layer_3_delta = back_propagation(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda)\n",
    "        \n",
    "        g = (np.linalg.norm(layer_1_delta) + np.linalg.norm(layer_2_delta) + np.linalg.norm(layer_3_delta))/3\n",
    "        if i%20==0:\n",
    "            print(\"g is \", g, loss)   \n",
    "        if (g <= epsilon):\n",
    "            time_lst.append(time.time()-start)\n",
    "            \n",
    "#             loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "            print(\"converge, break! current i: \", i,loss)\n",
    "            break\n",
    "            \n",
    "        if strategy == 0:\n",
    "            layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "            # generate random mask for each layer\n",
    "            l1_mask = np.random.choice([1, 0], size=layer_1_delta.shape, p=[blockrate, 1 - blockrate])\n",
    "            l2_mask = np.random.choice([1, 0], size=layer_2_delta.shape, p=[blockrate, 1 - blockrate])\n",
    "            l3_mask = np.random.choice([1, 0], size=layer_3_delta.shape, p=[blockrate, 1 - blockrate])\n",
    "            w_1 = w_1 - learning_rate * layer_1_delta * l1_mask\n",
    "            w_2 = w_2 - learning_rate * layer_2_delta * l2_mask\n",
    "            w_3 = w_3 - learning_rate * layer_3_delta * l3_mask\n",
    "        else:\n",
    "            # cyclic\n",
    "            # layer 1\n",
    "            layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "            layer_1_delta, _, _ = back_propagation_blocklayer(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda, 1)\n",
    "            w_1 = w_1 - learning_rate * layer_1_delta\n",
    "            # layer 2\n",
    "            layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "            _, layer_2_delta, _ = back_propagation_blocklayer(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda, 2)\n",
    "            w_2 = w_2 - learning_rate * layer_2_delta\n",
    "            # layer 3\n",
    "            layer_0, layer_1, layer_2, layer_3 = feed_forward_propagation(X, y, w_1,w_2,w_3,lmbda)\n",
    "            _, _, layer_3_delta = back_propagation_blocklayer(y, w_1, w_2, w_3, layer_0, layer_1, layer_2, layer_3,lmbda, 3)\n",
    "            w_3 = w_3 - learning_rate * layer_3_delta\n",
    "#         loss = cost(X_train, y_train, w_1, w_2, w_3, lmbda)\n",
    "#         if i%20==0:\n",
    "#             print(i,loss)\n",
    "        time_lst.append(time.time()-start)\n",
    "\n",
    "    return w_1, w_2, w_3, loss_lst, time_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be a hyperparameter that you tune, not an argument - Fill in the values\n",
    "lmbda =0.01\n",
    "w_size = 50\n",
    "lr = 0.002\n",
    "iterations = 5000 # 100\n",
    "T = 2000\n",
    "batch_size = 100\n",
    "epsilon = 0.05\n",
    "\n",
    "# Initialize weights\n",
    "w_1 = initialize_w(X_train.shape[1], w_size)\n",
    "\n",
    "w_2 = initialize_w(w_size,w_size)\n",
    "\n",
    "w_3 = initialize_w(w_size, 1)\n",
    "\n",
    "print(\"GD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_gd = time.time()\n",
    "w_1_star,w_2_star,w_3_star,loss_gd,time_gd = GD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations)\n",
    "end_gd = time.time()\n",
    "print(\"Training time for GD: \", end_gd-start_gd)\n",
    "\n",
    "print(\"PGD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_pgd = time.time()\n",
    "w_1_star,w_2_star,w_3_star,loss_pgd,time_pgd = PGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations)\n",
    "end_pgd = time.time()\n",
    "print(\"Training time for PGD: \", end_pgd-start_pgd)\n",
    "\n",
    "\n",
    "print(\"SGD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_sgd = time.time()\n",
    "w_1_star,w_2_star,w_3_star,loss_sgd,time_sgd = SGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, batch_size, iterations)\n",
    "end_sgd = time.time()\n",
    "print(\"Training time for SGD: \", end_sgd-start_sgd)\n",
    "\n",
    "\n",
    "\n",
    "print(\"SVRG\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_svrg = time.time()\n",
    "w_1_star1,w_2_star1,w_3_star1,loss_svrg,time_svrg = SVRG(X_train, y_train, w_1, w_2, w_3, lmbda, lr, T, batch_size, iterations)\n",
    "end_svrg = time.time()\n",
    "print(\"Training time for SVRG: \", end_svrg-start_svrg)\n",
    "\n",
    "print(\"BCD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "strategy = 0 # 0 for random block mask, 1 for layer block\n",
    "blockrate = 0.5\n",
    "start_bcd = time.time()\n",
    "w_1_star,w_2_star,w_3_star ,loss_bcd,time_bcd = BCD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations, strategy, blockrate)\n",
    "end_bcd = time.time()\n",
    "print(\"Training time for BCD: \", end_bcd-start_bcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be a hyperparameter that you tune, not an argument - Fill in the values\n",
    "lmbda =0.01\n",
    "w_size = 50\n",
    "lr = 0.001\n",
    "iterations = 5000 # 100\n",
    "T = 2000\n",
    "batch_size = 100\n",
    "epsilon = 0.05\n",
    "\n",
    "# Initialize weights\n",
    "w_1 = initialize_w(X_train.shape[1], w_size)\n",
    "\n",
    "w_2 = initialize_w(w_size,w_size)\n",
    "\n",
    "w_3 = initialize_w(w_size, 1)\n",
    "\n",
    "print(\"GD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_gd = time.time()\n",
    "w_1_star,w_2_star,w_3_star, loss_gd1,time_gd1 = GD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations)\n",
    "end_gd = time.time()\n",
    "print(\"Training time for GD: \", end_gd-start_gd)\n",
    "\n",
    "print(\"PGD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_pgd = time.time()\n",
    "w_1_star,w_2_star,w_3_star, loss_pgd1,time_pgd1 = PGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations)\n",
    "end_pgd = time.time()\n",
    "print(\"Training time for PGD: \", end_pgd-start_pgd)\n",
    "\n",
    "\n",
    "print(\"SGD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_sgd = time.time()\n",
    "w_1_star,w_2_star,w_3_star, loss_sgd1,time_sgd1 = SGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, batch_size, iterations)\n",
    "end_sgd = time.time()\n",
    "print(\"Training time for SGD: \", end_sgd-start_sgd)\n",
    "\n",
    "\n",
    "\n",
    "print(\"SVRG\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_svrg = time.time()\n",
    "w_1_star1,w_2_star1,w_3_star1, loss_svrg1,time_svrg1 = SVRG(X_train, y_train, w_1, w_2, w_3, lmbda, lr, T, batch_size, iterations)\n",
    "end_svrg = time.time()\n",
    "print(\"Training time for SVRG: \", end_svrg-start_svrg)\n",
    "\n",
    "print(\"BCD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "strategy = 0 # 0 for random block mask, 1 for layer block\n",
    "blockrate = 0.5\n",
    "start_bcd = time.time()\n",
    "w_1_star,w_2_star,w_3_star, loss_bcd1,time_bcd1 = BCD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations, strategy, blockrate)\n",
    "end_bcd = time.time()\n",
    "print(\"Training time for BCD: \", end_bcd-start_bcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "    1\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Should be a hyperparameter that you tune, not an argument - Fill in the values\n",
    "lmbda =0.1\n",
    "w_size = 50\n",
    "lr = 0.001\n",
    "iterations = 5000 # 100\n",
    "T = 2000\n",
    "batch_size = 100\n",
    "epsilon = 0.05\n",
    "\n",
    "# Initialize weights\n",
    "w_1 = initialize_w(X_train.shape[1], w_size)\n",
    "\n",
    "w_2 = initialize_w(w_size,w_size)\n",
    "\n",
    "w_3 = initialize_w(w_size, 1)\n",
    "\n",
    "print(\"GD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_gd = time.time()\n",
    "w_1_star,w_2_star,w_3_star, loss_gd2, time_gd2 = GD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations)\n",
    "end_gd = time.time()\n",
    "print(\"Training time for GD: \", end_gd-start_gd)\n",
    "\n",
    "print(\"PGD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_pgd = time.time()\n",
    "w_1_star,w_2_star,w_3_star, loss_pgd2,time_pgd2 = PGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations)\n",
    "end_pgd = time.time()\n",
    "print(\"Training time for PGD: \", end_pgd-start_pgd)\n",
    "\n",
    "\n",
    "print(\"SGD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_sgd = time.time()\n",
    "w_1_star,w_2_star,w_3_star, loss_sgd2,time_sgd2 = SGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, batch_size, iterations)\n",
    "end_sgd = time.time()\n",
    "print(\"Training time for SGD: \", end_sgd-start_sgd)\n",
    "\n",
    "\n",
    "\n",
    "print(\"SVRG\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_svrg = time.time()\n",
    "w_1_star1,w_2_star1,w_3_star2, loss_svrg2,time_svrg2 = SVRG(X_train, y_train, w_1, w_2, w_3, lmbda, lr, T, batch_size, iterations)\n",
    "end_svrg = time.time()\n",
    "print(\"Training time for SVRG: \", end_svrg-start_svrg)\n",
    "\n",
    "print(\"BCD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "strategy = 0 # 0 for random block mask, 1 for layer block\n",
    "blockrate = 0.5\n",
    "start_bcd = time.time()\n",
    "w_1_star,w_2_star,w_3_star, loss_bcd2,time_bcd2 = BCD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations, strategy, blockrate)\n",
    "end_bcd = time.time()\n",
    "print(\"Training time for BCD: \", end_bcd-start_bcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "    Same parameter\n",
    "    \n",
    "'''\n",
    "\n",
    "\n",
    "# Should be a hyperparameter that you tune, not an argument - Fill in the values\n",
    "lmbda =0.1\n",
    "w_size = 50\n",
    "lr = 0.001\n",
    "iterations = 5000 # 100\n",
    "T = 2000\n",
    "batch_size = 100\n",
    "epsilon = 0.05\n",
    "\n",
    "# Initialize weights\n",
    "w_1 = initialize_w(X_train.shape[1], w_size)\n",
    "\n",
    "w_2 = initialize_w(w_size,w_size)\n",
    "\n",
    "w_3 = initialize_w(w_size, 1)\n",
    "\n",
    "print(\"GD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_gd = time.time()\n",
    "w_1_star,w_2_star,w_3_star, loss_gd2, time_gd2 = GD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations)\n",
    "end_gd = time.time()\n",
    "print(\"Training time for GD: \", end_gd-start_gd)\n",
    "\n",
    "print(\"PGD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_pgd = time.time()\n",
    "w_1_star,w_2_star,w_3_star, loss_pgd2,time_pgd2 = PGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations)\n",
    "end_pgd = time.time()\n",
    "print(\"Training time for PGD: \", end_pgd-start_pgd)\n",
    "\n",
    "\n",
    "print(\"SGD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_sgd = time.time()\n",
    "w_1_star,w_2_star,w_3_star, loss_sgd2,time_sgd2 = SGD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, batch_size, iterations)\n",
    "end_sgd = time.time()\n",
    "print(\"Training time for SGD: \", end_sgd-start_sgd)\n",
    "\n",
    "\n",
    "\n",
    "print(\"SVRG\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "start_svrg = time.time()\n",
    "w_1_star1,w_2_star1,w_3_star2, loss_svrg2,time_svrg2 = SVRG(X_train, y_train, w_1, w_2, w_3, lmbda, lr, T, batch_size, iterations)\n",
    "end_svrg = time.time()\n",
    "print(\"Training time for SVRG: \", end_svrg-start_svrg)\n",
    "\n",
    "print(\"BCD\\tinitial loss is :\", cost(X_train, y_train, w_1, w_2, w_3, lmbda))\n",
    "strategy = 0 # 0 for random block mask, 1 for layer block\n",
    "blockrate = 0.5\n",
    "start_bcd = time.time()\n",
    "w_1_star,w_2_star,w_3_star, loss_bcd2,time_bcd2 = BCD(X_train, y_train, w_1, w_2, w_3, lmbda, lr, iterations, strategy, blockrate)\n",
    "end_bcd = time.time()\n",
    "print(\"Training time for BCD: \", end_bcd-start_bcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "# Plot results\n",
    "ax[0].legend(loc=\"upper right\")\n",
    "ax[0].set_xlabel(r\"Iteration\", fontsize=16)\n",
    "ax[0].set_ylabel(\"Loss\", fontsize=16)\n",
    "ax[0].set_title(\"CA3 - Training a deep neural network for the ghg Dataset\")\n",
    "ax[0].set_ylim(ymin=0, ymax=5)\n",
    "ax[0].plot(np.arange(len(loss_gd)), loss_gd, label=\"GD\")\n",
    "ax[0].plot(np.arange(len(loss_pgd[:120])), loss_pgd[:120], label=\"PGD\")\n",
    "ax[0].plot(np.arange(len(loss_sgd)), loss_sgd, label=\"SGD\")\n",
    "ax[0].plot(np.arange(len(loss_svrg)), loss_svrg, label=\"SVRG\")\n",
    "ax[0].plot(np.arange(len(loss_bcd)), loss_bcd, label=\"BCD\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].legend(loc=\"upper right\")\n",
    "ax[1].set_xlabel(r\"Time [s]\", fontsize=16)\n",
    "ax[1].set_ylabel(\"Loss\", fontsize=16)\n",
    "ax[1].set_ylim(ymin=0, ymax=5)\n",
    "ax[1].plot(time_gd, loss_gd, label=\"GD\")\n",
    "ax[1].plot(time_pgd[:120], loss_pgd[:120], label=\"PGD\")\n",
    "ax[1].plot(time_sgd, loss_sgd, label=\"SGD\")\n",
    "ax[1].plot(time_svrg, loss_svrg, label=\"SVRG\")\n",
    "ax[1].plot(time_bcd, loss_bcd, label=\"BCD\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.savefig(\"ghg-power-Loss-Iter.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(time_svrg1))\n",
    "print(len(loss_svrg1))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "# Plot results\n",
    "ax[0].legend(loc=\"upper right\")\n",
    "ax[0].set_xlabel(r\"Iteration\", fontsize=16)\n",
    "ax[0].set_ylabel(\"Loss\", fontsize=16)\n",
    "ax[0].set_title(\"CA3 - Training a deep neural network for the ghg Dataset\")\n",
    "ax[0].set_ylim(ymin=0, ymax=5)\n",
    "ax[0].plot(np.arange(len(loss_gd1)), loss_gd1, label=\"GD\")\n",
    "ax[0].plot(np.arange(len(loss_pgd1[:280])), loss_pgd1[:280], label=\"PGD\")\n",
    "ax[0].plot(np.arange(len(loss_sgd1)), loss_sgd1, label=\"SGD\")\n",
    "ax[0].plot(np.arange(len(loss_svrg1[:10])), loss_svrg1[:10], label=\"SVRG\")\n",
    "ax[0].plot(np.arange(len(loss_bcd1)), loss_bcd1, label=\"BCD\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].legend(loc=\"upper right\")\n",
    "ax[1].set_xlabel(r\"Time [s]\", fontsize=16)\n",
    "ax[1].set_ylabel(\"Loss\", fontsize=16)\n",
    "ax[1].set_ylim(ymin=0, ymax=5)\n",
    "ax[1].plot(time_gd1, loss_gd1, label=\"GD\")\n",
    "ax[1].plot(time_pgd1[:280], loss_pgd1[:280], label=\"PGD\")\n",
    "ax[1].plot(time_sgd1, loss_sgd1, label=\"SGD\")\n",
    "ax[1].plot(time_svrg1, loss_svrg1[:10], label=\"SVRG\")\n",
    "ax[1].plot(time_bcd1, loss_bcd1, label=\"BCD\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.savefig(\"ghg-power-Loss-Iter1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "# Plot results\n",
    "ax[0].legend(loc=\"upper right\")\n",
    "ax[0].set_xlabel(r\"Iteration\", fontsize=16)\n",
    "ax[0].set_ylabel(\"Loss\", fontsize=16)\n",
    "ax[0].set_title(\"CA3 - Training a deep neural network for the ghg Dataset\")\n",
    "ax[0].set_ylim(ymin=0, ymax=5)\n",
    "ax[0].plot(np.arange(len(loss_gd2)), loss_gd2, label=\"GD\")\n",
    "ax[0].plot(np.arange(len(loss_pgd2[:114])), loss_pgd2[:114], label=\"PGD\")\n",
    "ax[0].plot(np.arange(len(loss_sgd2)), loss_sgd2, label=\"SGD\")\n",
    "ax[0].plot(np.arange(len(loss_svrg1)), loss_svrg1, label=\"SVRG\")\n",
    "ax[0].plot(np.arange(len(loss_bcd2)), loss_bcd2, label=\"BCD\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].legend(loc=\"upper right\")\n",
    "ax[1].set_xlabel(r\"Time [s]\", fontsize=16)\n",
    "ax[1].set_ylabel(\"Loss\", fontsize=16)\n",
    "ax[1].set_ylim(ymin=0, ymax=5)\n",
    "ax[1].plot(time_gd2, loss_gd2, label=\"GD\")\n",
    "ax[1].plot(time_pgd2[:114], loss_pgd2[:114], label=\"PGD\")\n",
    "ax[1].plot(time_sgd2, loss_sgd2, label=\"SGD\")\n",
    "ax[1].plot(time_svrg2, loss_svrg2, label=\"SVRG\")\n",
    "ax[1].plot(time_bcd2, loss_bcd2, label=\"BCD\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.savefig(\"ghg-power-Loss-Iter2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('sklearn': conda)",
   "language": "python",
   "name": "python38164bitsklearncondaab2615fd75db424388456508542e2d0a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
